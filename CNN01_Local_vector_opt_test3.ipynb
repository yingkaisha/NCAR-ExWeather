{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35ec06d-99e2-426d-8350-28623a7ed5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:30:52.765851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "\n",
    "import re\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('lead1', help='lead')\n",
    "# parser.add_argument('lead2', help='lead')\n",
    "# parser.add_argument('lead3', help='lead')\n",
    "# parser.add_argument('lead4', help='lead')\n",
    "# parser.add_argument('lead_name', help='lead')\n",
    "# parser.add_argument('model_tag', help='lead')\n",
    "# args = vars(parser.parse_args())\n",
    "\n",
    "lead1 = 2 #int(args['lead1'])\n",
    "lead2 = 3 #int(args['lead2'])\n",
    "lead3 = 4 #int(args['lead3'])\n",
    "\n",
    "lead_name = 3 #int(args['lead_name'])\n",
    "model_tag = 'alt' #args['model_tag']\n",
    "\n",
    "filepath_vec = \"/glade/work/ksha/NCAR/\"\n",
    "\n",
    "if (lead1 < 9) or (lead1 > 18):\n",
    "    path_name1 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name1 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "\n",
    "if (lead2 < 9) or (lead2 > 18):\n",
    "    path_name2 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name2 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "    \n",
    "if (lead3 < 9) or (lead3 > 18):\n",
    "    path_name3 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "\n",
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def verif_metric(VALID_target, Y_pred, ref):\n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    metric = BS\n",
    "    return metric / ref\n",
    "\n",
    "def feature_extract(filenames, lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max):\n",
    "    \n",
    "    lon_out = []\n",
    "    lat_out = []\n",
    "    elev_out = []\n",
    "    mon_out = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+180-151)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        month = day.month\n",
    "        \n",
    "        month_norm = (month - 1)/(12-1)\n",
    "        \n",
    "        lon = lon_80km[indx, indy]\n",
    "        lat = lat_80km[indx, indy]\n",
    "\n",
    "        lon = (lon - lon_minmax[0])/(lon_minmax[1] - lon_minmax[0])\n",
    "        lat = (lat - lat_minmax[0])/(lat_minmax[1] - lat_minmax[0])\n",
    "\n",
    "        elev = elev_80km[indx, indy]\n",
    "        elev = elev / elev_max\n",
    "        \n",
    "        lon_out.append(lon)\n",
    "        lat_out.append(lat)\n",
    "        elev_out.append(elev)\n",
    "        mon_out.append(month_norm)\n",
    "        \n",
    "    return np.array(lon_out), np.array(lat_out), np.array(elev_out), np.array(mon_out)\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    \n",
    "    IN_vec = keras.Input((384,))    \n",
    "    IN_elev = keras.Input((3,))\n",
    "    IN = keras.layers.Concatenate()([IN_vec, IN_elev])\n",
    "    \n",
    "    X = IN\n",
    "    #\n",
    "    X = keras.layers.Dense(1024, activity_regularizer=keras.regularizers.L2(1e-2))(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "    \n",
    "    X = keras.layers.Dropout(0.4)(X)\n",
    "    \n",
    "    X = keras.layers.Dense(512, activity_regularizer=keras.regularizers.L2(1e-2))(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "    \n",
    "    X = keras.layers.Dropout(0.4)(X)\n",
    "    \n",
    "    X = keras.layers.Dense(128, activity_regularizer=keras.regularizers.L2(1e-2))(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=[IN_vec, IN_elev], outputs=OUT)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f933e6-fcb7-4e02-9bb6-c3d33b4ff170",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(save_dir+'HRRR_domain.hdf', 'r') as h5io:\n",
    "    lon_3km = h5io['lon_3km'][...]\n",
    "    lat_3km = h5io['lat_3km'][...]\n",
    "    lon_80km = h5io['lon_80km'][...]\n",
    "    lat_80km = h5io['lat_80km'][...]\n",
    "    elev_3km = h5io['elev_3km'][...]\n",
    "    land_mask_80km = h5io['land_mask_80km'][...]\n",
    "    \n",
    "grid_shape = land_mask_80km.shape\n",
    "\n",
    "elev_80km = du.interp2d_wraper(lon_3km, lat_3km, elev_3km, lon_80km, lat_80km, method='linear')\n",
    "\n",
    "elev_80km[np.isnan(elev_80km)] = 0\n",
    "elev_80km[elev_80km<0] = 0\n",
    "elev_max = np.max(elev_80km)\n",
    "\n",
    "lon_80km_mask = lon_80km[land_mask_80km]\n",
    "lat_80km_mask = lat_80km[land_mask_80km]\n",
    "\n",
    "lon_minmax = [np.min(lon_80km_mask), np.max(lon_80km_mask)]\n",
    "lat_minmax = [np.min(lat_80km_mask), np.max(lat_80km_mask)]\n",
    "\n",
    "filename_train_lead1 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1, lead1)))\n",
    "filename_train_lead2 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name2, lead2)))\n",
    "filename_train_lead3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name3, lead3)))\n",
    "\n",
    "IND_TRAIN_lead = np.load('/glade/work/ksha/NCAR/IND_TRAIN_lead_full.npy', allow_pickle=True)[()]\n",
    "TRAIN_ind1 = IND_TRAIN_lead['lead{}'.format(lead1)]\n",
    "TRAIN_ind2 = IND_TRAIN_lead['lead{}'.format(lead2)]\n",
    "TRAIN_ind3 = IND_TRAIN_lead['lead{}'.format(lead3)]\n",
    "\n",
    "data_lead1_p0 = np.load('{}TRAIN_pp15_pred_lead{}_part0_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p1 = np.load('{}TRAIN_pp15_pred_lead{}_part1_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p2 = np.load('{}TRAIN_pp15_pred_lead{}_part2_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead2_p0 = np.load('{}TRAIN_pp15_pred_lead{}_part0_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p1 = np.load('{}TRAIN_pp15_pred_lead{}_part1_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p2 = np.load('{}TRAIN_pp15_pred_lead{}_part2_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead3_p0 = np.load('{}TRAIN_pp15_pred_lead{}_part0_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p1 = np.load('{}TRAIN_pp15_pred_lead{}_part1_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p2 = np.load('{}TRAIN_pp15_pred_lead{}_part2_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "TRAIN_lead1 = np.concatenate((data_lead1_p0['y_vector'], data_lead1_p1['y_vector'], data_lead1_p2['y_vector']), axis=0)\n",
    "TRAIN_lead2 = np.concatenate((data_lead2_p0['y_vector'], data_lead2_p1['y_vector'], data_lead2_p2['y_vector']), axis=0)\n",
    "TRAIN_lead3 = np.concatenate((data_lead3_p0['y_vector'], data_lead3_p1['y_vector'], data_lead3_p2['y_vector']), axis=0)\n",
    "\n",
    "TRAIN_lead1_y = np.concatenate((data_lead1_p0['y_true'], data_lead1_p1['y_true'], data_lead1_p2['y_true']), axis=0)\n",
    "TRAIN_lead2_y = np.concatenate((data_lead2_p0['y_true'], data_lead2_p1['y_true'], data_lead2_p2['y_true']), axis=0)\n",
    "TRAIN_lead3_y = np.concatenate((data_lead3_p0['y_true'], data_lead3_p1['y_true'], data_lead3_p2['y_true']), axis=0)\n",
    "\n",
    "L = len(TRAIN_ind1)\n",
    "\n",
    "filename_train1_pick = []\n",
    "filename_train2_pick = []\n",
    "filename_train3_pick = []\n",
    "\n",
    "TRAIN_X = np.empty((L, 384))\n",
    "TRAIN_Y = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1 = int(TRAIN_ind1[i])\n",
    "    ind_lead2 = int(TRAIN_ind2[i])\n",
    "    ind_lead3 = int(TRAIN_ind3[i])\n",
    "    \n",
    "    filename_train1_pick.append(filename_train_lead1[ind_lead1])\n",
    "    filename_train2_pick.append(filename_train_lead2[ind_lead2])\n",
    "    filename_train3_pick.append(filename_train_lead3[ind_lead3])\n",
    "    \n",
    "    TRAIN_X[i, 0:128]   = TRAIN_lead1[ind_lead1, :]\n",
    "    TRAIN_X[i, 128:256] = TRAIN_lead2[ind_lead2, :]\n",
    "    TRAIN_X[i, 256:384] = TRAIN_lead3[ind_lead3, :]\n",
    " \n",
    "    TRAIN_Y[i] = TRAIN_lead2_y[ind_lead2]\n",
    "    \n",
    "lon_norm, lat_norm, elev_norm, mon_norm = feature_extract(\n",
    "    filename_train1_pick, lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "TRAIN_stn = np.concatenate((lon_norm[:, None], lat_norm[:, None], elev_norm[:, None]), axis=1)\n",
    "TRAIN_merge = TRAIN_stn\n",
    "\n",
    "TRAIN_256_pos = TRAIN_X[TRAIN_Y==1, :]\n",
    "TRAIN_256_neg = TRAIN_X[TRAIN_Y==0, :]\n",
    "\n",
    "TRAIN_stn_pos = TRAIN_merge[TRAIN_Y==1]\n",
    "TRAIN_stn_neg = TRAIN_merge[TRAIN_Y==0]\n",
    "\n",
    "filename_valid_lead1 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1, lead1)))\n",
    "filename_valid_lead2 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name2, lead2)))\n",
    "filename_valid_lead3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name3, lead3)))\n",
    "\n",
    "valid_lead1 = np.load('{}TEST_pp15_pred_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "valid_lead2 = np.load('{}TEST_pp15_pred_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "valid_lead3 = np.load('{}TEST_pp15_pred_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "VALID_lead1 = valid_lead1['y_vector']\n",
    "VALID_lead2 = valid_lead2['y_vector']\n",
    "VALID_lead3 = valid_lead3['y_vector']\n",
    "\n",
    "VALID_lead1_y = valid_lead1['y_true']\n",
    "VALID_lead2_y = valid_lead2['y_true']\n",
    "VALID_lead3_y = valid_lead3['y_true']\n",
    "\n",
    "IND_VALID_lead = np.load('/glade/work/ksha/NCAR/IND_VALID_lead_full.npy', allow_pickle=True)[()]\n",
    "\n",
    "VALID_ind1 = IND_VALID_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2 = IND_VALID_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3 = IND_VALID_lead['lead{}'.format(lead3)]\n",
    "\n",
    "L = len(VALID_ind1)\n",
    "\n",
    "filename_valid1_pick = []\n",
    "filename_valid2_pick = []\n",
    "filename_valid3_pick = []\n",
    "\n",
    "VALID_X = np.empty((L, 384))\n",
    "VALID_Y = np.zeros(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1 = int(VALID_ind1[i])\n",
    "    ind_lead2 = int(VALID_ind2[i])\n",
    "    ind_lead3 = int(VALID_ind3[i])\n",
    "    \n",
    "    filename_valid1_pick.append(filename_valid_lead1[ind_lead1])\n",
    "    filename_valid2_pick.append(filename_valid_lead2[ind_lead2])\n",
    "    filename_valid3_pick.append(filename_valid_lead3[ind_lead3])\n",
    "    \n",
    "    VALID_X[i, 0:128]   = VALID_lead1[ind_lead1, :]\n",
    "    VALID_X[i, 128:256] = VALID_lead2[ind_lead2, :]\n",
    "    VALID_X[i, 256:384] = VALID_lead3[ind_lead3, :]\n",
    "    \n",
    "    if 'pos' in filename_valid_lead2[ind_lead2]:\n",
    "        if VALID_lead2_y[ind_lead2] == 1.0:\n",
    "            VALID_Y[i] = 1.0\n",
    "        else:\n",
    "            egwrshat\n",
    "        \n",
    "lon_norm, lat_norm, elev_norm, mon_norm = feature_extract(\n",
    "    filename_valid1_pick, lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "        \n",
    "VALID_stn = np.concatenate((lon_norm[:, None], lat_norm[:, None], elev_norm[:, None]), axis=1)\n",
    "VALID_merge = VALID_stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1cceb-d21e-402f-995b-16ad8f3b8c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:36:08.627565: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-23 21:36:08.629239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-23 21:36:08.677034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-01-23 21:36:08.677083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-23 21:36:08.755377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-23 21:36:08.755441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-01-23 21:36:08.818948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-23 21:36:08.930066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-23 21:36:09.061930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-23 21:36:09.208070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-23 21:36:09.335600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-23 21:36:09.336359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-23 21:36:09.337212: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-23 21:36:09.337419: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-23 21:36:09.337829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-01-23 21:36:09.337865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-23 21:36:09.337890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-01-23 21:36:09.337901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-01-23 21:36:09.337910: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-23 21:36:09.337919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-23 21:36:09.337928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-23 21:36:09.337938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-01-23 21:36:09.337947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-01-23 21:36:09.338453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-23 21:36:09.338499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-01-23 21:36:12.103917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-23 21:36:12.103957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-01-23 21:36:12.103976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-01-23 21:36:12.105045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:61:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:36:13.593325: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-01-23 21:36:13.598412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2600000000 Hz\n",
      "2023-01-23 21:36:13.810234: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1.1 to 0.9997425407337768\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 21:36:20.215638: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 8.504392623901367 seconds ---\n",
      "Validation loss 0.9997943197309155 NOT improved\n",
      "Validation loss 0.9998368111754904 NOT improved\n",
      "Validation loss 0.9998570200737817 NOT improved\n",
      "Validation loss 0.999877090002003 NOT improved\n",
      "Validation loss 0.99988360909603 NOT improved\n",
      "Validation loss 0.9998716354530787 NOT improved\n",
      "Validation loss 0.9998554166413102 NOT improved\n",
      "Validation loss 0.9998771871986468 NOT improved\n",
      "Validation loss 0.9998358484495525 NOT improved\n",
      "Validation loss 0.9998373455744018 NOT improved\n",
      "Validation loss 0.999789348714935 NOT improved\n",
      "Validation loss 0.9998168865715185 NOT improved\n",
      "Validation loss 0.9997431758960577 NOT improved\n",
      "Validation loss improved from 0.9997425407337768 to 0.9996367039575064\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 101.4269049167633 seconds ---\n",
      "Validation loss improved from 0.9996367039575064 to 0.9991981529631887\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 7.704988956451416 seconds ---\n",
      "Validation loss 0.9996431399251366 NOT improved\n",
      "Validation loss improved from 0.9991981529631887 to 0.9991071402173033\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 7.0518224239349365 seconds ---\n",
      "Validation loss 0.9991800270284688 NOT improved\n",
      "Validation loss improved from 0.9991071402173033 to 0.9988551937459895\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.871078968048096 seconds ---\n",
      "Validation loss improved from 0.9988551937459895 to 0.9975139240943306\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.493228197097778 seconds ---\n",
      "Validation loss 0.9985679048952407 NOT improved\n",
      "Validation loss improved from 0.9975139240943306 to 0.9953539673697467\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.912123918533325 seconds ---\n",
      "Validation loss 0.995639043274991 NOT improved\n",
      "Validation loss improved from 0.9953539673697467 to 0.9928920227197611\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.501780033111572 seconds ---\n",
      "Validation loss improved from 0.9928920227197611 to 0.9899327019413607\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.977742671966553 seconds ---\n",
      "Validation loss improved from 0.9899327019413607 to 0.9859051921568601\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.816039085388184 seconds ---\n",
      "Validation loss improved from 0.9859051921568601 to 0.9771176618816061\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.5068957805633545 seconds ---\n",
      "Validation loss improved from 0.9771176618816061 to 0.9562129737023404\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 7.147847414016724 seconds ---\n",
      "Validation loss improved from 0.9562129737023404 to 0.9507055928865904\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.502542018890381 seconds ---\n",
      "Validation loss 0.9951228304383105 NOT improved\n",
      "Validation loss 0.993459781778389 NOT improved\n",
      "Validation loss 0.98197066118637 NOT improved\n",
      "Validation loss improved from 0.9507055928865904 to 0.8751485329310009\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.8450093269348145 seconds ---\n",
      "Validation loss 0.895637776909625 NOT improved\n",
      "Validation loss 0.8782115107024052 NOT improved\n",
      "Validation loss 0.8873116428674521 NOT improved\n",
      "Validation loss 0.9185547110132078 NOT improved\n",
      "Validation loss 0.9919157719523547 NOT improved\n",
      "Validation loss 0.9258411226724811 NOT improved\n",
      "Validation loss 0.8799219474746621 NOT improved\n",
      "Validation loss 0.9014572070888114 NOT improved\n",
      "Validation loss 1.27844183251351 NOT improved\n",
      "Early stopping\n",
      "Training round 1\n",
      "Validation loss 0.9998108330470163 NOT improved\n",
      "Validation loss 0.9998240260514182 NOT improved\n",
      "Validation loss 0.9998420784957135 NOT improved\n",
      "Validation loss 0.9998530134016985 NOT improved\n",
      "Validation loss 0.9998541770814162 NOT improved\n",
      "Validation loss 0.9998511925129825 NOT improved\n",
      "Validation loss 0.9998726792435314 NOT improved\n",
      "Validation loss 0.9998597365167561 NOT improved\n",
      "Validation loss 0.9998629320861522 NOT improved\n",
      "Validation loss 0.9998368201225502 NOT improved\n",
      "Validation loss 0.9998255983426764 NOT improved\n",
      "Validation loss 0.9997867915953178 NOT improved\n",
      "Validation loss 0.9997949537372529 NOT improved\n",
      "Validation loss 0.9997652175792154 NOT improved\n",
      "Validation loss 0.9996684652575157 NOT improved\n",
      "Validation loss 0.9995223047459848 NOT improved\n",
      "Validation loss 0.9995598768003963 NOT improved\n",
      "Validation loss 0.9995847084978584 NOT improved\n",
      "Validation loss 0.9992538338257129 NOT improved\n",
      "Validation loss 0.9986130261545193 NOT improved\n",
      "Validation loss 0.9985687209140619 NOT improved\n",
      "Validation loss 0.9990596497368456 NOT improved\n",
      "Validation loss 0.9951207417198868 NOT improved\n",
      "Validation loss 0.99666222949134 NOT improved\n",
      "Validation loss 0.997544030634649 NOT improved\n",
      "Validation loss 0.992975685504724 NOT improved\n",
      "Validation loss 0.9890696243528385 NOT improved\n",
      "Validation loss 0.9932092547598755 NOT improved\n",
      "Validation loss 0.9967371649677564 NOT improved\n",
      "Validation loss 0.9783940227412153 NOT improved\n",
      "Validation loss 0.9704203055950347 NOT improved\n",
      "Validation loss 0.9120677734380789 NOT improved\n",
      "Validation loss improved from 0.8751485329310009 to 0.8586396138009346\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 7.034696340560913 seconds ---\n",
      "Validation loss 0.9852035938296189 NOT improved\n",
      "Validation loss 0.9057236556726029 NOT improved\n",
      "Validation loss 0.9818189363814831 NOT improved\n",
      "Validation loss 0.9602788405170726 NOT improved\n",
      "Validation loss 0.9073313658372413 NOT improved\n",
      "Validation loss 0.9652304355041283 NOT improved\n",
      "Validation loss 0.9635308320902232 NOT improved\n",
      "Validation loss 0.9373505097179694 NOT improved\n",
      "Validation loss 0.8633310174061809 NOT improved\n",
      "Validation loss 0.9013206043005667 NOT improved\n",
      "Validation loss 3.0039464421072677 NOT improved\n",
      "Early stopping\n",
      "Training round 2\n",
      "Validation loss 0.9997521818973766 NOT improved\n",
      "Validation loss 0.9998260528029579 NOT improved\n",
      "Validation loss 0.9998250721120779 NOT improved\n",
      "Validation loss 0.9998365515840782 NOT improved\n",
      "Validation loss 0.9998510334916012 NOT improved\n",
      "Validation loss 0.9998464765592422 NOT improved\n",
      "Validation loss 0.999829287183222 NOT improved\n",
      "Validation loss 0.9998297573755449 NOT improved\n",
      "Validation loss 0.9998207115909147 NOT improved\n",
      "Validation loss 0.999799896450211 NOT improved\n",
      "Validation loss 0.9997476419045895 NOT improved\n",
      "Validation loss 0.9997293528692378 NOT improved\n",
      "Validation loss 0.9996858125739643 NOT improved\n",
      "Validation loss 0.9997147426512254 NOT improved\n",
      "Validation loss 0.9995456783248494 NOT improved\n",
      "Validation loss 0.99958493963205 NOT improved\n",
      "Validation loss 0.9993927960284762 NOT improved\n",
      "Validation loss 0.999115258760801 NOT improved\n",
      "Validation loss 0.9992519635733581 NOT improved\n",
      "Validation loss 0.9988388635893345 NOT improved\n",
      "Validation loss 0.9984540944120381 NOT improved\n",
      "Validation loss 0.9977819469156595 NOT improved\n",
      "Validation loss 0.998859359620174 NOT improved\n",
      "Validation loss 0.9982865184185732 NOT improved\n",
      "Validation loss 0.9975582753452767 NOT improved\n",
      "Validation loss 0.9968434646617611 NOT improved\n",
      "Validation loss 0.9887518410208139 NOT improved\n",
      "Validation loss 0.9939464073163194 NOT improved\n",
      "Validation loss 0.9933964129865763 NOT improved\n",
      "Validation loss 0.9671774423059197 NOT improved\n",
      "Validation loss 0.9685866557591768 NOT improved\n",
      "Validation loss 0.982198080845973 NOT improved\n",
      "Validation loss 0.9842692285347675 NOT improved\n",
      "Validation loss 0.9844327622938855 NOT improved\n",
      "Validation loss 0.9601425518676262 NOT improved\n",
      "Validation loss 0.9845652260310682 NOT improved\n",
      "Validation loss 0.9209763560057606 NOT improved\n",
      "Validation loss 0.9511230838890461 NOT improved\n",
      "Validation loss 0.859078258831828 NOT improved\n",
      "Validation loss 0.962964101605242 NOT improved\n",
      "Validation loss 0.9608638682510785 NOT improved\n",
      "Validation loss 0.9184038676710882 NOT improved\n",
      "Validation loss 0.9500866425363517 NOT improved\n",
      "Validation loss 0.9227474737719871 NOT improved\n",
      "Validation loss 0.8792057516085737 NOT improved\n",
      "Validation loss 1.035815705112955 NOT improved\n",
      "Early stopping\n",
      "Training round 3\n",
      "Validation loss 0.9995686610531161 NOT improved\n",
      "Validation loss 0.9997847910987723 NOT improved\n",
      "Validation loss 0.9998618591345914 NOT improved\n",
      "Validation loss 0.9998820379102213 NOT improved\n",
      "Validation loss 0.9998877374254007 NOT improved\n",
      "Validation loss 0.9998957446064909 NOT improved\n",
      "Validation loss 0.999901518338749 NOT improved\n",
      "Validation loss 0.9998896073714978 NOT improved\n",
      "Validation loss 0.9998828986192956 NOT improved\n",
      "Validation loss 0.9998510199411552 NOT improved\n",
      "Validation loss 0.99983989672295 NOT improved\n",
      "Validation loss 0.999845814557285 NOT improved\n",
      "Validation loss 0.9998185273137336 NOT improved\n",
      "Validation loss 0.9997384088054171 NOT improved\n",
      "Validation loss 0.9997166358208059 NOT improved\n",
      "Validation loss 0.9996211017525789 NOT improved\n",
      "Validation loss 0.9995500645557369 NOT improved\n",
      "Validation loss 0.9994443993173099 NOT improved\n",
      "Validation loss 0.9994253655539134 NOT improved\n",
      "Validation loss 0.9993928315301863 NOT improved\n",
      "Validation loss 0.9985089340588227 NOT improved\n",
      "Validation loss 0.9980127432464929 NOT improved\n",
      "Validation loss 0.9988418617391143 NOT improved\n",
      "Validation loss 0.9970756204328298 NOT improved\n",
      "Validation loss 0.9953751383529614 NOT improved\n",
      "Validation loss 0.9977308031803599 NOT improved\n",
      "Validation loss 0.9934199576084701 NOT improved\n",
      "Validation loss 0.9964996978133798 NOT improved\n",
      "Validation loss 0.9911520284609109 NOT improved\n",
      "Validation loss 0.9820743250328698 NOT improved\n",
      "Validation loss 0.9859536747971976 NOT improved\n",
      "Validation loss 0.9875506214660806 NOT improved\n",
      "Validation loss 0.949490265895642 NOT improved\n",
      "Validation loss 0.9757379869318386 NOT improved\n",
      "Validation loss 0.9891568762793627 NOT improved\n",
      "Validation loss 0.949800555304888 NOT improved\n",
      "Validation loss 0.9383148718051145 NOT improved\n",
      "Validation loss 0.9439811293614199 NOT improved\n",
      "Validation loss 0.8974342726903011 NOT improved\n",
      "Validation loss 0.9127945509266152 NOT improved\n",
      "Validation loss 0.9517842695821694 NOT improved\n",
      "Validation loss 0.8920636599234222 NOT improved\n",
      "Validation loss 0.8700964084591386 NOT improved\n",
      "Validation loss 0.9948657836590021 NOT improved\n",
      "Validation loss 3.027375485586373 NOT improved\n",
      "Early stopping\n",
      "Training round 4\n",
      "Validation loss 0.999771207341442 NOT improved\n",
      "Validation loss 0.9998198021071651 NOT improved\n",
      "Validation loss 0.9998598117845156 NOT improved\n",
      "Validation loss 0.9998814022949777 NOT improved\n",
      "Validation loss 0.9998798921151453 NOT improved\n",
      "Validation loss 0.9998743947761364 NOT improved\n",
      "Validation loss 0.9998753514080818 NOT improved\n",
      "Validation loss 0.9998488349128857 NOT improved\n",
      "Validation loss 0.9998579144091669 NOT improved\n",
      "Validation loss 0.9998582965099373 NOT improved\n",
      "Validation loss 0.9998079980964087 NOT improved\n",
      "Validation loss 0.9997376479563838 NOT improved\n",
      "Validation loss 0.9997389468409743 NOT improved\n",
      "Validation loss 0.9997168283282206 NOT improved\n",
      "Validation loss 0.9996414416400052 NOT improved\n",
      "Validation loss 0.9994072481959825 NOT improved\n",
      "Validation loss 0.9992735486280274 NOT improved\n",
      "Validation loss 0.9995340414992047 NOT improved\n",
      "Validation loss 0.9990247663184774 NOT improved\n",
      "Validation loss 0.998742223444462 NOT improved\n",
      "Validation loss 0.997017365000383 NOT improved\n",
      "Validation loss 0.9990857730052394 NOT improved\n",
      "Validation loss 0.9972701016151883 NOT improved\n",
      "Validation loss 0.9963886077686683 NOT improved\n",
      "Validation loss 0.9966355784696231 NOT improved\n",
      "Validation loss 0.9814111374932161 NOT improved\n",
      "Validation loss 0.9959061545195258 NOT improved\n",
      "Validation loss 0.982759053851091 NOT improved\n",
      "Validation loss 0.9832590019985137 NOT improved\n",
      "Validation loss 0.994612714030129 NOT improved\n",
      "Validation loss 0.9972320678664943 NOT improved\n",
      "Validation loss 0.9647632399104321 NOT improved\n",
      "Validation loss 0.9846288086778104 NOT improved\n",
      "Validation loss 0.8950777024518265 NOT improved\n",
      "Validation loss 0.9776390517014718 NOT improved\n",
      "Validation loss 0.9096846280885021 NOT improved\n",
      "Validation loss 0.9918538739040305 NOT improved\n",
      "Validation loss 0.9639195421680928 NOT improved\n",
      "Validation loss improved from 0.8586396138009346 to 0.8412161481627674\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/alt_lead3\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/alt_lead3/assets\n",
      "--- 6.587463617324829 seconds ---\n",
      "Validation loss 0.9413231981618069 NOT improved\n",
      "Validation loss 0.8901662107382262 NOT improved\n",
      "Validation loss 0.8945564592166981 NOT improved\n",
      "Validation loss 1.1702259020241361 NOT improved\n",
      "Early stopping\n",
      "Training round 5\n",
      "Validation loss 0.9996685283283304 NOT improved\n",
      "Validation loss 0.9997468461823987 NOT improved\n",
      "Validation loss 0.999831473096692 NOT improved\n",
      "Validation loss 0.9998750421884646 NOT improved\n",
      "Validation loss 0.9998841940977224 NOT improved\n",
      "Validation loss 0.9998953170745105 NOT improved\n",
      "Validation loss 0.9998715126982829 NOT improved\n",
      "Validation loss 0.9998781163858986 NOT improved\n",
      "Validation loss 0.999863761860127 NOT improved\n",
      "Validation loss 0.9998510236217018 NOT improved\n",
      "Validation loss 0.9998187040878609 NOT improved\n",
      "Validation loss 0.9998379886712662 NOT improved\n",
      "Validation loss 0.9998183092745689 NOT improved\n",
      "Validation loss 0.9997171222774299 NOT improved\n",
      "Validation loss 0.9997566669795267 NOT improved\n",
      "Validation loss 0.9997807534861984 NOT improved\n",
      "Validation loss 0.9997674650793357 NOT improved\n",
      "Validation loss 0.9994373327373448 NOT improved\n",
      "Validation loss 0.9993443461077094 NOT improved\n",
      "Validation loss 0.9992204221277823 NOT improved\n",
      "Validation loss 0.9981544435848304 NOT improved\n",
      "Validation loss 0.9961902900117359 NOT improved\n",
      "Validation loss 0.9978791400950826 NOT improved\n",
      "Validation loss 0.9989316327346272 NOT improved\n",
      "Validation loss 0.9960729824431616 NOT improved\n",
      "Validation loss 0.9947077308996937 NOT improved\n",
      "Validation loss 0.9960931078056876 NOT improved\n",
      "Validation loss 0.9839494881591443 NOT improved\n",
      "Validation loss 0.9897315694613421 NOT improved\n",
      "Validation loss 0.987876527979883 NOT improved\n",
      "Validation loss 0.9730017394176105 NOT improved\n",
      "Validation loss 0.9945625344456177 NOT improved\n",
      "Validation loss 0.9511828221138731 NOT improved\n",
      "Validation loss 0.9893493194327925 NOT improved\n",
      "Validation loss 0.9145812249808329 NOT improved\n",
      "Validation loss 0.9598150254043943 NOT improved\n",
      "Validation loss 0.9592993117427884 NOT improved\n",
      "Validation loss 0.9929685524077349 NOT improved\n",
      "Validation loss 0.8460970874052964 NOT improved\n",
      "Validation loss 0.8705644225712637 NOT improved\n",
      "Validation loss 1.0407849592886016 NOT improved\n",
      "Early stopping\n",
      "Training round 6\n",
      "Validation loss 0.9995125886635111 NOT improved\n",
      "Validation loss 0.9997291635243595 NOT improved\n",
      "Validation loss 0.9998233551084312 NOT improved\n",
      "Validation loss 0.9998565816116791 NOT improved\n",
      "Validation loss 0.9998701781904575 NOT improved\n",
      "Validation loss 0.9998730183440429 NOT improved\n",
      "Validation loss 0.9998582169246585 NOT improved\n",
      "Validation loss 0.9998717084242389 NOT improved\n",
      "Validation loss 0.9998569122091938 NOT improved\n",
      "Validation loss 0.9998212138058536 NOT improved\n",
      "Validation loss 0.9998200310484812 NOT improved\n",
      "Validation loss 0.9997297912782431 NOT improved\n",
      "Validation loss 0.9997766768112296 NOT improved\n",
      "Validation loss 0.9997236686124509 NOT improved\n",
      "Validation loss 0.9997113203058396 NOT improved\n",
      "Validation loss 0.99958830986349 NOT improved\n",
      "Validation loss 0.9993031676994982 NOT improved\n",
      "Validation loss 0.9994083260617229 NOT improved\n",
      "Validation loss 0.9985616982966461 NOT improved\n",
      "Validation loss 0.9981917465923025 NOT improved\n",
      "Validation loss 0.9985722309044184 NOT improved\n",
      "Validation loss 0.9989879447182652 NOT improved\n",
      "Validation loss 0.9938385929149867 NOT improved\n",
      "Validation loss 0.9963781576043804 NOT improved\n",
      "Validation loss 0.9952619830572575 NOT improved\n",
      "Validation loss 0.9966586452952837 NOT improved\n",
      "Validation loss 0.9868385506222851 NOT improved\n",
      "Validation loss 0.9837736473859285 NOT improved\n",
      "Validation loss 0.9877135558292421 NOT improved\n",
      "Validation loss 0.9632506145289277 NOT improved\n",
      "Validation loss 0.9900387193991866 NOT improved\n",
      "Validation loss 0.9759183519089357 NOT improved\n",
      "Validation loss 0.8660378655572063 NOT improved\n",
      "Validation loss 0.9558914047003181 NOT improved\n",
      "Validation loss 0.9947836643721162 NOT improved\n",
      "Validation loss 0.8543862180828713 NOT improved\n",
      "Validation loss 0.9202528802028648 NOT improved\n",
      "Validation loss 0.8928725664929168 NOT improved\n",
      "Validation loss 1.557964563245833 NOT improved\n",
      "Early stopping\n",
      "Training round 7\n",
      "Validation loss 0.9997589448334541 NOT improved\n",
      "Validation loss 0.9997923159885517 NOT improved\n",
      "Validation loss 0.9998361479110327 NOT improved\n",
      "Validation loss 0.999848063242462 NOT improved\n",
      "Validation loss 0.9998511067450706 NOT improved\n",
      "Validation loss 0.9998341871026246 NOT improved\n",
      "Validation loss 0.9998506731490481 NOT improved\n",
      "Validation loss 0.9998305240722052 NOT improved\n",
      "Validation loss 0.9997838949030313 NOT improved\n",
      "Validation loss 0.9998096689293777 NOT improved\n",
      "Validation loss 0.9997157976912416 NOT improved\n",
      "Validation loss 0.999746005832949 NOT improved\n",
      "Validation loss 0.9996775648863752 NOT improved\n",
      "Validation loss 0.9997494266846689 NOT improved\n",
      "Validation loss 0.9995233005463398 NOT improved\n",
      "Validation loss 0.9994934462551579 NOT improved\n",
      "Validation loss 0.9994849693170385 NOT improved\n",
      "Validation loss 0.998199537525324 NOT improved\n",
      "Validation loss 0.998810518881676 NOT improved\n",
      "Validation loss 0.997989490244983 NOT improved\n",
      "Validation loss 0.9983783092571611 NOT improved\n",
      "Validation loss 0.9948638555798847 NOT improved\n",
      "Validation loss 0.9979978627640718 NOT improved\n",
      "Validation loss 0.9966016681394119 NOT improved\n",
      "Validation loss 0.9884984715711537 NOT improved\n",
      "Validation loss 0.9926559040382392 NOT improved\n",
      "Validation loss 0.9948089291970466 NOT improved\n",
      "Validation loss 0.9867065929146808 NOT improved\n",
      "Validation loss 0.9915576336862564 NOT improved\n",
      "Validation loss 0.9845585887055818 NOT improved\n",
      "Validation loss 0.9181024152885574 NOT improved\n",
      "Validation loss 0.9464833476244591 NOT improved\n",
      "Validation loss 0.9581087079492545 NOT improved\n",
      "Validation loss 0.9774374820773221 NOT improved\n",
      "Validation loss 0.9837999100512292 NOT improved\n",
      "Validation loss 0.9528665763620836 NOT improved\n",
      "Validation loss 0.8729116790917516 NOT improved\n",
      "Validation loss 0.9670119087095694 NOT improved\n",
      "Validation loss 0.8691601649998923 NOT improved\n",
      "Validation loss 0.8600401486253114 NOT improved\n",
      "Validation loss 1.5536920597607207 NOT improved\n",
      "Early stopping\n",
      "Training round 8\n",
      "Validation loss 0.9993823698610297 NOT improved\n",
      "Validation loss 0.9997110640152 NOT improved\n",
      "Validation loss 0.9998234552863062 NOT improved\n",
      "Validation loss 0.9998611098897812 NOT improved\n",
      "Validation loss 0.9998701771506281 NOT improved\n",
      "Validation loss 0.9998918442005134 NOT improved\n",
      "Validation loss 0.9998706013541747 NOT improved\n",
      "Validation loss 0.9998217277883458 NOT improved\n",
      "Validation loss 0.9998167032694828 NOT improved\n",
      "Validation loss 0.9998210142644478 NOT improved\n",
      "Validation loss 0.999818860272279 NOT improved\n",
      "Validation loss 0.9997788430481775 NOT improved\n",
      "Validation loss 0.9997348284270962 NOT improved\n",
      "Validation loss 0.999609940079334 NOT improved\n",
      "Validation loss 0.9994268012123053 NOT improved\n",
      "Validation loss 0.9995467157046122 NOT improved\n",
      "Validation loss 0.999471148230105 NOT improved\n",
      "Validation loss 0.9991223015405049 NOT improved\n",
      "Validation loss 0.9990485684686335 NOT improved\n",
      "Validation loss 0.9980538208985801 NOT improved\n",
      "Validation loss 0.9991732188437815 NOT improved\n",
      "Validation loss 0.9969134479660533 NOT improved\n",
      "Validation loss 0.9975263033776115 NOT improved\n",
      "Validation loss 0.9939941850211976 NOT improved\n",
      "Validation loss 0.9869373618122284 NOT improved\n",
      "Validation loss 0.9890224966137133 NOT improved\n",
      "Validation loss 0.9666234328164307 NOT improved\n",
      "Validation loss 0.9881568122068571 NOT improved\n",
      "Validation loss 0.9859382625722521 NOT improved\n",
      "Validation loss 0.9919227527835045 NOT improved\n",
      "Validation loss 0.9920800735250773 NOT improved\n",
      "Validation loss 0.9682237264593393 NOT improved\n",
      "Validation loss 0.9198968084371394 NOT improved\n",
      "Validation loss 0.8654036601401546 NOT improved\n",
      "Validation loss 0.942444553606206 NOT improved\n",
      "Validation loss 0.8714675081226938 NOT improved\n",
      "Validation loss 0.8594354889343059 NOT improved\n",
      "Validation loss 0.9570452435272415 NOT improved\n",
      "Validation loss 0.9351038243044494 NOT improved\n",
      "Validation loss 0.930835073905575 NOT improved\n",
      "Validation loss 0.8984607758119318 NOT improved\n",
      "Validation loss 0.9067636786459217 NOT improved\n",
      "Validation loss 0.9759468285155017 NOT improved\n",
      "Validation loss 0.9190284732836626 NOT improved\n",
      "Validation loss 0.9678751472480946 NOT improved\n",
      "Validation loss 0.8739024272968227 NOT improved\n",
      "Validation loss 0.8746947709497677 NOT improved\n",
      "Validation loss 1.0307663789581583 NOT improved\n",
      "Early stopping\n",
      "Training round 9\n",
      "Validation loss 0.9996848808347722 NOT improved\n",
      "Validation loss 0.9997747860753791 NOT improved\n",
      "Validation loss 0.9998327918928462 NOT improved\n",
      "Validation loss 0.9998552982044833 NOT improved\n",
      "Validation loss 0.9998724847159233 NOT improved\n",
      "Validation loss 0.9998633230366856 NOT improved\n",
      "Validation loss 0.9998602994322451 NOT improved\n",
      "Validation loss 0.9998587916098867 NOT improved\n",
      "Validation loss 0.999852138728856 NOT improved\n",
      "Validation loss 0.9998273009041271 NOT improved\n",
      "Validation loss 0.9998067110635003 NOT improved\n",
      "Validation loss 0.999753339406405 NOT improved\n",
      "Validation loss 0.9997497484942973 NOT improved\n",
      "Validation loss 0.9996662217805184 NOT improved\n",
      "Validation loss 0.9997480368712897 NOT improved\n",
      "Validation loss 0.9995721426256724 NOT improved\n",
      "Validation loss 0.999498344010559 NOT improved\n",
      "Validation loss 0.9996598675711511 NOT improved\n",
      "Validation loss 0.998981998239295 NOT improved\n",
      "Validation loss 0.9993513671535209 NOT improved\n",
      "Validation loss 0.9977277261836711 NOT improved\n",
      "Validation loss 0.9987980736205454 NOT improved\n",
      "Validation loss 0.9976386095778166 NOT improved\n",
      "Validation loss 0.9967115120085039 NOT improved\n",
      "Validation loss 0.9935309282530957 NOT improved\n",
      "Validation loss 0.997317780095119 NOT improved\n",
      "Validation loss 0.9881367506507818 NOT improved\n",
      "Validation loss 0.987948729351464 NOT improved\n",
      "Validation loss 0.9986802977714407 NOT improved\n",
      "Validation loss 0.9756377270163628 NOT improved\n",
      "Validation loss 0.8654424508438663 NOT improved\n",
      "Validation loss 0.9803847851737775 NOT improved\n",
      "Validation loss 0.9696900506840515 NOT improved\n",
      "Validation loss 0.9733406884717778 NOT improved\n",
      "Validation loss 0.9647011329980629 NOT improved\n",
      "Validation loss 1.1123751994735767 NOT improved\n",
      "Early stopping\n",
      "Training round 10\n",
      "Validation loss 0.9994978926628224 NOT improved\n",
      "Validation loss 0.9997668133560091 NOT improved\n",
      "Validation loss 0.9998646128001568 NOT improved\n",
      "Validation loss 0.9998703119433469 NOT improved\n",
      "Validation loss 0.9998771837637275 NOT improved\n",
      "Validation loss 0.999873275495659 NOT improved\n",
      "Validation loss 0.9998597086101043 NOT improved\n",
      "Validation loss 0.9998734662672694 NOT improved\n",
      "Validation loss 0.9998425510874305 NOT improved\n",
      "Validation loss 0.9998183995218664 NOT improved\n",
      "Validation loss 0.9997870993465559 NOT improved\n",
      "Validation loss 0.9997959801624037 NOT improved\n",
      "Validation loss 0.9996781899157965 NOT improved\n",
      "Validation loss 0.9996823050130882 NOT improved\n",
      "Validation loss 0.999649575709573 NOT improved\n",
      "Validation loss 0.9995741621702466 NOT improved\n",
      "Validation loss 0.999613656493488 NOT improved\n",
      "Validation loss 0.9993994681316504 NOT improved\n",
      "Validation loss 0.9991621839173628 NOT improved\n",
      "Validation loss 0.9991138647947861 NOT improved\n",
      "Validation loss 0.9987457571963404 NOT improved\n",
      "Validation loss 0.9980454316939864 NOT improved\n",
      "Validation loss 0.9975404265935015 NOT improved\n",
      "Validation loss 0.9975784334543465 NOT improved\n",
      "Validation loss 0.9925940699207012 NOT improved\n",
      "Validation loss 0.99623799234777 NOT improved\n",
      "Validation loss 0.9895687313607735 NOT improved\n",
      "Validation loss 0.993192755703492 NOT improved\n",
      "Validation loss 0.9839379977387903 NOT improved\n",
      "Validation loss 0.9907388160408069 NOT improved\n",
      "Validation loss 0.9643989237549052 NOT improved\n",
      "Validation loss 0.9787926505636688 NOT improved\n",
      "Validation loss 0.9935009384626068 NOT improved\n",
      "Validation loss 0.9646539074171383 NOT improved\n",
      "Validation loss 0.9479208213617178 NOT improved\n",
      "Validation loss 0.9268030653080158 NOT improved\n",
      "Validation loss 0.9307989043256971 NOT improved\n",
      "Validation loss 0.9362101364408041 NOT improved\n",
      "Validation loss 0.9102377165692555 NOT improved\n",
      "Validation loss 0.9114687919316574 NOT improved\n",
      "Validation loss 0.9062140794499517 NOT improved\n",
      "Validation loss 1.089465106640243 NOT improved\n",
      "Early stopping\n",
      "Training round 11\n",
      "Validation loss 0.9996128996025315 NOT improved\n",
      "Validation loss 0.9997665197388313 NOT improved\n",
      "Validation loss 0.9998147157237093 NOT improved\n",
      "Validation loss 0.9998477856233458 NOT improved\n",
      "Validation loss 0.9998690949173555 NOT improved\n",
      "Validation loss 0.9998585447619868 NOT improved\n",
      "Validation loss 0.9998466732323887 NOT improved\n",
      "Validation loss 0.9998545370719046 NOT improved\n",
      "Validation loss 0.9998145757789525 NOT improved\n",
      "Validation loss 0.9998228991505307 NOT improved\n",
      "Validation loss 0.9998334713366798 NOT improved\n",
      "Validation loss 0.9997817560887662 NOT improved\n",
      "Validation loss 0.9997390465760394 NOT improved\n",
      "Validation loss 0.9998071634595713 NOT improved\n",
      "Validation loss 0.9996664249798235 NOT improved\n",
      "Validation loss 0.9996496605788572 NOT improved\n",
      "Validation loss 0.9994686268633486 NOT improved\n",
      "Validation loss 0.9992986272314694 NOT improved\n",
      "Validation loss 0.9992588543677593 NOT improved\n",
      "Validation loss 0.999076975488745 NOT improved\n",
      "Validation loss 0.9988233487928453 NOT improved\n",
      "Validation loss 0.9967753586524306 NOT improved\n",
      "Validation loss 0.9975898495178022 NOT improved\n",
      "Validation loss 0.996065049083249 NOT improved\n",
      "Validation loss 0.994950055990823 NOT improved\n",
      "Validation loss 0.9964876300139128 NOT improved\n",
      "Validation loss 0.9963633570214157 NOT improved\n",
      "Validation loss 0.9943514351484422 NOT improved\n",
      "Validation loss 0.9617108402473651 NOT improved\n",
      "Validation loss 0.9556588958727934 NOT improved\n",
      "Validation loss 0.9736453726450652 NOT improved\n",
      "Validation loss 0.9919204219507437 NOT improved\n",
      "Validation loss 0.9698429383732043 NOT improved\n",
      "Validation loss 0.9574612511506655 NOT improved\n",
      "Validation loss 0.9693737752542757 NOT improved\n",
      "Validation loss 0.9196229798017472 NOT improved\n",
      "Validation loss 0.8777677544395086 NOT improved\n",
      "Validation loss 0.9115508007280008 NOT improved\n",
      "Validation loss 0.9801818391528957 NOT improved\n",
      "Validation loss 0.9163369757649239 NOT improved\n",
      "Validation loss 1.8625880295443733 NOT improved\n",
      "Early stopping\n",
      "Training round 12\n",
      "Validation loss 0.9996921037499785 NOT improved\n",
      "Validation loss 0.9997992412775067 NOT improved\n",
      "Validation loss 0.9998601335248546 NOT improved\n",
      "Validation loss 0.9998790980502783 NOT improved\n",
      "Validation loss 0.9998820491414273 NOT improved\n",
      "Validation loss 0.9998879914026162 NOT improved\n",
      "Validation loss 0.9998871769910115 NOT improved\n",
      "Validation loss 0.9998682311701039 NOT improved\n",
      "Validation loss 0.9998608869658548 NOT improved\n",
      "Validation loss 0.9998519185408603 NOT improved\n",
      "Validation loss 0.9998103804495881 NOT improved\n",
      "Validation loss 0.9998062954886222 NOT improved\n",
      "Validation loss 0.999760310812886 NOT improved\n",
      "Validation loss 0.9997712216443403 NOT improved\n",
      "Validation loss 0.9996378985932017 NOT improved\n",
      "Validation loss 0.9996420270648695 NOT improved\n",
      "Validation loss 0.9997466232680604 NOT improved\n",
      "Validation loss 0.999398774744548 NOT improved\n",
      "Validation loss 0.9994840791958051 NOT improved\n",
      "Validation loss 0.9988510205767088 NOT improved\n",
      "Validation loss 0.9992723890465448 NOT improved\n",
      "Validation loss 0.9994226612832455 NOT improved\n",
      "Validation loss 0.9974591806282084 NOT improved\n",
      "Validation loss 0.9983291753018827 NOT improved\n",
      "Validation loss 0.9943642113684956 NOT improved\n",
      "Validation loss 0.994932765428185 NOT improved\n",
      "Validation loss 0.9958299112449308 NOT improved\n",
      "Validation loss 0.9942922013723022 NOT improved\n",
      "Validation loss 0.9978497630869153 NOT improved\n",
      "Validation loss 0.9973262202616417 NOT improved\n",
      "Validation loss 0.9938334393222105 NOT improved\n",
      "Validation loss 0.9287908296245125 NOT improved\n",
      "Validation loss 0.9840940131236295 NOT improved\n",
      "Validation loss 0.8472575435996481 NOT improved\n",
      "Validation loss 0.9919648343828273 NOT improved\n",
      "Validation loss 0.8552078145955178 NOT improved\n",
      "Validation loss 0.8861033134980747 NOT improved\n",
      "Validation loss 0.9296634896639011 NOT improved\n",
      "Validation loss 0.858731942985985 NOT improved\n",
      "Validation loss 0.866718006576014 NOT improved\n",
      "Validation loss 0.8717469353451852 NOT improved\n",
      "Validation loss 1.1800083863125403 NOT improved\n",
      "Early stopping\n",
      "Training round 13\n",
      "Validation loss 0.9997263656548938 NOT improved\n",
      "Validation loss 0.9997647791849543 NOT improved\n",
      "Validation loss 0.9998289069913694 NOT improved\n",
      "Validation loss 0.9998464047416902 NOT improved\n",
      "Validation loss 0.9998562096036302 NOT improved\n",
      "Validation loss 0.9998675797920666 NOT improved\n",
      "Validation loss 0.9998722030020278 NOT improved\n",
      "Validation loss 0.9998572586749579 NOT improved\n",
      "Validation loss 0.9998434865041383 NOT improved\n",
      "Validation loss 0.9998461556658013 NOT improved\n",
      "Validation loss 0.9998570680003307 NOT improved\n",
      "Validation loss 0.999819854534924 NOT improved\n",
      "Validation loss 0.9997595317939278 NOT improved\n",
      "Validation loss 0.999774407907479 NOT improved\n",
      "Validation loss 0.9997586745251799 NOT improved\n",
      "Validation loss 0.9995518217371268 NOT improved\n",
      "Validation loss 0.9995765317434576 NOT improved\n",
      "Validation loss 0.9994315684531997 NOT improved\n",
      "Validation loss 0.9991418439610438 NOT improved\n",
      "Validation loss 0.9991371809897519 NOT improved\n",
      "Validation loss 0.9984781607514788 NOT improved\n",
      "Validation loss 0.9981800940402868 NOT improved\n",
      "Validation loss 0.9984131573230397 NOT improved\n",
      "Validation loss 0.9988030086422599 NOT improved\n",
      "Validation loss 0.9978830531021833 NOT improved\n",
      "Validation loss 0.9913802021622605 NOT improved\n",
      "Validation loss 0.9876870889275885 NOT improved\n",
      "Validation loss 0.9888144849903877 NOT improved\n",
      "Validation loss 0.9903367772601274 NOT improved\n",
      "Validation loss 0.8967442057180272 NOT improved\n",
      "Validation loss 0.9917698800428932 NOT improved\n",
      "Validation loss 0.9931579662549832 NOT improved\n",
      "Validation loss 0.9778816718262697 NOT improved\n",
      "Validation loss 0.9397518539292782 NOT improved\n",
      "Validation loss 0.9427758383115724 NOT improved\n",
      "Validation loss 0.9497098793945358 NOT improved\n",
      "Validation loss 0.8756230649793552 NOT improved\n",
      "Validation loss 0.9324479697223071 NOT improved\n",
      "Validation loss 1.1922524112719626 NOT improved\n",
      "Early stopping\n",
      "Training round 14\n",
      "Validation loss 0.9996888419017078 NOT improved\n",
      "Validation loss 0.9998068327273187 NOT improved\n",
      "Validation loss 0.9998318565538817 NOT improved\n",
      "Validation loss 0.9998454640060103 NOT improved\n",
      "Validation loss 0.9998747898994528 NOT improved\n",
      "Validation loss 0.9998871834487695 NOT improved\n",
      "Validation loss 0.9998638984295494 NOT improved\n",
      "Validation loss 0.9998639902749501 NOT improved\n",
      "Validation loss 0.9998542571680884 NOT improved\n",
      "Validation loss 0.9998509133414735 NOT improved\n",
      "Validation loss 0.9998207998984868 NOT improved\n",
      "Validation loss 0.9998055748359765 NOT improved\n",
      "Validation loss 0.9997642402386947 NOT improved\n",
      "Validation loss 0.9995291703484368 NOT improved\n",
      "Validation loss 0.999704229854672 NOT improved\n",
      "Validation loss 0.9997474962715108 NOT improved\n",
      "Validation loss 0.9994822702767575 NOT improved\n",
      "Validation loss 0.9995648462185379 NOT improved\n",
      "Validation loss 0.9986833142933055 NOT improved\n",
      "Validation loss 0.9989555658793597 NOT improved\n",
      "Validation loss 0.9965535983231735 NOT improved\n",
      "Validation loss 0.9977045161626212 NOT improved\n",
      "Validation loss 0.9983644639796492 NOT improved\n",
      "Validation loss 0.9942219839447293 NOT improved\n",
      "Validation loss 0.9963078660227387 NOT improved\n",
      "Validation loss 0.997175039998903 NOT improved\n",
      "Validation loss 0.9949275115673379 NOT improved\n",
      "Validation loss 0.9913878060658855 NOT improved\n",
      "Validation loss 0.964172361050466 NOT improved\n",
      "Validation loss 0.9445926233248527 NOT improved\n",
      "Validation loss 0.9661680991107681 NOT improved\n",
      "Validation loss 0.987698845532883 NOT improved\n",
      "Validation loss 0.989428068842949 NOT improved\n",
      "Validation loss 0.9005062377855791 NOT improved\n",
      "Validation loss 0.9753102351490184 NOT improved\n",
      "Validation loss 0.867611662011223 NOT improved\n",
      "Validation loss 0.9527246558543557 NOT improved\n",
      "Validation loss 0.976795273963641 NOT improved\n",
      "Validation loss 1.3945440198172332 NOT improved\n",
      "Early stopping\n",
      "Training round 15\n",
      "Validation loss 0.9994743221864527 NOT improved\n",
      "Validation loss 0.9997757206212616 NOT improved\n",
      "Validation loss 0.9998514270348556 NOT improved\n",
      "Validation loss 0.9998716105976253 NOT improved\n",
      "Validation loss 0.9998702514885629 NOT improved\n",
      "Validation loss 0.9998833734190565 NOT improved\n",
      "Validation loss 0.9998716175331868 NOT improved\n",
      "Validation loss 0.9998622138193043 NOT improved\n",
      "Validation loss 0.9998473670965962 NOT improved\n",
      "Validation loss 0.9998249033033965 NOT improved\n",
      "Validation loss 0.9998277622245486 NOT improved\n",
      "Validation loss 0.9997521680387115 NOT improved\n",
      "Validation loss 0.9997734508897191 NOT improved\n",
      "Validation loss 0.9996772723060033 NOT improved\n",
      "Validation loss 0.9996379543455454 NOT improved\n",
      "Validation loss 0.9995157653105888 NOT improved\n",
      "Validation loss 0.9990159247210743 NOT improved\n",
      "Validation loss 0.9994156783670141 NOT improved\n",
      "Validation loss 0.9985564696442452 NOT improved\n",
      "Validation loss 0.998563955852212 NOT improved\n",
      "Validation loss 0.9975021877152617 NOT improved\n",
      "Validation loss 0.998071901000727 NOT improved\n",
      "Validation loss 0.995587881659572 NOT improved\n",
      "Validation loss 0.9938836384657997 NOT improved\n",
      "Validation loss 0.9973668539171395 NOT improved\n",
      "Validation loss 0.9901324624179559 NOT improved\n",
      "Validation loss 0.9748609723481905 NOT improved\n",
      "Validation loss 0.9929639700205608 NOT improved\n",
      "Validation loss 0.98434001385029 NOT improved\n",
      "Validation loss 0.9917391983484541 NOT improved\n",
      "Validation loss 0.9492204045985885 NOT improved\n",
      "Validation loss 0.9662555112099618 NOT improved\n",
      "Validation loss 0.8968554301493892 NOT improved\n",
      "Validation loss 0.9445153696200523 NOT improved\n",
      "Validation loss 0.98382960675567 NOT improved\n",
      "Validation loss 0.8783093780645982 NOT improved\n",
      "Validation loss 0.9529764304768245 NOT improved\n",
      "Validation loss 1.403000852737301 NOT improved\n",
      "Early stopping\n",
      "Training round 16\n",
      "Validation loss 0.9996243474017347 NOT improved\n",
      "Validation loss 0.9997580975088839 NOT improved\n",
      "Validation loss 0.999818505183085 NOT improved\n",
      "Validation loss 0.9998513809347553 NOT improved\n",
      "Validation loss 0.9998555365846883 NOT improved\n",
      "Validation loss 0.999865371273546 NOT improved\n",
      "Validation loss 0.9998623344911158 NOT improved\n",
      "Validation loss 0.9998543525388974 NOT improved\n",
      "Validation loss 0.999813070675258 NOT improved\n",
      "Validation loss 0.999818389783903 NOT improved\n",
      "Validation loss 0.9998381773008466 NOT improved\n",
      "Validation loss 0.999754663492628 NOT improved\n",
      "Validation loss 0.9997143361666944 NOT improved\n",
      "Validation loss 0.9996074233986711 NOT improved\n",
      "Validation loss 0.9996460212502906 NOT improved\n",
      "Validation loss 0.9995644116243949 NOT improved\n",
      "Validation loss 0.9993886380652096 NOT improved\n",
      "Validation loss 0.999336500352759 NOT improved\n",
      "Validation loss 0.999364815074161 NOT improved\n",
      "Validation loss 0.9967615147647232 NOT improved\n",
      "Validation loss 0.9980911599573583 NOT improved\n",
      "Validation loss 0.9982339299754672 NOT improved\n",
      "Validation loss 0.9964478743581066 NOT improved\n",
      "Validation loss 0.9974052589540399 NOT improved\n",
      "Validation loss 0.9937515304663154 NOT improved\n",
      "Validation loss 0.9970317014695688 NOT improved\n",
      "Validation loss 0.9923215557046373 NOT improved\n",
      "Validation loss 0.9948971637403669 NOT improved\n",
      "Validation loss 0.9583984766697291 NOT improved\n",
      "Validation loss 0.9182970209159258 NOT improved\n",
      "Validation loss 0.9701136849215908 NOT improved\n",
      "Validation loss 0.9690732995462703 NOT improved\n",
      "Validation loss 0.9533666227883684 NOT improved\n",
      "Validation loss 0.9450791793332327 NOT improved\n",
      "Validation loss 0.9891333738126539 NOT improved\n",
      "Validation loss 0.9554729461471789 NOT improved\n",
      "Validation loss 0.8929865659292912 NOT improved\n",
      "Validation loss 0.9896030957926848 NOT improved\n",
      "Validation loss 0.9912467077423441 NOT improved\n",
      "Validation loss 0.9279842852162218 NOT improved\n",
      "Validation loss 0.9781063236121008 NOT improved\n",
      "Validation loss 0.9769030592176564 NOT improved\n",
      "Validation loss 2.405856310512198 NOT improved\n",
      "Early stopping\n",
      "Training round 17\n",
      "Validation loss 0.9997035665325211 NOT improved\n",
      "Validation loss 0.9998213927351588 NOT improved\n",
      "Validation loss 0.999858727967299 NOT improved\n",
      "Validation loss 0.9998717607745612 NOT improved\n",
      "Validation loss 0.9998546716666346 NOT improved\n",
      "Validation loss 0.9998547067755919 NOT improved\n",
      "Validation loss 0.9998572537048119 NOT improved\n",
      "Validation loss 0.9998457324007093 NOT improved\n",
      "Validation loss 0.9998396717193587 NOT improved\n",
      "Validation loss 0.9997999142749112 NOT improved\n",
      "Validation loss 0.9997918062602567 NOT improved\n",
      "Validation loss 0.9997439415941743 NOT improved\n",
      "Validation loss 0.9996761490643258 NOT improved\n",
      "Validation loss 0.9996073651693136 NOT improved\n",
      "Validation loss 0.9996879484269852 NOT improved\n",
      "Validation loss 0.9995276947482823 NOT improved\n",
      "Validation loss 0.9991546416655008 NOT improved\n",
      "Validation loss 0.9993395297098729 NOT improved\n",
      "Validation loss 0.9984830053113566 NOT improved\n",
      "Validation loss 0.9991471873665976 NOT improved\n",
      "Validation loss 0.999113370834196 NOT improved\n",
      "Validation loss 0.9962537669969697 NOT improved\n",
      "Validation loss 0.9937326805353146 NOT improved\n",
      "Validation loss 0.9956864762091286 NOT improved\n",
      "Validation loss 0.9967647427166163 NOT improved\n",
      "Validation loss 0.9965226039619315 NOT improved\n",
      "Validation loss 0.9726324044361309 NOT improved\n",
      "Validation loss 0.9963251174441432 NOT improved\n",
      "Validation loss 0.9867604193465699 NOT improved\n",
      "Validation loss 0.9651496574753841 NOT improved\n",
      "Validation loss 0.983586566314654 NOT improved\n",
      "Validation loss 0.9501960842821431 NOT improved\n",
      "Validation loss 0.954156071638542 NOT improved\n",
      "Validation loss 0.8580155691953572 NOT improved\n",
      "Validation loss 0.9291137371299425 NOT improved\n",
      "Validation loss 0.8638521976333767 NOT improved\n",
      "Validation loss 0.9756262738186665 NOT improved\n",
      "Validation loss 0.8994587157042899 NOT improved\n",
      "Validation loss 0.9445847925769213 NOT improved\n",
      "Validation loss 0.900945295162409 NOT improved\n",
      "Validation loss 0.9883605190410903 NOT improved\n",
      "Validation loss 0.9782083379926367 NOT improved\n",
      "Validation loss 0.9344329349031487 NOT improved\n",
      "Validation loss 0.9945152768013412 NOT improved\n",
      "Validation loss 2.495091642251016 NOT improved\n",
      "Early stopping\n",
      "Training round 18\n",
      "Validation loss 0.9995241635766163 NOT improved\n",
      "Validation loss 0.9997951465272833 NOT improved\n",
      "Validation loss 0.9998673099861846 NOT improved\n",
      "Validation loss 0.9998861601312731 NOT improved\n",
      "Validation loss 0.9998826002744104 NOT improved\n",
      "Validation loss 0.9998951734279622 NOT improved\n",
      "Validation loss 0.999883670362789 NOT improved\n",
      "Validation loss 0.9998694377969913 NOT improved\n",
      "Validation loss 0.9998726632219079 NOT improved\n",
      "Validation loss 0.9998590030633986 NOT improved\n",
      "Validation loss 0.9998417283493303 NOT improved\n",
      "Validation loss 0.9997534353918205 NOT improved\n",
      "Validation loss 0.9997326381670938 NOT improved\n",
      "Validation loss 0.9996808418131616 NOT improved\n",
      "Validation loss 0.9996801004002401 NOT improved\n",
      "Validation loss 0.9996686226718999 NOT improved\n",
      "Validation loss 0.9995300795002864 NOT improved\n",
      "Validation loss 0.9993869611057811 NOT improved\n",
      "Validation loss 0.9992548920448985 NOT improved\n",
      "Validation loss 0.999157150826688 NOT improved\n",
      "Validation loss 0.9986596642694493 NOT improved\n",
      "Validation loss 0.9976534006628529 NOT improved\n",
      "Validation loss 0.9961538290966406 NOT improved\n",
      "Validation loss 0.9959745695648384 NOT improved\n",
      "Validation loss 0.9949812501539763 NOT improved\n",
      "Validation loss 0.9962884244161617 NOT improved\n",
      "Validation loss 0.9833551744062383 NOT improved\n",
      "Validation loss 0.9867953929231292 NOT improved\n",
      "Validation loss 0.9960386929445575 NOT improved\n",
      "Validation loss 0.9937902410646015 NOT improved\n",
      "Validation loss 0.9826418964722626 NOT improved\n",
      "Validation loss 0.981326630338299 NOT improved\n",
      "Validation loss 0.9388432778592213 NOT improved\n",
      "Validation loss 0.9773369346912391 NOT improved\n",
      "Validation loss 0.9803158431216209 NOT improved\n",
      "Validation loss 0.9031965034713562 NOT improved\n",
      "Validation loss 0.9771054198842044 NOT improved\n",
      "Validation loss 0.8892245476148104 NOT improved\n",
      "Validation loss 0.9021046171209397 NOT improved\n",
      "Validation loss 0.8961522891812265 NOT improved\n",
      "Validation loss 0.901071937488388 NOT improved\n",
      "Validation loss 1.2001037438539164 NOT improved\n",
      "Early stopping\n",
      "Training round 19\n",
      "Validation loss 0.9996394187042651 NOT improved\n",
      "Validation loss 0.9997930716628858 NOT improved\n",
      "Validation loss 0.9998609551502505 NOT improved\n",
      "Validation loss 0.9998936040368357 NOT improved\n",
      "Validation loss 0.9999030126705382 NOT improved\n",
      "Validation loss 0.9999046320645939 NOT improved\n",
      "Validation loss 0.999915137489082 NOT improved\n",
      "Validation loss 0.9998912088712588 NOT improved\n",
      "Validation loss 0.99988144339283 NOT improved\n",
      "Validation loss 0.9998629967338177 NOT improved\n",
      "Validation loss 0.999841243990981 NOT improved\n",
      "Validation loss 0.9998487061747745 NOT improved\n",
      "Validation loss 0.9998179905906898 NOT improved\n",
      "Validation loss 0.9997588078926861 NOT improved\n",
      "Validation loss 0.9997452675911087 NOT improved\n",
      "Validation loss 0.9996635352029595 NOT improved\n",
      "Validation loss 0.9996463994192757 NOT improved\n",
      "Validation loss 0.9995421544939559 NOT improved\n",
      "Validation loss 0.9990814052669011 NOT improved\n",
      "Validation loss 0.9984432778469706 NOT improved\n",
      "Validation loss 0.9991129909971425 NOT improved\n",
      "Validation loss 0.998739084131136 NOT improved\n",
      "Validation loss 0.9963393110801977 NOT improved\n",
      "Validation loss 0.9987246685083714 NOT improved\n",
      "Validation loss 0.9941841110683421 NOT improved\n",
      "Validation loss 0.9894087097246993 NOT improved\n",
      "Validation loss 0.9958629730281744 NOT improved\n",
      "Validation loss 0.9920823888234044 NOT improved\n",
      "Validation loss 0.977486150770076 NOT improved\n",
      "Validation loss 0.9812671739301039 NOT improved\n",
      "Validation loss 0.9895867794987054 NOT improved\n",
      "Validation loss 0.9793933973741781 NOT improved\n",
      "Validation loss 0.9808350907386492 NOT improved\n",
      "Validation loss 0.8829303421111768 NOT improved\n",
      "Validation loss 0.9781234056020315 NOT improved\n",
      "Validation loss 0.894688308030791 NOT improved\n",
      "Validation loss 0.9841887166470213 NOT improved\n",
      "Validation loss 0.9105009438882666 NOT improved\n",
      "Validation loss 0.880669646612031 NOT improved\n",
      "Validation loss 0.965934197973533 NOT improved\n",
      "Validation loss 0.9096100849656439 NOT improved\n",
      "Validation loss 0.9254623095425758 NOT improved\n",
      "Validation loss 0.8849822588398657 NOT improved\n",
      "Validation loss 0.8633868417214429 NOT improved\n",
      "Validation loss 0.8643482235267962 NOT improved\n",
      "Validation loss 2.70119900908587 NOT improved\n",
      "Early stopping\n",
      "Training round 20\n",
      "Validation loss 0.9993742975669773 NOT improved\n",
      "Validation loss 0.9997894147952066 NOT improved\n",
      "Validation loss 0.9998471051762183 NOT improved\n",
      "Validation loss 0.9998700427499384 NOT improved\n",
      "Validation loss 0.9998806800193075 NOT improved\n",
      "Validation loss 0.9998842500978976 NOT improved\n",
      "Validation loss 0.9998701154678787 NOT improved\n",
      "Validation loss 0.9998517488411944 NOT improved\n",
      "Validation loss 0.9998642240150768 NOT improved\n",
      "Validation loss 0.9998552143866631 NOT improved\n",
      "Validation loss 0.9998194610298387 NOT improved\n",
      "Validation loss 0.9998043319758997 NOT improved\n",
      "Validation loss 0.999773287997457 NOT improved\n",
      "Validation loss 0.9996589471326006 NOT improved\n",
      "Validation loss 0.9996320413110895 NOT improved\n",
      "Validation loss 0.99963537250237 NOT improved\n",
      "Validation loss 0.9994631331687646 NOT improved\n",
      "Validation loss 0.9995161436004045 NOT improved\n",
      "Validation loss 0.9991517723382006 NOT improved\n",
      "Validation loss 0.998538221351646 NOT improved\n",
      "Validation loss 0.9985934415385489 NOT improved\n",
      "Validation loss 0.9979446821611075 NOT improved\n",
      "Validation loss 0.9940191594647597 NOT improved\n",
      "Validation loss 0.9969956975165756 NOT improved\n",
      "Validation loss 0.9957934497007634 NOT improved\n",
      "Validation loss 0.9979964350985324 NOT improved\n",
      "Validation loss 0.9937031945239176 NOT improved\n",
      "Validation loss 0.9871020696203575 NOT improved\n",
      "Validation loss 0.9932460289239067 NOT improved\n",
      "Validation loss 0.9768911445453361 NOT improved\n",
      "Validation loss 0.983816871305088 NOT improved\n",
      "Validation loss 0.9411303435429705 NOT improved\n",
      "Validation loss 0.956672027251931 NOT improved\n",
      "Validation loss 0.9389407192498417 NOT improved\n",
      "Validation loss 0.9866830518759568 NOT improved\n",
      "Validation loss 0.8792133188281052 NOT improved\n",
      "Validation loss 0.9078764121183378 NOT improved\n",
      "Validation loss 0.8628726328029843 NOT improved\n",
      "Validation loss 0.8963672002217269 NOT improved\n",
      "Validation loss 0.9717108430633287 NOT improved\n",
      "Validation loss 0.8956624918077977 NOT improved\n",
      "Validation loss 1.0912335002442022 NOT improved\n",
      "Early stopping\n",
      "Training round 21\n",
      "Validation loss 0.9996068815404947 NOT improved\n",
      "Validation loss 0.9997851542487433 NOT improved\n",
      "Validation loss 0.9998385260804018 NOT improved\n",
      "Validation loss 0.9998579579904975 NOT improved\n",
      "Validation loss 0.9998617380491293 NOT improved\n",
      "Validation loss 0.9998567188235082 NOT improved\n",
      "Validation loss 0.9998409830437209 NOT improved\n",
      "Validation loss 0.9998336151801808 NOT improved\n",
      "Validation loss 0.9998338818883518 NOT improved\n",
      "Validation loss 0.9997852819937314 NOT improved\n",
      "Validation loss 0.9997902039651597 NOT improved\n",
      "Validation loss 0.9996991855185856 NOT improved\n",
      "Validation loss 0.9996944385102413 NOT improved\n",
      "Validation loss 0.9996761870381055 NOT improved\n",
      "Validation loss 0.9995927743423474 NOT improved\n",
      "Validation loss 0.999520589291308 NOT improved\n",
      "Validation loss 0.9992919213429392 NOT improved\n",
      "Validation loss 0.9988102618682425 NOT improved\n",
      "Validation loss 0.9987798056681262 NOT improved\n",
      "Validation loss 0.9984333603038745 NOT improved\n",
      "Validation loss 0.9987035622030599 NOT improved\n",
      "Validation loss 0.9970259879851495 NOT improved\n",
      "Validation loss 0.9954403659821678 NOT improved\n",
      "Validation loss 0.9973296012865472 NOT improved\n",
      "Validation loss 0.9965107191576665 NOT improved\n",
      "Validation loss 0.9942159743041307 NOT improved\n",
      "Validation loss 0.993916170529108 NOT improved\n",
      "Validation loss 0.9853086749163297 NOT improved\n",
      "Validation loss 0.9781482634980948 NOT improved\n",
      "Validation loss 0.9961632758337206 NOT improved\n",
      "Validation loss 0.994847436054103 NOT improved\n",
      "Validation loss 0.9385238196405252 NOT improved\n",
      "Validation loss 0.942500074595002 NOT improved\n",
      "Validation loss 0.9773377547965534 NOT improved\n",
      "Validation loss 0.8945780916433123 NOT improved\n",
      "Validation loss 0.9810546807510293 NOT improved\n",
      "Validation loss 0.9579097307335733 NOT improved\n",
      "Validation loss 0.8853341438838801 NOT improved\n",
      "Validation loss 0.8940839437409573 NOT improved\n",
      "Validation loss 0.9149019609946654 NOT improved\n",
      "Validation loss 0.9614570113942446 NOT improved\n",
      "Validation loss 0.8729100586109275 NOT improved\n",
      "Validation loss 0.8493639275781452 NOT improved\n",
      "Validation loss 0.8791684995563663 NOT improved\n",
      "Validation loss 0.9173975861569013 NOT improved\n",
      "Validation loss 0.9028529432739345 NOT improved\n",
      "Validation loss 0.9597350415428917 NOT improved\n",
      "Validation loss 0.9816142716434963 NOT improved\n",
      "Validation loss 2.160141735606947 NOT improved\n",
      "Early stopping\n",
      "Training round 22\n",
      "Validation loss 0.9995930110848641 NOT improved\n",
      "Validation loss 0.9997705126927826 NOT improved\n",
      "Validation loss 0.9998496150596415 NOT improved\n",
      "Validation loss 0.9998765529049917 NOT improved\n",
      "Validation loss 0.999882427917285 NOT improved\n",
      "Validation loss 0.9998804979427938 NOT improved\n",
      "Validation loss 0.9998768370787996 NOT improved\n",
      "Validation loss 0.9998757247124717 NOT improved\n",
      "Validation loss 0.9998692147664022 NOT improved\n",
      "Validation loss 0.9998698097524723 NOT improved\n",
      "Validation loss 0.9998170802701108 NOT improved\n",
      "Validation loss 0.9997815080325839 NOT improved\n",
      "Validation loss 0.999753585333846 NOT improved\n",
      "Validation loss 0.9997549469379753 NOT improved\n",
      "Validation loss 0.9997276878811173 NOT improved\n",
      "Validation loss 0.9995993628647081 NOT improved\n",
      "Validation loss 0.9994971770969235 NOT improved\n",
      "Validation loss 0.9994833893775547 NOT improved\n",
      "Validation loss 0.9994420766261017 NOT improved\n",
      "Validation loss 0.9983324782667082 NOT improved\n",
      "Validation loss 0.9983245616967914 NOT improved\n",
      "Validation loss 0.9988610464420431 NOT improved\n",
      "Validation loss 0.9958196307266577 NOT improved\n",
      "Validation loss 0.9961137913383277 NOT improved\n",
      "Validation loss 0.9930240032780646 NOT improved\n",
      "Validation loss 0.9970801532993717 NOT improved\n",
      "Validation loss 0.9852305930243435 NOT improved\n",
      "Validation loss 0.9932612879807486 NOT improved\n",
      "Validation loss 0.9879772091095147 NOT improved\n",
      "Validation loss 0.9840851320187796 NOT improved\n",
      "Validation loss 0.9923161349743017 NOT improved\n",
      "Validation loss 0.9933363341634598 NOT improved\n",
      "Validation loss 0.9588320900259688 NOT improved\n",
      "Validation loss 0.8800431470148926 NOT improved\n",
      "Validation loss 0.9850603081303164 NOT improved\n",
      "Validation loss 0.9775994849826817 NOT improved\n",
      "Validation loss 0.9385909161909236 NOT improved\n",
      "Validation loss 0.9764069749771653 NOT improved\n",
      "Validation loss 0.8873779535525906 NOT improved\n",
      "Validation loss 0.8759875099731371 NOT improved\n",
      "Validation loss 0.9793159247129813 NOT improved\n",
      "Validation loss 1.0581374923697686 NOT improved\n",
      "Early stopping\n",
      "Training round 23\n",
      "Validation loss 0.9996758116439047 NOT improved\n",
      "Validation loss 0.9997943002934071 NOT improved\n",
      "Validation loss 0.9998536994071966 NOT improved\n",
      "Validation loss 0.9998737463234211 NOT improved\n",
      "Validation loss 0.9998846773106819 NOT improved\n",
      "Validation loss 0.9998830078667245 NOT improved\n",
      "Validation loss 0.9998742413559525 NOT improved\n",
      "Validation loss 0.9998642633705027 NOT improved\n",
      "Validation loss 0.99983579150221 NOT improved\n",
      "Validation loss 0.9998409737461175 NOT improved\n",
      "Validation loss 0.99981275096261 NOT improved\n",
      "Validation loss 0.9998162517333582 NOT improved\n",
      "Validation loss 0.999784569660594 NOT improved\n",
      "Validation loss 0.9997520143506725 NOT improved\n",
      "Validation loss 0.9996364637338954 NOT improved\n",
      "Validation loss 0.9996456871842663 NOT improved\n",
      "Validation loss 0.999541826066933 NOT improved\n",
      "Validation loss 0.9995737642748488 NOT improved\n",
      "Validation loss 0.9993599512219375 NOT improved\n",
      "Validation loss 0.9993456753898428 NOT improved\n",
      "Validation loss 0.9986244253818163 NOT improved\n",
      "Validation loss 0.9990792933901332 NOT improved\n",
      "Validation loss 0.9984367703417053 NOT improved\n",
      "Validation loss 0.9967865279226429 NOT improved\n",
      "Validation loss 0.9917232669802108 NOT improved\n",
      "Validation loss 0.9968412070744854 NOT improved\n",
      "Validation loss 0.9905394582313738 NOT improved\n",
      "Validation loss 0.9945634163738105 NOT improved\n",
      "Validation loss 0.996932694037057 NOT improved\n",
      "Validation loss 0.9805983270666734 NOT improved\n",
      "Validation loss 0.9926452949447687 NOT improved\n",
      "Validation loss 0.9929631328256601 NOT improved\n",
      "Validation loss 0.9912918481622356 NOT improved\n",
      "Validation loss 0.9850417376811094 NOT improved\n",
      "Validation loss 0.9859231640398832 NOT improved\n",
      "Validation loss 0.9894250471797719 NOT improved\n",
      "Validation loss 0.9797589214278645 NOT improved\n",
      "Validation loss 0.9593488236579576 NOT improved\n",
      "Validation loss 0.9506205886583812 NOT improved\n",
      "Validation loss 0.9032657184631224 NOT improved\n",
      "Validation loss 0.9385234920509757 NOT improved\n",
      "Validation loss 0.9420239096561214 NOT improved\n",
      "Validation loss 0.8532195928275922 NOT improved\n",
      "Validation loss 0.9015766923963631 NOT improved\n",
      "Validation loss 0.8743389037338598 NOT improved\n",
      "Validation loss 0.8946700819361313 NOT improved\n",
      "Validation loss 0.8863066460750322 NOT improved\n",
      "Validation loss 1.4791558847112942 NOT improved\n",
      "Early stopping\n",
      "Training round 24\n",
      "Validation loss 0.9994602603961665 NOT improved\n",
      "Validation loss 0.9997399999957363 NOT improved\n",
      "Validation loss 0.9998329990592446 NOT improved\n",
      "Validation loss 0.9998671848085146 NOT improved\n",
      "Validation loss 0.9998807619061052 NOT improved\n",
      "Validation loss 0.999896656412032 NOT improved\n",
      "Validation loss 0.9998875281059414 NOT improved\n",
      "Validation loss 0.9998896738117022 NOT improved\n",
      "Validation loss 0.9998796951083183 NOT improved\n",
      "Validation loss 0.9998918658916437 NOT improved\n",
      "Validation loss 0.9998475269921121 NOT improved\n",
      "Validation loss 0.9998342060446924 NOT improved\n",
      "Validation loss 0.9998198325104248 NOT improved\n",
      "Validation loss 0.9997036094661311 NOT improved\n",
      "Validation loss 0.9997430842972993 NOT improved\n",
      "Validation loss 0.9996640750672615 NOT improved\n",
      "Validation loss 0.9996613841218485 NOT improved\n",
      "Validation loss 0.9995205204245337 NOT improved\n",
      "Validation loss 0.9993301806075279 NOT improved\n",
      "Validation loss 0.9994099150983327 NOT improved\n",
      "Validation loss 0.9975474093435812 NOT improved\n",
      "Validation loss 0.9991033933792968 NOT improved\n",
      "Validation loss 0.9987390355422067 NOT improved\n",
      "Validation loss 0.998934839166601 NOT improved\n",
      "Validation loss 0.9953867364320813 NOT improved\n",
      "Validation loss 0.9964078775411521 NOT improved\n",
      "Validation loss 0.9919818581998174 NOT improved\n",
      "Validation loss 0.9972916925144355 NOT improved\n",
      "Validation loss 0.9952158835052307 NOT improved\n",
      "Validation loss 0.9866133956312793 NOT improved\n",
      "Validation loss 0.9639001071835998 NOT improved\n",
      "Validation loss 0.9203002313619546 NOT improved\n",
      "Validation loss 0.9926027944150001 NOT improved\n",
      "Validation loss 0.9601592453121026 NOT improved\n",
      "Validation loss 0.9303657165923415 NOT improved\n",
      "Validation loss 0.9648451528303883 NOT improved\n",
      "Validation loss 0.9372934803484321 NOT improved\n",
      "Validation loss 0.9583913838527497 NOT improved\n",
      "Validation loss 0.9299990323494924 NOT improved\n",
      "Validation loss 0.9713915977917871 NOT improved\n",
      "Validation loss 0.902490175910277 NOT improved\n",
      "Validation loss 0.9757820677214273 NOT improved\n",
      "Validation loss 1.7892410577026423 NOT improved\n",
      "Early stopping\n",
      "Training round 25\n",
      "Validation loss 0.9996278947041471 NOT improved\n",
      "Validation loss 0.9997335733312378 NOT improved\n",
      "Validation loss 0.9998045830639657 NOT improved\n",
      "Validation loss 0.9998437649916567 NOT improved\n",
      "Validation loss 0.999854623880588 NOT improved\n",
      "Validation loss 0.9998472607455132 NOT improved\n",
      "Validation loss 0.999856352349234 NOT improved\n",
      "Validation loss 0.999828376561054 NOT improved\n",
      "Validation loss 0.9998312489768572 NOT improved\n",
      "Validation loss 0.9997845446093024 NOT improved\n",
      "Validation loss 0.9997422675837478 NOT improved\n",
      "Validation loss 0.9997566978646112 NOT improved\n",
      "Validation loss 0.9996790919033556 NOT improved\n",
      "Validation loss 0.9996481958161684 NOT improved\n",
      "Validation loss 0.9994822144990849 NOT improved\n",
      "Validation loss 0.9995440866108473 NOT improved\n",
      "Validation loss 0.9993880181073722 NOT improved\n",
      "Validation loss 0.9990888358550652 NOT improved\n",
      "Validation loss 0.9990119399671324 NOT improved\n",
      "Validation loss 0.9985492795612001 NOT improved\n",
      "Validation loss 0.9987232951877472 NOT improved\n",
      "Validation loss 0.9982816975612542 NOT improved\n",
      "Validation loss 0.9948876800168227 NOT improved\n",
      "Validation loss 0.9973280351503877 NOT improved\n",
      "Validation loss 0.9879524248343251 NOT improved\n",
      "Validation loss 0.9942937923947679 NOT improved\n",
      "Validation loss 0.993741345352057 NOT improved\n",
      "Validation loss 0.9744046169485212 NOT improved\n",
      "Validation loss 0.9946555365950879 NOT improved\n",
      "Validation loss 0.9839988290548071 NOT improved\n",
      "Validation loss 0.962893188028829 NOT improved\n",
      "Validation loss 0.9878332523897556 NOT improved\n",
      "Validation loss 0.9248025720411097 NOT improved\n",
      "Validation loss 0.9651376652617292 NOT improved\n",
      "Validation loss 0.895810210843077 NOT improved\n",
      "Validation loss 0.89274691443297 NOT improved\n",
      "Validation loss 1.2768253648153776 NOT improved\n",
      "Early stopping\n",
      "Training round 26\n",
      "Validation loss 0.9996611501385767 NOT improved\n",
      "Validation loss 0.9997673607069486 NOT improved\n",
      "Validation loss 0.9998147600371332 NOT improved\n",
      "Validation loss 0.9998501946135692 NOT improved\n",
      "Validation loss 0.9998651735263884 NOT improved\n",
      "Validation loss 0.9998522681755171 NOT improved\n",
      "Validation loss 0.9998648223000011 NOT improved\n",
      "Validation loss 0.9998394162803067 NOT improved\n",
      "Validation loss 0.9998255626863679 NOT improved\n",
      "Validation loss 0.9998117391399918 NOT improved\n",
      "Validation loss 0.9997653324378507 NOT improved\n",
      "Validation loss 0.9997423806142574 NOT improved\n",
      "Validation loss 0.9997701715788803 NOT improved\n",
      "Validation loss 0.9997798919423379 NOT improved\n",
      "Validation loss 0.9995455218154662 NOT improved\n",
      "Validation loss 0.9994117544751173 NOT improved\n",
      "Validation loss 0.9993666894960255 NOT improved\n",
      "Validation loss 0.9994895450090214 NOT improved\n",
      "Validation loss 0.9994919608392918 NOT improved\n",
      "Validation loss 0.9991119824587263 NOT improved\n",
      "Validation loss 0.997372019577711 NOT improved\n",
      "Validation loss 0.9984667951353459 NOT improved\n",
      "Validation loss 0.9986777712323813 NOT improved\n",
      "Validation loss 0.9947149265323351 NOT improved\n",
      "Validation loss 0.9982765729138867 NOT improved\n",
      "Validation loss 0.9969463809385696 NOT improved\n",
      "Validation loss 0.9755069237621993 NOT improved\n",
      "Validation loss 0.9965253726250638 NOT improved\n",
      "Validation loss 0.9951144025318582 NOT improved\n",
      "Validation loss 0.9736155820785427 NOT improved\n",
      "Validation loss 0.9331552164668029 NOT improved\n",
      "Validation loss 0.9365471363904588 NOT improved\n",
      "Validation loss 0.919887775602005 NOT improved\n",
      "Validation loss 0.9509139554421026 NOT improved\n",
      "Validation loss 0.9767803808605795 NOT improved\n",
      "Validation loss 0.9741872081390598 NOT improved\n",
      "Validation loss 0.9759961246723292 NOT improved\n",
      "Validation loss 0.9667037166197556 NOT improved\n",
      "Validation loss 1.2545249495994633 NOT improved\n",
      "Early stopping\n",
      "Training round 27\n",
      "Validation loss 0.9997162883986401 NOT improved\n",
      "Validation loss 0.9998297598438584 NOT improved\n",
      "Validation loss 0.9998598442051212 NOT improved\n",
      "Validation loss 0.9998615765666258 NOT improved\n",
      "Validation loss 0.9998739317399181 NOT improved\n",
      "Validation loss 0.9998564660004721 NOT improved\n",
      "Validation loss 0.9998684172925547 NOT improved\n",
      "Validation loss 0.9998547403831365 NOT improved\n",
      "Validation loss 0.999844670975108 NOT improved\n",
      "Validation loss 0.9998614546368931 NOT improved\n",
      "Validation loss 0.9998299844490467 NOT improved\n",
      "Validation loss 0.9998260686085022 NOT improved\n",
      "Validation loss 0.9997806171556268 NOT improved\n",
      "Validation loss 0.9997915053178141 NOT improved\n",
      "Validation loss 0.9997445350180731 NOT improved\n",
      "Validation loss 0.9997525697223102 NOT improved\n",
      "Validation loss 0.9995606563198723 NOT improved\n",
      "Validation loss 0.9996565332719387 NOT improved\n",
      "Validation loss 0.9991789270896886 NOT improved\n",
      "Validation loss 0.999118423632149 NOT improved\n",
      "Validation loss 0.9989238064530356 NOT improved\n",
      "Validation loss 0.9986297297285505 NOT improved\n",
      "Validation loss 0.9979485647918429 NOT improved\n",
      "Validation loss 0.9936052869963143 NOT improved\n",
      "Validation loss 0.9958261691101 NOT improved\n",
      "Validation loss 0.9951972444928722 NOT improved\n",
      "Validation loss 0.9922782660415363 NOT improved\n",
      "Validation loss 0.9959880801235729 NOT improved\n",
      "Validation loss 0.9745324795567907 NOT improved\n",
      "Validation loss 0.9837411237843754 NOT improved\n",
      "Validation loss 0.9711137721578335 NOT improved\n",
      "Validation loss 0.9818325380728739 NOT improved\n",
      "Validation loss 0.9570844473098821 NOT improved\n",
      "Validation loss 0.9853706184065953 NOT improved\n",
      "Validation loss 0.9893992292519921 NOT improved\n",
      "Validation loss 0.956040289472466 NOT improved\n",
      "Validation loss 0.9453407489492477 NOT improved\n",
      "Validation loss 0.9550633774017412 NOT improved\n",
      "Validation loss 1.1165802926910748 NOT improved\n",
      "Early stopping\n",
      "Training round 28\n",
      "Validation loss 0.9996507160446263 NOT improved\n"
     ]
    }
   ],
   "source": [
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695, \n",
    "         2463725, 4734, 23234, 884, 2341, 362, 5, 234, 483, 785356, 23425, 3621, \n",
    "         58461, 80968765, 123, 425633, 5646, 67635, 76785, 34214]\n",
    "\n",
    "training_rounds = len(seeds)\n",
    "\n",
    "ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = '{}_lead{}'.format(model_tag, lead_name)\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(TRAIN_256_pos)\n",
    "L_neg = len(TRAIN_256_neg)\n",
    "\n",
    "record = 1.1\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "L_train = 16\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    if r == 0:\n",
    "        tol = 0\n",
    "    else:\n",
    "        tol = -200\n",
    "\n",
    "    model = create_model()\n",
    "    #\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "    \n",
    "    set_seeds(int(seeds[r]))\n",
    "    print('Training round {}'.format(r))\n",
    "\n",
    "    for i in range(epochs):            \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            N_pos = 32\n",
    "            N_neg = batch_size - N_pos\n",
    "\n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "            ind_neg_pick = ind_neg[:N_neg]\n",
    "            ind_pos_pick = ind_pos[:N_pos]\n",
    "\n",
    "            X_batch_neg = TRAIN_256_neg[ind_neg_pick, :]\n",
    "            X_batch_pos = TRAIN_256_pos[ind_pos_pick, :]\n",
    "            \n",
    "            X_batch_stn_neg = TRAIN_stn_neg[ind_neg_pick, :]\n",
    "            X_batch_stn_pos = TRAIN_stn_pos[ind_pos_pick, :]\n",
    "\n",
    "            X_batch = np.concatenate((X_batch_neg, X_batch_pos), axis=0)\n",
    "            X_batch_stn = np.concatenate((X_batch_stn_neg, X_batch_stn_pos), axis=0)\n",
    "\n",
    "            Y_batch = np.ones([batch_size,])\n",
    "            Y_batch[:N_neg] = 0.0\n",
    "\n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "\n",
    "            X_batch = X_batch[ind_, :]\n",
    "            X_batch_stn = X_batch_stn[ind_, :]\n",
    "            Y_batch = Y_batch[ind_]\n",
    "\n",
    "            # train on batch\n",
    "            model.train_on_batch([X_batch, X_batch_stn], Y_batch);\n",
    "\n",
    "        # epoch end operations\n",
    "        Y_pred = model.predict([VALID_X, VALID_merge])\n",
    "\n",
    "        Y_pred[Y_pred<0] = 0\n",
    "        Y_pred[Y_pred>1] = 1\n",
    "\n",
    "        record_temp = verif_metric(VALID_Y, Y_pred, ref)\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     model.save(model_path_backup)\n",
    "\n",
    "        if (record - record_temp > min_del):\n",
    "            print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "            record = record_temp\n",
    "            tol = 0\n",
    "            \n",
    "            #print('tol: {}'.format(tol))\n",
    "            # save\n",
    "            print('save to: {}'.format(model_path))\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            print('Validation loss {} NOT improved'.format(record_temp))\n",
    "            if record_temp > 1.01:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba732cc-cef9-43ff-89ff-0fb00f59075c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964176a0-95be-4e49-81c4-7a659751d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27199f0a-0bd1-490b-b929-922a8a39a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b0011-7eee-4cca-a80a-ed37a18c5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=keras.optimizers.Adam(lr=0))\n",
    "\n",
    "W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/alt_lead{}/'.format(lead_name))\n",
    "model.set_weights(W_old)\n",
    "\n",
    "ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "Y_pred = model.predict([VALID_X, VALID_merge])\n",
    "record_temp = verif_metric(VALID_Y, Y_pred, ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb05d2d-092a-4702-a245-fea6f96ffe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {}\n",
    "save_dict['Y_pred'] = Y_pred\n",
    "save_dict['VALID_Y'] = VALID_Y\n",
    "np.save('{}RESULT_FULL_lead{}_{}.npy'.format(filepath_vec, lead_name, model_tag), save_dict)\n",
    "print('{}RESULT_FULL_lead{}_{}.npy'.format(filepath_vec, lead_name, model_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60af67c-8780-42cf-ab0a-d6434b1b5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b4252-b6b6-441e-abb0-c0bca38f3471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec269e-6211-4a90-a8d9-fa78fc3f33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94898f-0118-4b3e-91e7-c7b8f07b5187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a664b-35c5-4d13-bd1d-63fdfac63233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469f1cd-f820-44a1-9ba6-4f228782f813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0e7c-4640-4e65-b675-995d3a1a06d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e5327-70b2-4ba4-94bc-f1e37f04bdba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d4ab4-6e8d-49fb-adb5-29205a17e6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf3bf0-b6ad-4be6-b2f4-3bead7aaa672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b6f27-7551-4a81-8669-3482c0da28a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca671f-cd07-4052-85c6-10c359bc8c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
