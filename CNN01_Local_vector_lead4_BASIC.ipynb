{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train classifier head with size-128 feature vectors on a 64-by-64 grid cell\n",
    "Nearby grid cells are not considered\n",
    "Revamped\n",
    "'''\n",
    "\n",
    "# general tools\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('lead1', help='lead1')\n",
    "parser.add_argument('lead2', help='lead2')\n",
    "parser.add_argument('lead3', help='lead3')\n",
    "parser.add_argument('lead4', help='lead4')\n",
    "\n",
    "parser.add_argument('lead_name', help='lead_name')\n",
    "parser.add_argument('model_tag', help='model_tag')\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "# =============== #\n",
    "\n",
    "lead1 = int(args['lead1'])\n",
    "lead2 = int(args['lead2'])\n",
    "lead3 = int(args['lead3'])\n",
    "lead4 = int(args['lead4'])\n",
    "\n",
    "lead_name = args['lead_name']\n",
    "model_tag = args['model_tag']\n",
    "\n",
    "L_vec = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================ #\n",
    "# Geographical information\n",
    "\n",
    "with h5py.File(save_dir+'HRRR_domain.hdf', 'r') as h5io:\n",
    "    lon_3km = h5io['lon_3km'][...]\n",
    "    lat_3km = h5io['lat_3km'][...]\n",
    "    lon_80km = h5io['lon_80km'][...]\n",
    "    lat_80km = h5io['lat_80km'][...]\n",
    "    elev_3km = h5io['elev_3km'][...]\n",
    "    land_mask_80km = h5io['land_mask_80km'][...]\n",
    "    \n",
    "grid_shape = land_mask_80km.shape\n",
    "\n",
    "elev_80km = du.interp2d_wraper(lon_3km, lat_3km, elev_3km, lon_80km, lat_80km, method='linear')\n",
    "\n",
    "elev_80km[np.isnan(elev_80km)] = 0\n",
    "elev_80km[elev_80km<0] = 0\n",
    "elev_max = np.max(elev_80km)\n",
    "\n",
    "lon_80km_mask = lon_80km[land_mask_80km]\n",
    "lat_80km_mask = lat_80km[land_mask_80km]\n",
    "\n",
    "lon_minmax = [np.min(lon_80km_mask), np.max(lon_80km_mask)]\n",
    "lat_minmax = [np.min(lat_80km_mask), np.max(lat_80km_mask)]\n",
    "\n",
    "# ============================================================ #\n",
    "# File path\n",
    "path_name1_v3 = path_batch_v3\n",
    "path_name2_v3 = path_batch_v3\n",
    "path_name3_v3 = path_batch_v3\n",
    "path_name4_v3 = path_batch_v3\n",
    "\n",
    "path_name1_v4 = path_batch_v4x\n",
    "path_name2_v4 = path_batch_v4x\n",
    "path_name3_v4 = path_batch_v4x\n",
    "path_name4_v4 = path_batch_v4x\n",
    "\n",
    "path_name1_v4_test = path_batch_v4\n",
    "path_name2_v4_test = path_batch_v4\n",
    "path_name3_v4_test = path_batch_v4\n",
    "path_name4_v4_test = path_batch_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6434b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================= #\n",
    "# Read batch file names (npy)\n",
    "\n",
    "filename_train_lead1_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_train_lead2_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name2_v3, lead2)))\n",
    "filename_train_lead3_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name3_v3, lead3)))\n",
    "filename_train_lead4_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name4_v3, lead4)))\n",
    "\n",
    "filename_valid_lead1_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_valid_lead2_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name2_v3, lead2)))\n",
    "filename_valid_lead3_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name3_v3, lead3)))\n",
    "filename_valid_lead4_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name4_v3, lead4)))\n",
    "\n",
    "# ============================================================ #\n",
    "# Consistency check indices\n",
    "\n",
    "IND_TRAIN_lead = np.load('/glade/work/ksha/NCAR/IND_TRAIN_lead_full.npy', allow_pickle=True)[()]\n",
    "TRAIN_ind1_v3 = IND_TRAIN_lead['lead{}'.format(lead1)]\n",
    "TRAIN_ind2_v3 = IND_TRAIN_lead['lead{}'.format(lead2)]\n",
    "TRAIN_ind3_v3 = IND_TRAIN_lead['lead{}'.format(lead3)]\n",
    "TRAIN_ind4_v3 = IND_TRAIN_lead['lead{}'.format(lead4)]\n",
    "\n",
    "IND_VALID_lead = np.load('/glade/work/ksha/NCAR/IND_VALID_lead_full.npy', allow_pickle=True)[()]\n",
    "VALID_ind1_v3 = IND_VALID_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2_v3 = IND_VALID_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3_v3 = IND_VALID_lead['lead{}'.format(lead3)]\n",
    "VALID_ind4_v3 = IND_VALID_lead['lead{}'.format(lead4)]\n",
    "\n",
    "# ============================================================== #\n",
    "# Load feature vectors (HRRR v3, training)\n",
    "\n",
    "data_lead1_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead2_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead3_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead4_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "TRAIN_lead1_v3 = np.concatenate((data_lead1_p0['y_vector'], data_lead1_p1['y_vector'], data_lead1_p2['y_vector']), axis=0)\n",
    "TRAIN_lead2_v3 = np.concatenate((data_lead2_p0['y_vector'], data_lead2_p1['y_vector'], data_lead2_p2['y_vector']), axis=0)\n",
    "TRAIN_lead3_v3 = np.concatenate((data_lead3_p0['y_vector'], data_lead3_p1['y_vector'], data_lead3_p2['y_vector']), axis=0)\n",
    "TRAIN_lead4_v3 = np.concatenate((data_lead4_p0['y_vector'], data_lead4_p1['y_vector'], data_lead4_p2['y_vector']), axis=0)\n",
    "\n",
    "TRAIN_lead1_y_v3 = np.concatenate((data_lead1_p0['y_true'], data_lead1_p1['y_true'], data_lead1_p2['y_true']), axis=0)\n",
    "TRAIN_lead2_y_v3 = np.concatenate((data_lead2_p0['y_true'], data_lead2_p1['y_true'], data_lead2_p2['y_true']), axis=0)\n",
    "TRAIN_lead3_y_v3 = np.concatenate((data_lead3_p0['y_true'], data_lead3_p1['y_true'], data_lead3_p2['y_true']), axis=0)\n",
    "TRAIN_lead4_y_v3 = np.concatenate((data_lead4_p0['y_true'], data_lead4_p1['y_true'], data_lead4_p2['y_true']), axis=0)\n",
    "\n",
    "# =========================================================== #\n",
    "# Load feature vectors (HRRR v3, validation)\n",
    "\n",
    "data_lead1_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "VALID_lead1_v3 = data_lead1_valid['y_vector']\n",
    "VALID_lead2_v3 = data_lead2_valid['y_vector']\n",
    "VALID_lead3_v3 = data_lead3_valid['y_vector']\n",
    "VALID_lead4_v3 = data_lead4_valid['y_vector']\n",
    "\n",
    "VALID_lead1_y_v3 = data_lead1_valid['y_true']\n",
    "VALID_lead2_y_v3 = data_lead2_valid['y_true']\n",
    "VALID_lead3_y_v3 = data_lead3_valid['y_true']\n",
    "VALID_lead4_y_v3 = data_lead4_valid['y_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5178f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================= #\n",
    "# Collect feature vectors from all batch files (HRRR v3, validation)\n",
    "\n",
    "L = len(TRAIN_ind2_v3)\n",
    "\n",
    "filename_train1_pick_v3 = []\n",
    "filename_train2_pick_v3 = []\n",
    "filename_train3_pick_v3 = []\n",
    "filename_train4_pick_v3 = []\n",
    "\n",
    "TRAIN_X_lead1 = np.empty((L, 128))\n",
    "TRAIN_X_lead2 = np.empty((L, 128))\n",
    "TRAIN_X_lead3 = np.empty((L, 128))\n",
    "TRAIN_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "TRAIN_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(TRAIN_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(TRAIN_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(TRAIN_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(TRAIN_ind4_v3[i])\n",
    "    \n",
    "    filename_train1_pick_v3.append(filename_train_lead1_v3[ind_lead1_v3])\n",
    "    filename_train2_pick_v3.append(filename_train_lead2_v3[ind_lead2_v3])\n",
    "    filename_train3_pick_v3.append(filename_train_lead3_v3[ind_lead3_v3])\n",
    "    filename_train4_pick_v3.append(filename_train_lead4_v3[ind_lead4_v3])\n",
    "    \n",
    "    TRAIN_X_lead1[i, :] = TRAIN_lead1_v3[ind_lead1_v3, :]\n",
    "    TRAIN_X_lead2[i, :] = TRAIN_lead2_v3[ind_lead2_v3, :]\n",
    "    TRAIN_X_lead3[i, :] = TRAIN_lead3_v3[ind_lead3_v3, :]\n",
    "    TRAIN_X_lead4[i, :] = TRAIN_lead4_v3[ind_lead4_v3, :]\n",
    "    \n",
    "    TRAIN_Y_v3[i] = TRAIN_lead3_y_v3[ind_lead3_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d627315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "# Collect feature vectors from all batch files (HRRR v3, validation)\n",
    "L = len(VALID_ind2_v3)\n",
    "\n",
    "filename_valid1_pick_v3 = []\n",
    "filename_valid2_pick_v3 = []\n",
    "filename_valid3_pick_v3 = []\n",
    "filename_valid4_pick_v3 = []\n",
    "\n",
    "VALID_X_lead1 = np.empty((L, 128))\n",
    "VALID_X_lead2 = np.empty((L, 128))\n",
    "VALID_X_lead3 = np.empty((L, 128))\n",
    "VALID_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "VALID_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(VALID_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(VALID_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(VALID_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(VALID_ind4_v3[i])\n",
    "    \n",
    "    filename_valid1_pick_v3.append(filename_valid_lead1_v3[ind_lead1_v3])\n",
    "    filename_valid2_pick_v3.append(filename_valid_lead2_v3[ind_lead2_v3])\n",
    "    filename_valid3_pick_v3.append(filename_valid_lead3_v3[ind_lead3_v3])\n",
    "    filename_valid4_pick_v3.append(filename_valid_lead4_v3[ind_lead4_v3])\n",
    "    \n",
    "    VALID_X_lead1[i, :] = VALID_lead1_v3[ind_lead1_v3, :]\n",
    "    VALID_X_lead2[i, :] = VALID_lead2_v3[ind_lead2_v3, :]\n",
    "    VALID_X_lead3[i, :] = VALID_lead3_v3[ind_lead3_v3, :]\n",
    "    VALID_X_lead4[i, :] = VALID_lead4_v3[ind_lead4_v3, :]\n",
    "    \n",
    "    VALID_Y_v3[i] = VALID_lead3_y_v3[ind_lead3_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a79b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TRAIN = np.concatenate((TRAIN_X_lead1[:, None, :], TRAIN_X_lead2[:, None, :],\n",
    "                            TRAIN_X_lead3[:, None, :], TRAIN_X_lead4[:, None, :]), axis=1)\n",
    "\n",
    "ALL_VALID = np.concatenate((VALID_X_lead1[:, None, :], VALID_X_lead2[:, None, :],\n",
    "                            VALID_X_lead3[:, None, :], VALID_X_lead4[:, None, :]), axis=1)\n",
    "\n",
    "ALL_VEC = np.concatenate((ALL_TRAIN, ALL_VALID), axis=0)\n",
    "\n",
    "TRAIN_Y = np.concatenate((TRAIN_Y_v3, VALID_Y_v3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ea99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================== #\n",
    "# Separate pos and neg samples for balanced training\n",
    "\n",
    "TRAIN_pos_x = ALL_VEC[TRAIN_Y==1]\n",
    "TRAIN_neg_x = ALL_VEC[TRAIN_Y==0]\n",
    "\n",
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = mu.feature_extract(filename_train3_pick_v3, \n",
    "                                                 lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "TRAIN_stn_v3 = np.concatenate((lon_norm_v3[:, None], lat_norm_v3[:, None]), axis=1)\n",
    "\n",
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = mu.feature_extract(filename_valid3_pick_v3, \n",
    "                                                 lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "VALID_stn_v3 = np.concatenate((lon_norm_v3[:, None], lat_norm_v3[:, None]), axis=1)\n",
    "\n",
    "ALL_stn = np.concatenate((TRAIN_stn_v3, VALID_stn_v3))\n",
    "\n",
    "TRAIN_stn_pos = ALL_stn[TRAIN_Y==1]\n",
    "TRAIN_stn_neg = ALL_stn[TRAIN_Y==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================== #\n",
    "# HRRR v4x validation set\n",
    "# ====================================================== #\n",
    "# Read batch file names (npy)\n",
    "\n",
    "filename_valid_lead1 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead1)))\n",
    "filename_valid_lead2 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name2_v4_test, lead2)))\n",
    "filename_valid_lead3 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name3_v4_test, lead3)))\n",
    "filename_valid_lead4 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name4_v4_test, lead4)))\n",
    "\n",
    "# =============================== #\n",
    "# Load feature vectors\n",
    "\n",
    "valid_lead1 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "valid_lead2 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "valid_lead3 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "valid_lead4 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "VALID_lead1 = valid_lead1['y_vector']\n",
    "VALID_lead2 = valid_lead2['y_vector']\n",
    "VALID_lead3 = valid_lead3['y_vector']\n",
    "VALID_lead4 = valid_lead4['y_vector']\n",
    "\n",
    "VALID_lead1_y = valid_lead1['y_true']\n",
    "VALID_lead2_y = valid_lead2['y_true']\n",
    "VALID_lead3_y = valid_lead3['y_true']\n",
    "VALID_lead4_y = valid_lead4['y_true']\n",
    "\n",
    "# ============================================================ #\n",
    "# Consistency check indices\n",
    "\n",
    "IND_TEST_lead = np.load('/glade/work/ksha/NCAR/IND_TEST_lead_v4.npy', allow_pickle=True)[()]\n",
    "\n",
    "VALID_ind1 = IND_TEST_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2 = IND_TEST_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3 = IND_TEST_lead['lead{}'.format(lead3)]\n",
    "VALID_ind4 = IND_TEST_lead['lead{}'.format(lead4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "# Collect feature vectors from all batch files\n",
    "\n",
    "L = len(VALID_ind2)\n",
    "\n",
    "filename_valid1_pick = []\n",
    "filename_valid2_pick = []\n",
    "filename_valid3_pick = []\n",
    "filename_valid4_pick = []\n",
    "\n",
    "VALID_X_lead1 = np.empty((L, 128))\n",
    "VALID_X_lead2 = np.empty((L, 128))\n",
    "VALID_X_lead3 = np.empty((L, 128))\n",
    "VALID_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "VALID_Y = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1 = int(VALID_ind1[i])\n",
    "    ind_lead2 = int(VALID_ind2[i])\n",
    "    ind_lead3 = int(VALID_ind3[i])\n",
    "    ind_lead4 = int(VALID_ind4[i])\n",
    "    \n",
    "    filename_valid1_pick.append(filename_valid_lead1[ind_lead1])\n",
    "    filename_valid2_pick.append(filename_valid_lead2[ind_lead2])\n",
    "    filename_valid3_pick.append(filename_valid_lead3[ind_lead3])\n",
    "    filename_valid4_pick.append(filename_valid_lead4[ind_lead4])\n",
    "    \n",
    "    VALID_X_lead1[i, :] = VALID_lead1[ind_lead1, :]\n",
    "    VALID_X_lead2[i, :] = VALID_lead2[ind_lead2, :]\n",
    "    VALID_X_lead3[i, :] = VALID_lead3[ind_lead3, :]\n",
    "    VALID_X_lead4[i, :] = VALID_lead4[ind_lead4, :]\n",
    "    \n",
    "    VALID_Y[i] = VALID_lead3_y[ind_lead3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9796b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_VEC = np.concatenate((VALID_X_lead1[:, None, :], VALID_X_lead2[:, None, :],\n",
    "                            VALID_X_lead3[:, None, :], VALID_X_lead4[:, None, :]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be85d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "# extract location information\n",
    "\n",
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = mu.feature_extract(filename_valid3_pick, \n",
    "                                                 lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "VALID_stn_v3 = np.concatenate((lon_norm_v3[:, None], lat_norm_v3[:, None]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589828bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================== #\n",
    "# Set randmo seeds\n",
    "\n",
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695, \n",
    "         2463725, 4734, 23234, 884, 2341, 362, 5, 234, 483, 785356, 23425, 3621, \n",
    "         58461, 80968765, 123, 425633, 5646, 67635, 76785, 34214, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "\n",
    "training_rounds = len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================== #\n",
    "ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "grid_shape = lon_80km.shape\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = '{}_lead{}'.format(model_tag, lead_name)\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(TRAIN_stn_pos)\n",
    "L_neg = len(TRAIN_stn_neg)\n",
    "# =========== Model Section ========== #\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = '{}_lead{}'.format(model_tag, lead_name)\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "record = 1.1\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "L_train = 16\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    if r == 0:\n",
    "        tol = 0\n",
    "    else:\n",
    "        tol = -200\n",
    "\n",
    "    model = mu.create_classif_head()\n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "    \n",
    "    set_seeds(int(seeds[r]))\n",
    "    print('Training round {}'.format(r))\n",
    "\n",
    "    for i in range(epochs):            \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            N_pos = 32\n",
    "            N_neg = batch_size - N_pos\n",
    "\n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "            ind_neg_pick = ind_neg[:N_neg]\n",
    "            ind_pos_pick = ind_pos[:N_pos]\n",
    "\n",
    "            X_batch_neg = TRAIN_neg_x[ind_neg_pick, :]\n",
    "            X_batch_pos = TRAIN_pos_x[ind_pos_pick, :]\n",
    "            \n",
    "            X_batch_stn_neg = TRAIN_stn_neg[ind_neg_pick, :]\n",
    "            X_batch_stn_pos = TRAIN_stn_pos[ind_pos_pick, :]\n",
    "\n",
    "            X_batch = np.concatenate((X_batch_neg, X_batch_pos), axis=0)\n",
    "            X_batch_stn = np.concatenate((X_batch_stn_neg, X_batch_stn_pos), axis=0)\n",
    "\n",
    "            Y_batch = np.ones([batch_size,])\n",
    "            Y_batch[:N_neg] = 0.0\n",
    "\n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "\n",
    "            X_batch = X_batch[ind_, :]\n",
    "            X_batch_stn = X_batch_stn[ind_, :]\n",
    "            Y_batch = Y_batch[ind_]\n",
    "\n",
    "            # train on batch\n",
    "            model.train_on_batch([X_batch, X_batch_stn], Y_batch);\n",
    "\n",
    "        # epoch end operations\n",
    "        Y_pred = model.predict([VALID_VEC, VALID_stn_v3])\n",
    "\n",
    "        Y_pred[Y_pred<0] = 0\n",
    "        Y_pred[Y_pred>1] = 1\n",
    "\n",
    "        record_temp = verif_metric(VALID_Y, Y_pred, ref)\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     model.save(model_path_backup)\n",
    "\n",
    "        if (record - record_temp > min_del):\n",
    "            print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "            record = record_temp\n",
    "            tol = 0\n",
    "            \n",
    "            #print('tol: {}'.format(tol))\n",
    "            # save\n",
    "            print('save to: {}'.format(model_path))\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            print('Validation loss {} NOT improved'.format(record_temp))\n",
    "            if record_temp > 1.0:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0601bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955624c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a16923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================ #\n",
    "# Inference\n",
    "    \n",
    "model = mu.create_classif_head()\n",
    "\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=keras.optimizers.Adam(lr=0))\n",
    "\n",
    "W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/{}_lead{}'.format(model_tag, lead_name))\n",
    "model.set_weights(W_old)\n",
    "        \n",
    "Y_pred_valid = model.predict([VALID_Input, VALID_Input_stn])\n",
    "\n",
    "# Save results\n",
    "save_dict = {}\n",
    "save_dict['Y_pred_valid'] = Y_pred_valid\n",
    "save_dict['VALID_Y'] = VALID_Y\n",
    "\n",
    "np.save('{}RESULT_lead{}_{}.npy'.format(filepath_vec, lead_name, model_tag), save_dict)\n",
    "print('{}RESULT_lead{}_{}.npy'.format(filepath_vec, lead_name, model_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561b514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f21f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
