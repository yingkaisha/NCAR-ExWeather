{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fcce9f-12d3-4505-9624-1c40d99cbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 11:38:55.745837: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('lead1', help='lead1')\n",
    "# parser.add_argument('lead2', help='lead2')\n",
    "# parser.add_argument('lead3', help='lead3')\n",
    "# parser.add_argument('lead4', help='lead4')\n",
    "\n",
    "# parser.add_argument('lead_name', help='lead_name')\n",
    "# parser.add_argument('model_tag', help='model_tag')\n",
    "\n",
    "# args = vars(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba91d2d2-d6db-400f-8c4f-378bd31368d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead1 = 2\n",
    "lead2 = 3\n",
    "lead3 = 4\n",
    "lead4 = 5\n",
    "\n",
    "lead_name = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e37518a-59ce-4111-b706-528e8bff4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def verif_metric(VALID_target, Y_pred, ref):\n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    metric = BS\n",
    "    return metric / ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca717df-62c5-4025-88cb-b26fda908774",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_vec = \"/glade/work/ksha/NCAR/\"\n",
    "\n",
    "path_name1_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "path_name2_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/' \n",
    "path_name3_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "path_name4_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "\n",
    "path_name1_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name2_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name3_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name4_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "\n",
    "path_name1_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name2_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name3_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name4_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725bf972-a39b-4436-ba7a-87bc71e4af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train_lead1 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_train_lead2 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead2)))\n",
    "filename_train_lead3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead3)))\n",
    "filename_train_lead4 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5aa02f-d7ad-4320-8676-9a7bb5ea6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_valid_lead1 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_valid_lead2 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1_v3, lead2)))\n",
    "filename_valid_lead3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1_v3, lead3)))\n",
    "filename_valid_lead4 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1_v3, lead4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78ac36c-add1-4231-91c2-d7c40c232355",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_test_lead1 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead1)))\n",
    "filename_test_lead2 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead2)))\n",
    "filename_test_lead3 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead3)))\n",
    "filename_test_lead4 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262507d4-1fa4-4b3f-b3aa-79ee32f08639",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lead1 = np.load('/glade/work/ksha/NCAR/TRAIN_MINMAX_lead{}.npy'.format(lead1), allow_pickle=True)\n",
    "data_lead2 = np.load('/glade/work/ksha/NCAR/TRAIN_MINMAX_lead{}.npy'.format(lead2), allow_pickle=True)\n",
    "data_lead3 = np.load('/glade/work/ksha/NCAR/TRAIN_MINMAX_lead{}.npy'.format(lead3), allow_pickle=True)\n",
    "data_lead4 = np.load('/glade/work/ksha/NCAR/TRAIN_MINMAX_lead{}.npy'.format(lead4), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a620c5b-7718-445b-871b-79ae19074f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MAX_lead1 = data_lead1[()]['TRAIN_MAX']\n",
    "TRAIN_MEAN_lead1 = data_lead1[()]['TRAIN_MEAN']\n",
    "\n",
    "TRAIN_MAX_lead2 = data_lead2[()]['TRAIN_MAX']\n",
    "TRAIN_MEAN_lead2 = data_lead2[()]['TRAIN_MEAN']\n",
    "\n",
    "TRAIN_MAX_lead3 = data_lead3[()]['TRAIN_MAX']\n",
    "TRAIN_MEAN_lead3 = data_lead3[()]['TRAIN_MEAN']\n",
    "\n",
    "TRAIN_MAX_lead4 = data_lead4[()]['TRAIN_MAX']\n",
    "TRAIN_MEAN_lead4 = data_lead4[()]['TRAIN_MEAN']\n",
    "\n",
    "TEST_MAX_lead1 = data_lead1[()]['TEST_MAX']\n",
    "TEST_MEAN_lead1 = data_lead1[()]['TEST_MEAN']\n",
    "\n",
    "TEST_MAX_lead2 = data_lead2[()]['TEST_MAX']\n",
    "TEST_MEAN_lead2 = data_lead2[()]['TEST_MEAN']\n",
    "\n",
    "TEST_MAX_lead3 = data_lead3[()]['TEST_MAX']\n",
    "TEST_MEAN_lead3 = data_lead3[()]['TEST_MEAN']\n",
    "\n",
    "TEST_MAX_lead4 = data_lead4[()]['TEST_MAX']\n",
    "TEST_MEAN_lead4 = data_lead4[()]['TEST_MEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52775524-c37b-48d2-ad77-0bbf87a06db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IND_TRAIN_lead = np.load('/glade/work/ksha/NCAR/IND_TRAIN_lead_full.npy', allow_pickle=True)[()]\n",
    "TRAIN_ind1_v3 = IND_TRAIN_lead['lead{}'.format(lead1)]\n",
    "TRAIN_ind2_v3 = IND_TRAIN_lead['lead{}'.format(lead2)]\n",
    "TRAIN_ind3_v3 = IND_TRAIN_lead['lead{}'.format(lead3)]\n",
    "TRAIN_ind4_v3 = IND_TRAIN_lead['lead{}'.format(lead4)]\n",
    "\n",
    "IND_VALID_lead = np.load('/glade/work/ksha/NCAR/IND_VALID_lead_full.npy', allow_pickle=True)[()]\n",
    "VALID_ind1_v3 = IND_VALID_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2_v3 = IND_VALID_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3_v3 = IND_VALID_lead['lead{}'.format(lead3)]\n",
    "VALID_ind4_v3 = IND_VALID_lead['lead{}'.format(lead4)]\n",
    "\n",
    "IND_TEST_lead = np.load('/glade/work/ksha/NCAR/IND_TEST_lead_v4.npy', allow_pickle=True)[()]\n",
    "\n",
    "TEST_ind1 = IND_TEST_lead['lead{}'.format(lead1)]\n",
    "TEST_ind2 = IND_TEST_lead['lead{}'.format(lead2)]\n",
    "TEST_ind3 = IND_TEST_lead['lead{}'.format(lead3)]\n",
    "TEST_ind4 = IND_TEST_lead['lead{}'.format(lead4)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48cc2413-3aa1-47db-b6db-92bd197254e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f003840-165d-4ad3-b707-18734e18297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(TRAIN_ind2_v3)\n",
    "\n",
    "filename_train1_pick_v3 = []\n",
    "filename_train2_pick_v3 = []\n",
    "filename_train3_pick_v3 = []\n",
    "filename_train4_pick_v3 = []\n",
    "\n",
    "TRAIN_X_lead1 = np.empty((L, 30))\n",
    "TRAIN_X_lead2 = np.empty((L, 30))\n",
    "TRAIN_X_lead3 = np.empty((L, 30))\n",
    "TRAIN_X_lead4 = np.empty((L, 30))\n",
    "\n",
    "TRAIN_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(TRAIN_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(TRAIN_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(TRAIN_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(TRAIN_ind4_v3[i])\n",
    "    \n",
    "    filename_train1_pick_v3.append(filename_train_lead1[ind_lead1_v3])\n",
    "    filename_train2_pick_v3.append(filename_train_lead2[ind_lead2_v3])\n",
    "    filename_train3_pick_v3.append(filename_train_lead3[ind_lead3_v3])\n",
    "    filename_train4_pick_v3.append(filename_train_lead4[ind_lead4_v3])\n",
    "    \n",
    "    TRAIN_X_lead1[i, 0:15] = TRAIN_MAX_lead1[ind_lead1_v3, :]\n",
    "    TRAIN_X_lead1[i, 15:30] = TRAIN_MEAN_lead1[ind_lead1_v3, :]\n",
    "    \n",
    "    TRAIN_X_lead2[i, 0:15] = TRAIN_MAX_lead2[ind_lead2_v3, :]\n",
    "    TRAIN_X_lead2[i, 15:30] = TRAIN_MEAN_lead2[ind_lead2_v3, :]\n",
    "    \n",
    "    TRAIN_X_lead3[i, 0:15] = TRAIN_MAX_lead3[ind_lead3_v3, :]\n",
    "    TRAIN_X_lead3[i, 15:30] = TRAIN_MEAN_lead3[ind_lead3_v3, :]\n",
    "    \n",
    "    TRAIN_X_lead4[i, 0:15] = TRAIN_MAX_lead4[ind_lead4_v3, :]\n",
    "    TRAIN_X_lead4[i, 15:30] = TRAIN_MEAN_lead4[ind_lead4_v3, :]\n",
    "    \n",
    "    #TRAIN_Y_v3[i] = TRAIN_lead3_y_v3[ind_lead3_v3]\n",
    "    \n",
    "    if 'pos' in filename_train_lead3[ind_lead3_v3]:\n",
    "        TRAIN_Y_v3[i] = 1.0\n",
    "    else:\n",
    "        TRAIN_Y_v3[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd078c-3672-4849-bef8-59a187fb7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#where valid starts\n",
    "L_valid1 = len(filename_train_lead1)\n",
    "L_valid2 = len(filename_train_lead2)\n",
    "L_valid3 = len(filename_train_lead3)\n",
    "L_valid4 = len(filename_train_lead4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d17396ac-cb8b-4431-8a00-add3f704e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(VALID_ind2_v3)\n",
    "\n",
    "filename_valid1_pick_v3 = []\n",
    "filename_valid2_pick_v3 = []\n",
    "filename_valid3_pick_v3 = []\n",
    "filename_valid4_pick_v3 = []\n",
    "\n",
    "VALID_X_lead1 = np.empty((L, 30))\n",
    "VALID_X_lead2 = np.empty((L, 30))\n",
    "VALID_X_lead3 = np.empty((L, 30))\n",
    "VALID_X_lead4 = np.empty((L, 30))\n",
    "\n",
    "VALID_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(VALID_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(VALID_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(VALID_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(VALID_ind4_v3[i])\n",
    "    \n",
    "    filename_valid1_pick_v3.append(filename_valid_lead1[ind_lead1_v3])\n",
    "    filename_valid2_pick_v3.append(filename_valid_lead2[ind_lead2_v3])\n",
    "    filename_valid3_pick_v3.append(filename_valid_lead3[ind_lead3_v3])\n",
    "    filename_valid4_pick_v3.append(filename_valid_lead4[ind_lead4_v3])\n",
    "    \n",
    "    VALID_X_lead1[i, 0:15] = TRAIN_MAX_lead1[ind_lead1_v3+L_valid1, :]\n",
    "    VALID_X_lead1[i, 15:30] = TRAIN_MEAN_lead1[ind_lead1_v3+L_valid1, :]\n",
    "    \n",
    "    VALID_X_lead2[i, 0:15] = TRAIN_MAX_lead2[ind_lead2_v3+L_valid2, :]\n",
    "    VALID_X_lead2[i, 15:30] = TRAIN_MEAN_lead2[ind_lead2_v3+L_valid2, :]\n",
    "    \n",
    "    VALID_X_lead3[i, 0:15] = TRAIN_MAX_lead3[ind_lead3_v3+L_valid3, :]\n",
    "    VALID_X_lead3[i, 15:30] = TRAIN_MEAN_lead3[ind_lead3_v3+L_valid3, :]\n",
    "    \n",
    "    VALID_X_lead4[i, 0:15] = TRAIN_MAX_lead4[ind_lead4_v3+L_valid4, :]\n",
    "    VALID_X_lead4[i, 15:30] = TRAIN_MEAN_lead4[ind_lead4_v3+L_valid4, :]\n",
    "    \n",
    "    #TRAIN_Y_v3[i] = TRAIN_lead3_y_v3[ind_lead3_v3]\n",
    "    \n",
    "    if 'pos' in filename_valid_lead3[ind_lead3_v3]:\n",
    "        VALID_Y_v3[i] = 1.0\n",
    "    else:\n",
    "        VALID_Y_v3[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "815843fb-10d4-49a0-92d5-64f5647458ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_lead1 = np.concatenate((TRAIN_X_lead1, VALID_X_lead1), axis=0)\n",
    "ALL_lead2 = np.concatenate((TRAIN_X_lead2, VALID_X_lead2), axis=0)\n",
    "ALL_lead3 = np.concatenate((TRAIN_X_lead3, VALID_X_lead3), axis=0)\n",
    "ALL_lead4 = np.concatenate((TRAIN_X_lead4, VALID_X_lead4), axis=0)\n",
    "\n",
    "ALL_X = np.concatenate((ALL_lead1, ALL_lead2, ALL_lead3, ALL_lead4), axis=1)\n",
    "ALL_Y = np.concatenate((TRAIN_Y_v3, VALID_Y_v3), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03634420-bf8f-4059-a273-9690e8928ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_pos_v3 = ALL_X[ALL_Y==1, ...]\n",
    "TRAIN_neg_v3 = ALL_X[ALL_Y==0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "173d0072-00c3-487e-9a6d-2c209f1861c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(TEST_ind2)\n",
    "\n",
    "filename_test1_pick_v3 = []\n",
    "filename_test2_pick_v3 = []\n",
    "filename_test3_pick_v3 = []\n",
    "filename_test4_pick_v3 = []\n",
    "\n",
    "TEST_X_lead1 = np.empty((L, 30))\n",
    "TEST_X_lead2 = np.empty((L, 30))\n",
    "TEST_X_lead3 = np.empty((L, 30))\n",
    "TEST_X_lead4 = np.empty((L, 30))\n",
    "\n",
    "TEST_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(TEST_ind1[i])\n",
    "    ind_lead2_v3 = int(TEST_ind2[i])\n",
    "    ind_lead3_v3 = int(TEST_ind3[i])\n",
    "    ind_lead4_v3 = int(TEST_ind4[i])\n",
    "    \n",
    "    filename_test1_pick_v3.append(filename_test_lead1[ind_lead1_v3])\n",
    "    filename_test2_pick_v3.append(filename_test_lead2[ind_lead2_v3])\n",
    "    filename_test3_pick_v3.append(filename_test_lead3[ind_lead3_v3])\n",
    "    filename_test4_pick_v3.append(filename_test_lead4[ind_lead4_v3])\n",
    "    \n",
    "    TEST_X_lead1[i, 0:15] = TEST_MAX_lead1[ind_lead1_v3, :]\n",
    "    TEST_X_lead1[i, 15:30] = TEST_MEAN_lead1[ind_lead1_v3, :]\n",
    "    \n",
    "    TEST_X_lead2[i, 0:15] = TEST_MAX_lead2[ind_lead2_v3, :]\n",
    "    TEST_X_lead2[i, 15:30] = TEST_MEAN_lead2[ind_lead2_v3, :]\n",
    "    \n",
    "    TEST_X_lead3[i, 0:15] = TEST_MAX_lead3[ind_lead3_v3, :]\n",
    "    TEST_X_lead3[i, 15:30] = TEST_MEAN_lead3[ind_lead3_v3, :]\n",
    "    \n",
    "    TEST_X_lead4[i, 0:15] = TEST_MAX_lead4[ind_lead4_v3, :]\n",
    "    TEST_X_lead4[i, 15:30] = TEST_MEAN_lead4[ind_lead4_v3, :]\n",
    "    \n",
    "    #TRAIN_Y_v3[i] = TRAIN_lead3_y_v3[ind_lead3_v3]\n",
    "    \n",
    "    if 'pos' in filename_test_lead3[ind_lead3_v3]:\n",
    "        TEST_Y_v3[i] = 1.0\n",
    "    else:\n",
    "        TEST_Y_v3[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fd3b836-fe31-4185-a1b2-8e8f90d5acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TEST_X = np.concatenate((TEST_X_lead1, TEST_X_lead2, TEST_X_lead3, TEST_X_lead4), axis=1)\n",
    "ALL_TEST_Y = TEST_Y_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a309b54-3129-44a2-a8fa-5c3c7a54dd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/TESTv4_day029_neg_neg_neg_indx18_indy49_lead5.npy'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_test4_pick_v3[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3629c474-ec3d-4a48-871d-6a8188e40761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "    \n",
    "#     IN = keras.Input((4, 30))\n",
    "#     X = IN\n",
    "#     X = keras.layers.Conv1D(128, kernel_size=2, strides=1, padding='valid')(X)\n",
    "#     X = keras.layers.Activation(\"gelu\")(X)\n",
    "    \n",
    "#     X = keras.layers.GlobalMaxPool1D()(X) #X = keras.layers.Flatten()(X)\n",
    "\n",
    "#     X = keras.layers.Dense(64)(X)\n",
    "#     X = keras.layers.Activation(\"relu\")(X)\n",
    "#     X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "#     OUT = X\n",
    "#     OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "#     model = keras.models.Model(inputs=IN, outputs=OUT)\n",
    "#     return model\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    \n",
    "    IN = keras.Input((120,))\n",
    "    X = IN\n",
    "    #\n",
    "    X = keras.layers.Dense(64, activity_regularizer=keras.regularizers.L2(1e-2))(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN, outputs=OUT)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4450a-f5db-4edb-8d23-5ad4c1a1b537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d82c7f9-5f57-405a-b06b-9e4de5b8abd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 1.1\n",
      "Training round 0\n",
      "Validation loss improved from 1.1 to 0.9998896972320412\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/test_lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/test_lead4/assets\n",
      "--- 5.178749322891235 seconds ---\n",
      "Validation loss 1.0007726997289714 NOT improved\n",
      "Validation loss 1.0036550397416941 NOT improved\n",
      "Validation loss 1.0073285917394195 NOT improved\n",
      "Validation loss 1.013033279568838 NOT improved\n",
      "Early stopping\n",
      "Training round 1\n",
      "Validation loss improved from 0.9998896972320412 to 0.9928250850136829\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/test_lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/test_lead4/assets\n",
      "--- 4.9707653522491455 seconds ---\n",
      "Validation loss 0.9941025732125967 NOT improved\n",
      "Validation loss 0.9950920151546303 NOT improved\n",
      "Validation loss 0.9964146942967965 NOT improved\n",
      "Validation loss 0.9989560517935429 NOT improved\n",
      "Validation loss 1.0050502482948152 NOT improved\n",
      "Validation loss 1.014393345505942 NOT improved\n",
      "Early stopping\n",
      "Training round 2\n",
      "Validation loss 0.9999577464439225 NOT improved\n",
      "Validation loss 0.9999299209722906 NOT improved\n",
      "Validation loss 0.9998866359182519 NOT improved\n",
      "Validation loss 0.9998218979585817 NOT improved\n",
      "Validation loss 0.9997303655759338 NOT improved\n",
      "Validation loss 0.9996237496960562 NOT improved\n",
      "Validation loss 0.9994555924234987 NOT improved\n",
      "Validation loss 0.9992130813920272 NOT improved\n",
      "Validation loss 0.9988725870083631 NOT improved\n",
      "Validation loss 0.9984828584371784 NOT improved\n",
      "Validation loss 0.9980779400913201 NOT improved\n",
      "Validation loss 0.997638371140411 NOT improved\n",
      "Validation loss 0.9972524400687288 NOT improved\n",
      "Validation loss 0.9972020727487073 NOT improved\n",
      "Validation loss 0.9974765160686413 NOT improved\n",
      "Validation loss 0.9981933685036087 NOT improved\n",
      "Validation loss 0.9995043274805171 NOT improved\n",
      "Validation loss 1.0021169907480558 NOT improved\n",
      "Validation loss 1.0047072753606945 NOT improved\n",
      "Validation loss 1.0080789753832535 NOT improved\n",
      "Validation loss 1.0112969701161796 NOT improved\n",
      "Early stopping\n",
      "Training round 3\n",
      "Validation loss 0.9998655934390106 NOT improved\n",
      "Validation loss 1.0003714486389903 NOT improved\n",
      "Validation loss 1.002896711621321 NOT improved\n",
      "Validation loss 1.0070719867183355 NOT improved\n",
      "Validation loss 1.011957480271879 NOT improved\n",
      "Early stopping\n",
      "Training round 4\n",
      "Validation loss 0.9993352545089065 NOT improved\n",
      "Validation loss 0.9991277419360297 NOT improved\n",
      "Validation loss 0.9988250839949323 NOT improved\n",
      "Validation loss 0.9988430130920442 NOT improved\n",
      "Validation loss 0.9996035289758382 NOT improved\n",
      "Validation loss 1.0010609361692988 NOT improved\n",
      "Validation loss 1.00357587705889 NOT improved\n",
      "Validation loss 1.0073954344101408 NOT improved\n",
      "Validation loss 1.0129005124301909 NOT improved\n",
      "Early stopping\n",
      "Training round 5\n",
      "Validation loss 0.9996800383469356 NOT improved\n",
      "Validation loss 0.9994111441645793 NOT improved\n",
      "Validation loss 0.9990139449592986 NOT improved\n",
      "Validation loss 0.9985183708197787 NOT improved\n",
      "Validation loss 0.9979959849090909 NOT improved\n",
      "Validation loss 0.9970266783554559 NOT improved\n",
      "Validation loss 0.9961188487393995 NOT improved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x2b706c314b80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/work/ksha/anaconda3/lib/python3.9/weakref.py\", line 370, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695, \n",
    "         2463725, 4734, 23234, 884, 2341, 362, 5, 234, 483, 785356, 23425, 3621, \n",
    "         58461, 80968765, 123, 425633, 5646, 67635, 76785, 34214]\n",
    "\n",
    "training_rounds = len(seeds)\n",
    "\n",
    "ref = np.sum(ALL_TEST_Y) / len(ALL_TEST_Y)\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = '{}_lead{}'.format('test', 4)\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(TRAIN_pos_v3)\n",
    "L_neg = len(TRAIN_neg_v3)\n",
    "\n",
    "record = 1.1\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "L_train = 16\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    if r == 0:\n",
    "        tol = 0\n",
    "    else:\n",
    "        tol = -200\n",
    "\n",
    "    model = create_model()\n",
    "    #\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "    \n",
    "    set_seeds(int(seeds[r]))\n",
    "    print('Training round {}'.format(r))\n",
    "\n",
    "    for i in range(epochs):            \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            N_pos = 32\n",
    "            N_neg = batch_size - N_pos\n",
    "\n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "            ind_neg_pick = ind_neg[:N_neg]\n",
    "            ind_pos_pick = ind_pos[:N_pos]\n",
    "\n",
    "            X_batch_neg = TRAIN_neg_v3[ind_neg_pick, :]\n",
    "            X_batch_pos = TRAIN_pos_v3[ind_pos_pick, :]\n",
    "\n",
    "            X_batch = np.concatenate((X_batch_neg, X_batch_pos), axis=0)\n",
    "\n",
    "            Y_batch = np.ones([batch_size,])\n",
    "            Y_batch[:N_neg] = 0.0\n",
    "\n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "\n",
    "            X_batch = X_batch[ind_, :]\n",
    "            Y_batch = Y_batch[ind_]\n",
    "\n",
    "            # train on batch\n",
    "            model.train_on_batch(X_batch, Y_batch);\n",
    "\n",
    "        # epoch end operations\n",
    "        Y_pred = model.predict([ALL_TEST_X,])\n",
    "\n",
    "        Y_pred[Y_pred<0] = 0\n",
    "        Y_pred[Y_pred>1] = 1\n",
    "\n",
    "        record_temp = verif_metric(ALL_TEST_Y, Y_pred, ref)\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     model.save(model_path_backup)\n",
    "\n",
    "        if (record - record_temp > min_del):\n",
    "            print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "            record = record_temp\n",
    "            tol = 0\n",
    "            \n",
    "            #print('tol: {}'.format(tol))\n",
    "            # save\n",
    "            print('save to: {}'.format(model_path))\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            print('Validation loss {} NOT improved'.format(record_temp))\n",
    "            if record_temp > 1.01:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f85ae1b-2f0b-4909-a993-14e06679b4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3a24c-ba02-4c0e-bcc0-ae98eaad1167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3625c49-eb84-40cd-b90d-273008ac4a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
