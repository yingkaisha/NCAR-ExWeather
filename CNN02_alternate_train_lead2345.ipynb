{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e872fe44-4992-44df-8b73-efd3f8ae2c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 16:34:31.796051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5590427b-a292-43d2-bd5e-0dc3862c7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "weights_round = 5\n",
    "save_round = 1\n",
    "seeds = 777\n",
    "# ==================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d62389b-486a-40a6-96f3-a8034ee8cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "def create_model(input_shape=(64, 64, 15)):\n",
    "\n",
    "    depths=[3, 3, 27, 3]\n",
    "    projection_dims=[32, 64, 96, 128]\n",
    "    drop_path_rate=0.0\n",
    "    layer_scale_init_value=1e-6\n",
    "\n",
    "\n",
    "    model_name='Branch64X'\n",
    "    IN64 = layers.Input(shape=input_shape)\n",
    "    X = IN64\n",
    "    # ----- convnext block 0 ----- #\n",
    "\n",
    "    X = layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=\"{}_down0\".format(model_name))(X)\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_norm\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[0]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[0], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[0], name=\"{}_down0_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[0], name=\"{}_down0_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down0_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[0], name=\"{}_down0_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[0], name=\"{}_down0_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "\n",
    "    # ----- convnext block 1 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[1], kernel_size=2, strides=2, name=\"{}_down1\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[1]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[1], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[1], name=\"{}_down1_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[1], name=\"{}_down1_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down1_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[1], name=\"{}_down1_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[1], name=\"{}_down1_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 2 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[2], kernel_size=2, strides=2, name=\"{}_down2\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[2]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[2], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[2], name=\"{}_down2_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[2], name=\"{}_down2_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down2_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[2], name=\"{}_down2_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[2], name=\"{}_down2_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 3 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[3], kernel_size=2, padding='same', name=\"{}_down3\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[3]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[3], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[3], name=\"{}_down3_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[3], name=\"{}_down3_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down3_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[3], name=\"{}_down3_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[3], name=\"{}_down3_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    V1 = X\n",
    "\n",
    "    OUT = layers.GlobalMaxPooling2D(name=\"{}_head_pool64\".format(model_name))(V1)\n",
    "\n",
    "    model = Model(inputs=IN64, outputs=OUT, name=model_name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_head():\n",
    "\n",
    "    \n",
    "    IN_vec = keras.Input((128,))    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def verif_metric(VALID_target, Y_pred):\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    \n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9957d59d-cb4b-450d-bba4-6f7616a6610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3, lead2, pos: 5510, neg: 887822\n",
      "v3, lead3, pos: 4297, neg: 889035\n",
      "v3, lead4, pos: 3242, neg: 890090\n",
      "v3, lead5, pos: 2565, neg: 891795\n",
      "v4x, lead2, pos: 1995, neg: 348553\n",
      "v4x, lead3, pos: 1618, neg: 349958\n",
      "v4x, lead4, pos: 1218, neg: 350358\n",
      "v4x, lead5, pos: 962, neg: 350614\n",
      "v4, lead2, pos: 2174, neg: 402858\n",
      "v4, lead3, pos: 1724, neg: 403308\n",
      "v4, lead4, pos: 1349, neg: 403683\n",
      "v4, lead5, pos: 1034, neg: 403998\n",
      "pos train: 4856 pos valid: 654 neg train: 742500 neg valid 145322\n",
      "pos train: 3787 pos valid: 510 neg train: 743569 neg valid 145466\n",
      "pos train: 2874 pos valid: 368 neg train: 744482 neg valid 145608\n",
      "pos train: 2289 pos valid: 276 neg train: 746095 neg valid 145700\n",
      "pos train: 1452 pos valid: 543 neg train: 225736 neg valid 122817\n",
      "pos train: 1187 pos valid: 431 neg train: 226001 neg valid 123957\n",
      "pos train: 898 pos valid: 320 neg train: 226290 neg valid 124068\n",
      "pos train: 721 pos valid: 241 neg train: 226467 neg valid 124147\n",
      "pos train: 0 pos valid: 13 neg train: 0 neg valid 29799\n",
      "pos train: 0 pos valid: 10 neg train: 0 neg valid 29802\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n"
     ]
    }
   ],
   "source": [
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "L_vars = len(ind_pick_from_batch)\n",
    "\n",
    "vers = ['v3', 'v4x', 'v4']\n",
    "leads = [2, 3, 4, 5,]\n",
    "filenames_pos = {}\n",
    "filenames_neg = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        if ver == 'v3' and lead < 23:\n",
    "            path_ = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "        elif ver == 'v3' and lead == 23:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "        elif ver == 'v4':\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "        else:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "            \n",
    "        filenames_pos['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*pos*lead{}.npy\".format(path_, lead)))\n",
    "        filenames_neg['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*neg_neg_neg*lead{}.npy\".format(path_, lead)))\n",
    "        \n",
    "        print('{}, lead{}, pos: {}, neg: {}'.format(ver, lead, \n",
    "                                                    len(filenames_pos['{}_lead{}'.format(ver, lead)]), \n",
    "                                                    len(filenames_neg['{}_lead{}'.format(ver, lead)])))\n",
    "\n",
    "def name_extract(filenames):\n",
    "    \n",
    "    date_base = datetime(2020, 7, 14)\n",
    "    date_base2 = datetime(2021, 1, 1)\n",
    "    \n",
    "    filename_train = []\n",
    "    filename_valid = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+365+30)]\n",
    "    \n",
    "    base_ref = datetime(2019, 10, 1)\n",
    "    date_list_v4x = [base_ref + timedelta(days=day) for day in range(429)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4x' in name:\n",
    "            date_list = date_list_v4x\n",
    "        elif 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        \n",
    "        if (day - date_base).days < 0:\n",
    "            filename_train.append(name)\n",
    "            \n",
    "        else:\n",
    "            if (day - date_base2).days < 0:\n",
    "                filename_valid.append(name)\n",
    "\n",
    "        \n",
    "    return filename_train, filename_valid\n",
    "\n",
    "filenames_pos_train = {}\n",
    "filenames_neg_train = {}\n",
    "\n",
    "filenames_pos_valid = {}\n",
    "filenames_neg_valid = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        temp_namelist_pos = filenames_pos['{}_lead{}'.format(ver, lead)]\n",
    "        temp_namelist_neg = filenames_neg['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "        pos_train, pos_valid = name_extract(temp_namelist_pos)\n",
    "        neg_train, neg_valid = name_extract(temp_namelist_neg)\n",
    "        \n",
    "        print('pos train: {} pos valid: {} neg train: {} neg valid {}'.format(len(pos_train), \n",
    "                                                                              len(pos_valid), \n",
    "                                                                              len(neg_train), \n",
    "                                                                              len(neg_valid)))\n",
    "        \n",
    "        filenames_pos_train['{}_lead{}'.format(ver, lead)] = pos_train\n",
    "        filenames_neg_train['{}_lead{}'.format(ver, lead)] = neg_train\n",
    "        \n",
    "        filenames_pos_valid['{}_lead{}'.format(ver, lead)] = pos_valid\n",
    "        filenames_neg_valid['{}_lead{}'.format(ver, lead)] = neg_valid\n",
    "        \n",
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "pos_valid_all = []\n",
    "neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcba5024-528a-4639-a399-d1b699afc4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7404\n"
     ]
    }
   ],
   "source": [
    "filename_valid = neg_valid_all[::150] + pos_valid_all[::15]\n",
    "print(len(filename_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aed1ebc-fd99-438f-8723-2da7495cad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_valid = len(filename_valid)\n",
    "\n",
    "VALID_input_64 = np.empty((L_valid, 64, 64, L_vars))\n",
    "VALID_target = np.ones(L_valid)\n",
    "\n",
    "for i, name in enumerate(filename_valid):\n",
    "    data = np.load(name)\n",
    "    for k, c in enumerate(ind_pick_from_batch):\n",
    "        \n",
    "        VALID_input_64[i, ..., k] = data[..., c]\n",
    "\n",
    "        if 'pos' in name:\n",
    "            VALID_target[i] = 1.0\n",
    "        else:\n",
    "            VALID_target[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2219a0-b2a4-4b4c-89b7-54a4838c6ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 16:52:51.149598: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-22 16:52:51.151292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-22 16:52:51.223703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:62:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-22 16:52:51.223742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-22 16:52:51.287705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-22 16:52:51.287780: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-22 16:52:51.334149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-22 16:52:51.369543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-22 16:52:51.404131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-22 16:52:51.446643: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-22 16:52:51.484160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-22 16:52:51.484939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-22 16:52:51.485503: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-22 16:52:51.485749: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-22 16:52:51.486206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:62:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-22 16:52:51.486239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-22 16:52:51.486262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-22 16:52:51.486274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-22 16:52:51.486287: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-22 16:52:51.486299: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-22 16:52:51.486312: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-22 16:52:51.486324: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-22 16:52:51.486337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-22 16:52:51.486954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-22 16:52:51.486999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-22 16:52:52.067122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-22 16:52:52.067164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-02-22 16:52:52.067182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-02-22 16:52:52.068299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0)\n",
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n",
      "2023-02-22 16:53:16.228772: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-02-22 16:53:16.229290: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n",
      "2023-02-22 16:53:16.255111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-22 16:53:20.069556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015812163741361025\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/glade/work/ksha/NCAR/'\n",
    "\n",
    "flag_train = 'base'\n",
    "\n",
    "if flag_train == 'head':\n",
    "    flag_weights = 'base'\n",
    "else:\n",
    "    flag_weights = 'head'\n",
    "    \n",
    "model_head = create_model_head()\n",
    "model_base = create_model(input_shape=(64, 64, 15))\n",
    "\n",
    "IN = layers.Input(shape=(64, 64, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = Model(inputs=IN, outputs=OUT)\n",
    "\n",
    "W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_base{}/'.format(weights_round))\n",
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "model_final.set_weights(W_old)\n",
    "\n",
    "Y_pred = model_final.predict([VALID_input_64])\n",
    "record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7f3a3-c9ee-4b4d-9e49-56b393204864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 0.015812163741361025\n",
      "0.0159261462523178\n",
      "Validation loss 0.0159261462523178 NOT improved\n",
      "0.019557609727166875\n",
      "Validation loss 0.019557609727166875 NOT improved\n",
      "0.021551814944199405\n",
      "Validation loss 0.021551814944199405 NOT improved\n",
      "0.015518378315339702\n",
      "Validation loss improved from 0.015812163741361025 to 0.015518378315339702\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_peak_base1_lead2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 17:24:36.254811: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_peak_base1_lead2345/assets\n",
      "--- 471.66225266456604 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015823427175657667\n",
      "Validation loss 0.015823427175657667 NOT improved\n",
      "0.017076742393159306\n",
      "Validation loss 0.017076742393159306 NOT improved\n",
      "0.017290431363160427\n",
      "Validation loss 0.017290431363160427 NOT improved\n",
      "0.01635744903079458\n",
      "Validation loss 0.01635744903079458 NOT improved\n",
      "0.016879369008189187\n",
      "Validation loss 0.016879369008189187 NOT improved\n",
      "0.036685168511141536\n",
      "Validation loss 0.036685168511141536 NOT improved\n",
      "0.018015319557388786\n",
      "Validation loss 0.018015319557388786 NOT improved\n",
      "0.01873090229414648\n",
      "Validation loss 0.01873090229414648 NOT improved\n",
      "0.01680520276720369\n",
      "Validation loss 0.01680520276720369 NOT improved\n",
      "0.015923096860257904\n",
      "Validation loss 0.015923096860257904 NOT improved\n",
      "0.017320289442348747\n",
      "Validation loss 0.017320289442348747 NOT improved\n"
     ]
    }
   ],
   "source": [
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 64 #int(len(TRAIN_Y_pick) / batch_size)\n",
    "\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "key = 'RE2_peak_{}{}_lead{}{}{}{}'.format(flag_train, save_round, leads[0], leads[1], leads[2], leads[3],)\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp #0.01840167896363949 #record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        if flag_train == 'base':\n",
    "            N_pos = 20\n",
    "        else:\n",
    "            N_pos = 20\n",
    "            \n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "        file_pick_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "\n",
    "        file_pick_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "\n",
    "        if len(file_pick) != batch_size:\n",
    "            sregwet\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "\n",
    "            for l, c in enumerate(ind_pick_from_batch):\n",
    "                temp = data[..., c] \n",
    "                X_batch_64[k, ..., l] = temp\n",
    "\n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "            elif 'neg_neg_neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "\n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_64 = X_batch_64[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_64, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_64])\n",
    "    # Y_pred[Y_pred<0] = 0\n",
    "    # Y_pred[Y_pred>1] = 1\n",
    "\n",
    "    record_temp = verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     model.save(model_path_backup)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        #print('tol: {}'.format(tol))\n",
    "        # save\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97eb8f-7aae-499f-81e6-0b4ac7f1c48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
