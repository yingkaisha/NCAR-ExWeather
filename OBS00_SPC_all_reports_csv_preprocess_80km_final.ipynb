{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf179e2-3bad-4c48-81c1-2a43b404537b",
   "metadata": {},
   "source": [
    "# Pre-process SPC strom reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eac6a22-a283-4b10-951d-5e25bf48ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364dd02f-1606-4374-a4c1-e0bba7fdbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import preprocess_utils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38c13c3-622d-41a0-92fb-aceca6a59836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f739e3e-9195-413c-947a-2e62df53f103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from '/glade/u/home/ksha/NCAR/libs/data_utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd29a500-0cc5-431c-a4f0-23c8f4f59910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/work/ksha/NCAR/storm_report/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbed5f69-818d-468a-b49b-b74fafaacb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95089537-ab56-4e1d-92a1-6d668c014d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d322a4-35a8-4b4d-8f62-fedfe9893287",
   "metadata": {},
   "source": [
    "### Domain info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6258e53c-b972-4869-8a49-52c58a3e90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(save_dir+'HRRR_domain.hdf', 'r') as h5io:\n",
    "    lon_72km = h5io['lon_80km'][...]\n",
    "    lat_72km = h5io['lat_80km'][...]\n",
    "    lon_3km = h5io['lon_3km'][...]\n",
    "    lat_3km = h5io['lat_3km'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c024c6f-1716-496c-befe-d3094fc047a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preserve_inds = [4, 5, 6, 10, 15, 16]\n",
    "column_names = ['date', 'time', 'tz', 'mag', 'slat', 'slon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f0aa29-a1af-4595-9940-aba6bf683762",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2010\n",
    "file_torn = sorted(glob(report_dir+'{}_torn.csv'.format(year)))[0]\n",
    "df_torn = pd.read_csv(file_torn)\n",
    "df_torn = df_torn.iloc[:, preserve_inds]\n",
    "df_torn.columns = column_names\n",
    "\n",
    "file_wind = sorted(glob(report_dir+'{}_wind.csv'.format(year)))[0]\n",
    "df_wind = pd.read_csv(file_wind)\n",
    "df_wind = df_wind.iloc[:, preserve_inds]\n",
    "df_wind.columns = column_names\n",
    "\n",
    "file_hail = sorted(glob(report_dir+'{}_hail.csv'.format(year)))[0]\n",
    "df_hail = pd.read_csv(file_hail)\n",
    "df_hail = df_hail.iloc[:, preserve_inds]\n",
    "df_hail.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541f0844-ae47-4d6d-a2b5-42dea29cfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88358e3f-cd80-464f-8ac1-97d705b94e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [df_torn, df_wind, df_hail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915fa309-3cec-4994-a781-9762cdb9e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/scratch/ksha/ipykernel_136423/1414804474.py:1: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_merged = reduce(lambda  left,right: pd.merge(left, right, how='outer'), data_frames)\n"
     ]
    }
   ],
   "source": [
    "df_merged = reduce(lambda  left,right: pd.merge(left, right, how='outer'), data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2edb175d-ccc6-4128-9818-f68942384960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b80c1-a0a1-4d5c-852e-3a1d7ee84191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba293d3-f754-4aa1-b6e8-ecba9cda398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_v3_s = datetime(2018, 7, 15)\n",
    "base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "base_v4_s = datetime(2020, 12, 3)\n",
    "base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "base_ref = datetime(2010, 1, 1)\n",
    "\n",
    "date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+180-151)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed919a68-abb6-45b8-a419-8145b8494d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 12, 31, 0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list_v4[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e22df1-df13-4346-a039-ef351520dba1",
   "metadata": {},
   "source": [
    "**Merged version of all hazards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9107f-de8f-47d8-b913-e8ef38c97713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d118c6a7-f17c-412e-a38a-e32c1219204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve_inds = [4, 5, 6, 15, 16]\n",
    "# column_names = ['date', 'time', 'tz', 'slat', 'slon']\n",
    "\n",
    "# L_v3 = len(date_list_v3)\n",
    "# L_v4 = len(date_list_v4)\n",
    "\n",
    "# folds = 300\n",
    "\n",
    "# for lead in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]:\n",
    "\n",
    "#     #print('========== Processing lead time = {} =========='.format(lead))\n",
    "\n",
    "#     record_v3 = np.empty((L_v3, 2*folds))\n",
    "#     record_v3[...] = np.nan\n",
    "#     record_v4 = np.empty((L_v3, 2*folds))\n",
    "#     record_v4[...] = np.nan\n",
    "\n",
    "#     for y in range(2010, 2022):\n",
    "#         temp_day_old = 9999\n",
    "#         # Year info\n",
    "#         year_int = int(y)\n",
    "#         year = str(year_int)\n",
    "\n",
    "#         # Raw tornado files\n",
    "#         file_torn = sorted(glob(report_dir+'{}_torn.csv'.format(year)))[0]\n",
    "#         file_wind = sorted(glob(report_dir+'{}_wind.csv'.format(year)))[0]\n",
    "#         file_hail = sorted(glob(report_dir+'{}_hail.csv'.format(year)))[0]\n",
    "#         # import csv to pandas and then np.array\n",
    "#         df_torn = pd.read_csv(file_torn)\n",
    "#         df_torn = df_torn.iloc[:, preserve_inds]\n",
    "#         df_torn.columns = column_names\n",
    "        \n",
    "#         df_wind = pd.read_csv(file_wind)\n",
    "#         df_wind = df_wind.iloc[:, preserve_inds]\n",
    "#         df_wind.columns = column_names\n",
    "        \n",
    "#         df_hail = pd.read_csv(file_hail)\n",
    "#         df_hail = df_hail.iloc[:, preserve_inds]\n",
    "#         df_hail.columns = column_names\n",
    "        \n",
    "#         data_frames = [df_torn, df_wind, df_hail]\n",
    "#         df_merged = reduce(lambda  left,right: pd.merge(left, right, how='outer'), data_frames)\n",
    "        \n",
    "#         temp_array = df_merged.values\n",
    "\n",
    "#         # datetime and timezone processing\n",
    "#         L = len(temp_array)\n",
    "#         temp_tz = temp_array[:, 2]\n",
    "#         temp_dt_list = []\n",
    "#         flag_badboy = False\n",
    "\n",
    "#         for i in range(L):\n",
    "#             try:\n",
    "#                 # the string can be converted to datetime object\n",
    "#                 temp_localtime = datetime.strptime(temp_array[i, 0]+'|'+temp_array[i, 1], '%Y-%m-%d|%H:%M:%S')\n",
    "#                 flag_badboy = False\n",
    "#             except:\n",
    "#                 # the string cannot be converted; typically a \"?\"\n",
    "#                 temp_localtime = np.nan\n",
    "#                 flag_badboy = True\n",
    "\n",
    "#             # adjust timezones to UTC/GMT \n",
    "#             if flag_badboy is False:\n",
    "#                 temp_tz = temp_array[i, 2]\n",
    "#                 if temp_tz == 3:\n",
    "#                     temp_localtime = temp_localtime + timedelta(hours=6) # <--- !!! \"3\" means CST\n",
    "#                 elif temp_tz == 9:\n",
    "#                     temp_localtime = temp_localtime # \"9\" means GMT\n",
    "#                 else:\n",
    "#                     temp_localtime = np.nan # otherwise doint know\n",
    "\n",
    "#             temp_dt_list.append(temp_localtime)\n",
    "\n",
    "#         # Insert in-situ reports into hourly, gridded data frames    \n",
    "#         ## convert slat slon to domain indices\n",
    "\n",
    "#         slon = temp_array[:, 4]\n",
    "#         slat = temp_array[:, 3]\n",
    "\n",
    "#         flag_pick = np.logical_and(slon<-20, slat>5)\n",
    "\n",
    "#         slon = slon[flag_pick]\n",
    "#         slat = slat[flag_pick]\n",
    "\n",
    "#         L = len(slon)\n",
    "\n",
    "#         for i in range(L):\n",
    "\n",
    "#             # the time of a single record\n",
    "#             temp_datetime = temp_dt_list[i]\n",
    "#             temp_day = temp_datetime.day\n",
    "#             temp_hour = temp_datetime.hour\n",
    "\n",
    "#             temp_datetime_day = datetime(temp_datetime.year, temp_datetime.month, temp_day)\n",
    "\n",
    "#             if (temp_day_old == temp_day) is False:\n",
    "#                 count_v3 = 0\n",
    "#                 count_v4 = 0\n",
    "#                 temp_day_old = temp_day\n",
    "\n",
    "#             if temp_hour == lead:\n",
    "#                 diff_days = (temp_datetime_day - base_v4_s).days\n",
    "\n",
    "#                 if diff_days > 0:\n",
    "#                     #print('adding: {}'.format(temp_datetime))\n",
    "#                     record_v4[diff_days, 2*count_v4] = slon[i]\n",
    "#                     record_v4[diff_days, 2*count_v4+1] = slat[i]\n",
    "#                     count_v4 += 1\n",
    "\n",
    "#                 else:\n",
    "#                     diff_days = (temp_datetime_day - base_v3_s).days\n",
    "#                     if diff_days > 0:\n",
    "#                         #print('adding: {}'.format(temp_datetime))\n",
    "#                         record_v3[diff_days, 2*count_v3] = slon[i]\n",
    "#                         record_v3[diff_days, 2*count_v3+1] = slat[i]\n",
    "#                         count_v3 += 1\n",
    "\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#     tuple_save = (record_v3, record_v4)\n",
    "#     label_save = ['record_v3', 'record_v4']\n",
    "#     du.save_hdf5(tuple_save, label_save, save_dir_scratch, 'SPC_all_lead{}.hdf'.format(lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498e63b1-bbf9-4cfc-9f77-5a1c16548384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead0_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead1_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead2_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead3_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead4_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead5_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead6_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead7_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead8_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead9_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead10_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead11_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead12_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead13_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead14_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead15_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead16_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead17_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead18_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead19_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead20_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead21_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead22_old.hdf\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_all_lead23_old.hdf\n"
     ]
    }
   ],
   "source": [
    "# preserve_inds = [4, 5, 6, 15, 16]\n",
    "# column_names = ['date', 'time', 'tz', 'slat', 'slon']\n",
    "\n",
    "# L_v3 = len(date_list_v3)\n",
    "# L_v4 = len(date_list_v4)\n",
    "\n",
    "# folds = 300\n",
    "\n",
    "# for lead in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]:\n",
    "\n",
    "#     #print('========== Processing lead time = {} =========='.format(lead))\n",
    "\n",
    "#     record_v3 = np.empty((L_v3, 2*folds))\n",
    "#     record_v3[...] = np.nan\n",
    "#     record_v4 = np.empty((L_v3, 2*folds))\n",
    "#     record_v4[...] = np.nan\n",
    "\n",
    "#     for y in range(2010, 2022):\n",
    "#         temp_day_old = 9999\n",
    "#         # Year info\n",
    "#         year_int = int(y)\n",
    "#         year = str(year_int)\n",
    "\n",
    "#         # Raw tornado files\n",
    "#         file_torn = sorted(glob(report_dir+'{}_torn.csv'.format(year)))[0]\n",
    "#         file_wind = sorted(glob(report_dir+'{}_wind.csv'.format(year)))[0]\n",
    "#         file_hail = sorted(glob(report_dir+'{}_hail.csv'.format(year)))[0]\n",
    "#         # import csv to pandas and then np.array\n",
    "#         df_torn = pd.read_csv(file_torn)\n",
    "#         df_torn = df_torn.iloc[:, preserve_inds]\n",
    "#         df_torn.columns = column_names\n",
    "        \n",
    "#         df_wind = pd.read_csv(file_wind)\n",
    "#         df_wind = df_wind.iloc[:, preserve_inds]\n",
    "#         df_wind.columns = column_names\n",
    "        \n",
    "#         df_hail = pd.read_csv(file_hail)\n",
    "#         df_hail = df_hail.iloc[:, preserve_inds]\n",
    "#         df_hail.columns = column_names\n",
    "        \n",
    "#         data_frames = [df_torn, df_wind, df_hail]\n",
    "#         df_merged = reduce(lambda  left,right: pd.merge(left, right, how='outer'), data_frames)\n",
    "        \n",
    "#         temp_array = df_merged.values\n",
    "\n",
    "#         # datetime and timezone processing\n",
    "#         L = len(temp_array)\n",
    "#         temp_tz = temp_array[:, 2]\n",
    "#         temp_dt_list = []\n",
    "#         flag_badboy = False\n",
    "\n",
    "#         for i in range(L):\n",
    "#             try:\n",
    "#                 # the string can be converted to datetime object\n",
    "#                 temp_localtime = datetime.strptime(temp_array[i, 0]+'|'+temp_array[i, 1], '%Y-%m-%d|%H:%M:%S')\n",
    "#                 flag_badboy = False\n",
    "#             except:\n",
    "#                 # the string cannot be converted; typically a \"?\"\n",
    "#                 temp_localtime = np.nan\n",
    "#                 flag_badboy = True\n",
    "\n",
    "#             # adjust timezones to UTC/GMT \n",
    "#             if flag_badboy is False:\n",
    "#                 temp_tz = temp_array[i, 2]\n",
    "#                 if temp_tz == 3:\n",
    "#                     temp_localtime = temp_localtime + timedelta(hours=6) # <--- !!! \"3\" means CST\n",
    "#                 elif temp_tz == 9:\n",
    "#                     temp_localtime = temp_localtime # \"9\" means GMT\n",
    "#                 else:\n",
    "#                     temp_localtime = np.nan # otherwise doint know\n",
    "\n",
    "#             temp_dt_list.append(temp_localtime)\n",
    "\n",
    "#         # Insert in-situ reports into hourly, gridded data frames    \n",
    "#         ## convert slat slon to domain indices\n",
    "\n",
    "#         slon = temp_array[:, 4]\n",
    "#         slat = temp_array[:, 3]\n",
    "#         #mag = temp_array[:, 3]\n",
    "\n",
    "#         flag_pick = np.logical_and(slon<-20, slat>5)\n",
    "\n",
    "#         slon = slon[flag_pick]\n",
    "#         slat = slat[flag_pick]\n",
    "#         #mag = mag[flag_pick]\n",
    "\n",
    "#         L = len(slon)\n",
    "\n",
    "#         for i in range(L):\n",
    "\n",
    "#             # the time of a single record\n",
    "#             temp_datetime = temp_dt_list[i]\n",
    "#             temp_day = temp_datetime.day\n",
    "#             temp_hour = temp_datetime.hour\n",
    "\n",
    "#             temp_datetime_day = datetime(temp_datetime.year, temp_datetime.month, temp_day)\n",
    "\n",
    "#             if (temp_day_old == temp_day) is False:\n",
    "#                 count_v3 = 0\n",
    "#                 count_v4 = 0\n",
    "#                 temp_day_old = temp_day\n",
    "\n",
    "#             if temp_hour == lead:\n",
    "#                 diff_days = (temp_datetime_day - base_v4_s).days\n",
    "\n",
    "#                 if diff_days > 0:\n",
    "#                     #print('adding: {}'.format(temp_datetime))\n",
    "#                     record_v4[diff_days, 2*count_v4] = slon[i]\n",
    "#                     record_v4[diff_days, 2*count_v4+1] = slat[i]\n",
    "#                     #record_v4[diff_days, 3*count_v4+2] = mag[i]\n",
    "#                     count_v4 += 1\n",
    "\n",
    "#                 else:\n",
    "#                     diff_days = (temp_datetime_day - base_v3_s).days\n",
    "#                     if diff_days > 0:\n",
    "#                         #print('adding: {}'.format(temp_datetime))\n",
    "#                         record_v3[diff_days, 2*count_v3] = slon[i]\n",
    "#                         record_v3[diff_days, 2*count_v3+1] = slat[i]\n",
    "#                         #record_v3[diff_days, 3*count_v3+2] = mag[i]\n",
    "#                         count_v3 += 1\n",
    "\n",
    "#             else:\n",
    "#                 continue\n",
    "\n",
    "#     tuple_save = (record_v3, record_v4)\n",
    "#     label_save = ['record_v3', 'record_v4']\n",
    "#     du.save_hdf5(tuple_save, label_save, save_dir_scratch, 'SPC_all_lead{}_old.hdf'.format(lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c579196-aea0-443c-8abc-5e5499ede92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e065cd14-6398-4544-a6c9-e29ad90c4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridTree = cKDTree(list(zip(lon_72km.ravel(), lat_72km.ravel()))) #KDTree_wraper(xgrid, ygrid)\n",
    "grid_shape = lon_72km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe1206a-a4ce-4682-be7b-0093f5f18962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 93)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "coords_1 = (52.2296756, 21.0122287)\n",
    "coords_2 = (52.406374, 16.9251681)\n",
    "\n",
    "print geopy.distance.geodesic(coords_1, coords_2).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "894fea87-3693-462a-9013-cfeb836ce1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_earth(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    R = 6373.0\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = (np.sin(dlat/2))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(dlon/2))**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1741aba1-f3a5-4cd7-a79a-37dd6c1e762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_radi(lon, lat, stn_lon, stn_lat):\n",
    "    dist, indexes = gridTree.query(list(zip(np.array(stn_lon), np.array(stn_lat))))\n",
    "    indx_all, indy_all = np.unravel_index(indexes, grid_shape)\n",
    "    L = len(stn_lon)\n",
    "    \n",
    "    indx_out = np.empty([L, 10])\n",
    "    indy_out = np.empty([L, 10])\n",
    "    \n",
    "    indx_out[...] = np.nan\n",
    "    indy_out[...] = np.nan\n",
    "    \n",
    "    gap = 2\n",
    "    \n",
    "    for l in range(L):\n",
    "\n",
    "        indx = indx_all[l]\n",
    "        indy = indy_all[l]\n",
    "        \n",
    "        indx_l = []\n",
    "        indy_l = []\n",
    "        \n",
    "        indx_left = np.max([indx-gap, 0])\n",
    "        indy_bot = np.max([indy-gap, 0])\n",
    "    \n",
    "        lon_sub = lon[indx_left:indx+gap, indy_bot:indy+gap]\n",
    "        lat_sub = lat[indx_left:indx+gap, indy_bot:indy+gap]\n",
    "    \n",
    "        shape_sub = lon_sub.shape\n",
    "    \n",
    "        for xi in range(shape_sub[0]):\n",
    "            for yj in range(shape_sub[1]):\n",
    "                \n",
    "                lon_temp = lon_sub[xi, yj]\n",
    "                lat_temp = lat_sub[xi, yj]\n",
    "                \n",
    "                coords_1 = (lat_temp, lon_temp,)\n",
    "                coords_2 = (stn_lat[l], stn_lon[l])\n",
    "                \n",
    "                dist_km = geopy.distance.geodesic(coords_2, coords_1).km\n",
    "                \n",
    "                if dist_km < 40:\n",
    "                    indx_l.append(indx_left+xi)\n",
    "                    indy_l.append(indy_bot+yj)\n",
    "        \n",
    "        count = len(indx_l)\n",
    "        indx_out[l, :count] = np.array(indx_l)\n",
    "        indy_out[l, :count] = np.array(indy_l)\n",
    "        \n",
    "    return indx_out, indy_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1dec3408-8b52-4f55-ad78-90b70bf761dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_out, indy_out = search_radi(lon_72km, lat_72km, np.array(slon), np.array(slat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "32e42dd1-35d2-4439-8a70-7cadc1427d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1144, 10)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "21c86085-64a0-4c2a-85df-f0f4456782d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Processing lead time = 23 ==========\n",
      "Save to /glade/scratch/ksha/DRIVE/SPC_to_lead23_72km_all.hdf\n"
     ]
    }
   ],
   "source": [
    "preserve_inds = [4, 5, 6, 15, 16]\n",
    "column_names = ['date', 'time', 'tz', 'slat', 'slon']\n",
    "\n",
    "# ['date', 'time', 'tz', 'slat', 'slon']\n",
    "\n",
    "L_v3 = len(date_list_v3)\n",
    "L_v4 = len(date_list_v4)\n",
    "#[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
    "for lead in [23,]:\n",
    "    \n",
    "    torn_grid_v3 = np.empty((L_v3,)+lon_72km.shape+(3,))\n",
    "    torn_grid_v4 = np.empty((L_v4,)+lon_72km.shape+(3,))\n",
    "\n",
    "    print('========== Processing lead time = {} =========='.format(lead))\n",
    "\n",
    "    record_v3 = np.empty((L_v3, 3*30))*np.nan\n",
    "    record_v4 = np.empty((L_v3, 3*30))*np.nan\n",
    "\n",
    "    for y in range(2010, 2022):\n",
    "        \n",
    "        #temp_day_old = 9999\n",
    "        \n",
    "        # Year info\n",
    "        year_int = int(y)\n",
    "        year = str(year_int)\n",
    "\n",
    "        # Raw tornado files\n",
    "        file_torn = sorted(glob(report_dir+'{}_torn.csv'.format(year)))[0]\n",
    "        file_wind = sorted(glob(report_dir+'{}_wind.csv'.format(year)))[0]\n",
    "        file_hail = sorted(glob(report_dir+'{}_hail.csv'.format(year)))[0]\n",
    "        \n",
    "        # import csv to pandas and then np.array\n",
    "        df_torn = pd.read_csv(file_torn)\n",
    "        df_torn = df_torn.iloc[:, preserve_inds]\n",
    "        df_torn.columns = column_names\n",
    "        \n",
    "        df_wind = pd.read_csv(file_wind)\n",
    "        df_wind = df_wind.iloc[:, preserve_inds]\n",
    "        df_wind.columns = column_names\n",
    "        \n",
    "        df_hail = pd.read_csv(file_hail)\n",
    "        df_hail = df_hail.iloc[:, preserve_inds]\n",
    "        df_hail.columns = column_names\n",
    "        \n",
    "        data_frames = [df_torn, df_wind, df_hail]\n",
    "        #df_merged = reduce(lambda  left,right: pd.merge(left, right, how='outer'), data_frames)\n",
    "        \n",
    "        for c, df in enumerate(data_frames):\n",
    "        \n",
    "            temp_array = df.values\n",
    "\n",
    "            # datetime and timezone processing\n",
    "            L = len(temp_array)\n",
    "            temp_tz = temp_array[:, 2]\n",
    "            temp_dt_list = []\n",
    "            flag_badboy = False\n",
    "\n",
    "            for i in range(L):\n",
    "                try:\n",
    "                    # the string can be converted to datetime object\n",
    "                    temp_localtime = datetime.strptime(temp_array[i, 0]+'|'+temp_array[i, 1], '%Y-%m-%d|%H:%M:%S')\n",
    "                    flag_badboy = False\n",
    "                except:\n",
    "                    # the string cannot be converted; typically a \"?\"\n",
    "                    temp_localtime = np.nan\n",
    "                    flag_badboy = True\n",
    "\n",
    "                # adjust timezones to UTC/GMT \n",
    "                if flag_badboy is False:\n",
    "                    temp_tz = temp_array[i, 2]\n",
    "                    if temp_tz == 3:\n",
    "                        temp_localtime = temp_localtime + timedelta(hours=6) # <--- !!! \"3\" means CST\n",
    "                    elif temp_tz == 9:\n",
    "                        temp_localtime = temp_localtime # \"9\" means GMT\n",
    "                    else:\n",
    "                        temp_localtime = np.nan # otherwise doint know\n",
    "\n",
    "                temp_dt_list.append(temp_localtime)\n",
    "\n",
    "            # Insert in-situ reports into hourly, gridded data frames    \n",
    "            ## convert slat slon to domain indices\n",
    "\n",
    "            slon = temp_array[:, 4]\n",
    "            slat = temp_array[:, 3]\n",
    "\n",
    "            flag_pick = np.logical_and(slon<-20, slat>5)\n",
    "\n",
    "            slon = slon[flag_pick]\n",
    "            slat = slat[flag_pick]\n",
    "\n",
    "            L = len(slon)\n",
    "        \n",
    "            if L > 0:\n",
    "            \n",
    "                dist, indexes = gridTree.query(list(zip(np.array(slon), np.array(slat))))\n",
    "                indx, indy = np.unravel_index(indexes, grid_shape)\n",
    "                \n",
    "                indx_dist, indy_dist = search_radi(lon_72km, lat_72km, np.array(slon), np.array(slat))\n",
    "                \n",
    "                for i in range(L):\n",
    "\n",
    "                    # the time of a single record\n",
    "                    temp_datetime = temp_dt_list[i]\n",
    "                    temp_day = temp_datetime.day\n",
    "                    temp_hour = temp_datetime.hour\n",
    "\n",
    "                    temp_datetime_day = datetime(temp_datetime.year, temp_datetime.month, temp_day)\n",
    "                    \n",
    "                    if temp_hour == lead:\n",
    "                        diff_days = (temp_datetime_day - base_v4_s).days\n",
    "\n",
    "                        if diff_days > 0:\n",
    "                            #print('adding: {}'.format(temp_datetime))\n",
    "                            N = 10 - np.sum(np.isnan(indx_dist[i, :]))\n",
    "                            if N == 0:\n",
    "                                torn_grid_v4[diff_days, indx[i], indy[i], c] = 1.0\n",
    "                            else:\n",
    "                                for ni in range(N):\n",
    "                                    torn_grid_v4[diff_days, int(indx_dist[i, ni]), int(indy_dist[i, ni]), c] = 1.0\n",
    "                                    \n",
    "                        else:\n",
    "                            diff_days = (temp_datetime_day - base_v3_s).days\n",
    "                            if diff_days > 0:\n",
    "                                N = 10 - np.sum(np.isnan(indx_dist[i, :]))\n",
    "                                #print('adding: {}'.format(temp_datetime))\n",
    "                                if N == 0:\n",
    "                                    torn_grid_v3[diff_days, indx[i], indy[i], c] = 1.0\n",
    "                                else:\n",
    "                                    for ni in range(N):\n",
    "                                        torn_grid_v3[diff_days, int(indx_dist[i, ni]), int(indy_dist[i, ni]), c] = 1.0\n",
    "                                    \n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    tuple_save = (torn_grid_v3, torn_grid_v4)\n",
    "    label_save = ['record_v3', 'record_v4']\n",
    "    du.save_hdf5(tuple_save, label_save, save_dir_scratch, 'SPC_to_lead{}_72km_all.hdf'.format(lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6256f7e6-0af8-4439-a21a-697826fad647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbour_leads(lead):\n",
    "    out = [lead-2, lead-1, lead, lead+1]\n",
    "    flag_shift = [0, 0, 0, 0]\n",
    "    \n",
    "    for i in range(4):\n",
    "        if out[i] < 0:\n",
    "            out[i] = 24+out[i]\n",
    "            flag_shift[i] = -1\n",
    "        if out[i] > 23:\n",
    "            out[i] = out[i]-24\n",
    "            flag_shift[i] = +1\n",
    "            \n",
    "    return out, flag_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "82b1c873-73e7-46b1-aafd-d5578c11d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead = 4\n",
    "\n",
    "lead_window, flag_shift = neighbour_leads(lead)\n",
    "\n",
    "record_all = ()\n",
    "\n",
    "for i, lead_temp in enumerate(lead_window):\n",
    "    \n",
    "    flag_ = flag_shift[i]\n",
    "    \n",
    "    with h5py.File(save_dir_scratch+'SPC_to_lead{}_72km_all.hdf'.format(lead_temp), 'r') as h5io:\n",
    "        record_temp = h5io['record_v3'][...]\n",
    "        \n",
    "    if flag_shift[i] == 0:\n",
    "        record_all = record_all + (record_temp,)\n",
    "        \n",
    "    if flag_shift[i] == -1:\n",
    "        record_temp[1:, ...] = record_temp[:-1, ...]\n",
    "        record_temp[0, ...] = np.nan\n",
    "        record_all = record_all + (record_temp,)\n",
    "    \n",
    "    if flag_shift[i] == +1:\n",
    "        record_temp[:-1, ...] = record_temp[1:, ...]\n",
    "        record_temp[-1, ...] = np.nan\n",
    "        record_all = record_all + (record_temp,)\n",
    "\n",
    "\n",
    "shape_record = record_temp.shape      \n",
    "record_v3 = np.empty(shape_record)\n",
    "record_v3[...] = np.nan\n",
    "\n",
    "for i in range(4):\n",
    "    record_temp = record_all[i]\n",
    "    for day in range(shape_record[0]):\n",
    "        for ix in range(shape_record[1]):\n",
    "            for iy in range(shape_record[2]):\n",
    "                for event in range(shape_record[3]):\n",
    "                    if record_temp[day, ix, iy, event] > 0:\n",
    "                        record_v3[day, ix, iy, event] = 1.0\n",
    "                    elif record_v3[day, ix, iy, event] == 1.0:\n",
    "                        record_v3[day, ix, iy, event] = 1.0\n",
    "                    else:\n",
    "                        record_v3[day, ix, iy, event] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "855e7de9-4af5-46ab-a82a-3583af269e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4283.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(record_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "44c56b07-bd7e-4311-b621-8b6cbea5e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead = 4\n",
    "\n",
    "lead_window, flag_shift = neighbour_leads(lead)\n",
    "\n",
    "record_all = ()\n",
    "\n",
    "for i, lead_temp in enumerate(lead_window):\n",
    "    \n",
    "    flag_ = flag_shift[i]\n",
    "    \n",
    "    with h5py.File(save_dir_scratch+'SPC_to_lead{}_72km_all_old.hdf'.format(lead_temp), 'r') as h5io:\n",
    "        record_temp = h5io['record_v3'][...]\n",
    "        \n",
    "    if flag_shift[i] == 0:\n",
    "        record_all = record_all + (record_temp,)\n",
    "        \n",
    "    if flag_shift[i] == -1:\n",
    "        record_temp[1:, ...] = record_temp[:-1, ...]\n",
    "        record_temp[0, ...] = np.nan\n",
    "        record_all = record_all + (record_temp,)\n",
    "    \n",
    "    if flag_shift[i] == +1:\n",
    "        record_temp[:-1, ...] = record_temp[1:, ...]\n",
    "        record_temp[-1, ...] = np.nan\n",
    "        record_all = record_all + (record_temp,)\n",
    "\n",
    "\n",
    "shape_record = record_temp.shape      \n",
    "record_v3 = np.empty(shape_record)\n",
    "record_v3[...] = np.nan\n",
    "\n",
    "for i in range(4):\n",
    "    record_temp = record_all[i]\n",
    "    for day in range(shape_record[0]):\n",
    "        for ix in range(shape_record[1]):\n",
    "            for iy in range(shape_record[2]):\n",
    "                for event in range(shape_record[3]):\n",
    "                    if record_v3[day, ix, iy, event] == 1.0:\n",
    "                        record_v3[day, ix, iy, event] = 1.0\n",
    "                    elif record_temp[day, ix, iy, event] == 1.0:\n",
    "                        record_v3[day, ix, iy, event] = 1.0\n",
    "                    else:\n",
    "                        record_v3[day, ix, iy, event] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7ad46c56-e605-4e57-9c30-1e0b83c94426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641.0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(record_v3[..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "59fe9b45-3777-4ec2-91dd-ffdd1114e9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(record_v3[..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "37362cf6-3f76-47f5-8145-272863e71493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4254.0"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(record_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f15ee270-cdb0-46e8-b728-c99b32b10528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve_inds = [4, 5, 6, 15, 16]\n",
    "# column_names = ['date', 'time', 'tz', 'slat', 'slon']\n",
    "\n",
    "# # ['date', 'time', 'tz', 'slat', 'slon']\n",
    "\n",
    "# L_v3 = len(date_list_v3)\n",
    "# L_v4 = len(date_list_v4)\n",
    "\n",
    "# for lead in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]:\n",
    "    \n",
    "#     torn_grid_v3 = np.empty((L_v3,)+lon_72km.shape+(3,))\n",
    "#     torn_grid_v4 = np.empty((L_v4,)+lon_72km.shape+(3,))\n",
    "\n",
    "#     print('========== Processing lead time = {} =========='.format(lead))\n",
    "\n",
    "#     record_v3 = np.empty((L_v3, 3*30))*np.nan\n",
    "#     record_v4 = np.empty((L_v3, 3*30))*np.nan\n",
    "\n",
    "#     for y in range(2010, 2022):\n",
    "        \n",
    "#         #temp_day_old = 9999\n",
    "        \n",
    "#         # Year info\n",
    "#         year_int = int(y)\n",
    "#         year = str(year_int)\n",
    "\n",
    "#         # Raw tornado files\n",
    "#         file_torn = sorted(glob(report_dir+'{}_torn.csv'.format(year)))[0]\n",
    "#         file_wind = sorted(glob(report_dir+'{}_wind.csv'.format(year)))[0]\n",
    "#         file_hail = sorted(glob(report_dir+'{}_hail.csv'.format(year)))[0]\n",
    "        \n",
    "#         # import csv to pandas and then np.array\n",
    "#         df_torn = pd.read_csv(file_torn)\n",
    "#         df_torn = df_torn.iloc[:, preserve_inds]\n",
    "#         df_torn.columns = column_names\n",
    "        \n",
    "#         df_wind = pd.read_csv(file_wind)\n",
    "#         df_wind = df_wind.iloc[:, preserve_inds]\n",
    "#         df_wind.columns = column_names\n",
    "        \n",
    "#         df_hail = pd.read_csv(file_hail)\n",
    "#         df_hail = df_hail.iloc[:, preserve_inds]\n",
    "#         df_hail.columns = column_names\n",
    "        \n",
    "#         data_frames = [df_torn, df_wind, df_hail]\n",
    "#         #df_merged = reduce(lambda  left,right: pd.merge(left, right, how='outer'), data_frames)\n",
    "        \n",
    "#         for c, df in enumerate(data_frames):\n",
    "        \n",
    "#             temp_array = df.values\n",
    "\n",
    "#             # datetime and timezone processing\n",
    "#             L = len(temp_array)\n",
    "#             temp_tz = temp_array[:, 2]\n",
    "#             temp_dt_list = []\n",
    "#             flag_badboy = False\n",
    "\n",
    "#             for i in range(L):\n",
    "#                 try:\n",
    "#                     # the string can be converted to datetime object\n",
    "#                     temp_localtime = datetime.strptime(temp_array[i, 0]+'|'+temp_array[i, 1], '%Y-%m-%d|%H:%M:%S')\n",
    "#                     flag_badboy = False\n",
    "#                 except:\n",
    "#                     # the string cannot be converted; typically a \"?\"\n",
    "#                     temp_localtime = np.nan\n",
    "#                     flag_badboy = True\n",
    "\n",
    "#                 # adjust timezones to UTC/GMT \n",
    "#                 if flag_badboy is False:\n",
    "#                     temp_tz = temp_array[i, 2]\n",
    "#                     if temp_tz == 3:\n",
    "#                         temp_localtime = temp_localtime + timedelta(hours=6) # <--- !!! \"3\" means CST\n",
    "#                     elif temp_tz == 9:\n",
    "#                         temp_localtime = temp_localtime # \"9\" means GMT\n",
    "#                     else:\n",
    "#                         temp_localtime = np.nan # otherwise doint know\n",
    "\n",
    "#                 temp_dt_list.append(temp_localtime)\n",
    "\n",
    "#             # Insert in-situ reports into hourly, gridded data frames    \n",
    "#             ## convert slat slon to domain indices\n",
    "\n",
    "#             slon = temp_array[:, 4]\n",
    "#             slat = temp_array[:, 3]\n",
    "\n",
    "#             flag_pick = np.logical_and(slon<-20, slat>5)\n",
    "\n",
    "#             slon = slon[flag_pick]\n",
    "#             slat = slat[flag_pick]\n",
    "\n",
    "#             L = len(slon)\n",
    "        \n",
    "#             if L > 0:\n",
    "            \n",
    "#                 dist, indexes = gridTree.query(list(zip(np.array(slon), np.array(slat))))\n",
    "#                 indx, indy = np.unravel_index(indexes, grid_shape)\n",
    "            \n",
    "#                 for i in range(L):\n",
    "\n",
    "#                     # the time of a single record\n",
    "#                     temp_datetime = temp_dt_list[i]\n",
    "#                     temp_day = temp_datetime.day\n",
    "#                     temp_hour = temp_datetime.hour\n",
    "\n",
    "#                     temp_datetime_day = datetime(temp_datetime.year, temp_datetime.month, temp_day)\n",
    "\n",
    "#                     # if (temp_day_old == temp_day) is False:\n",
    "#                     #     count_v3 = 0\n",
    "#                     #     count_v4 = 0\n",
    "#                     #     temp_day_old = temp_day\n",
    "\n",
    "#                     if temp_hour == lead:\n",
    "#                         diff_days = (temp_datetime_day - base_v4_s).days\n",
    "\n",
    "#                         if diff_days > 0:\n",
    "#                             #print('adding: {}'.format(temp_datetime))\n",
    "#                             torn_grid_v4[diff_days, indx[i], indy[i], c] = 1.0\n",
    "#                             #count_v4 += 1\n",
    "\n",
    "#                         else:\n",
    "#                             diff_days = (temp_datetime_day - base_v3_s).days\n",
    "#                             if diff_days > 0:\n",
    "#                                 #print('adding: {}'.format(temp_datetime))\n",
    "#                                 torn_grid_v3[diff_days, indx[i], indy[i], c] = 1.0\n",
    "#                                 #count_v3 += 1\n",
    "\n",
    "#                     else:\n",
    "#                         continue\n",
    "\n",
    "#     tuple_save = (torn_grid_v3, torn_grid_v4)\n",
    "#     label_save = ['record_v3', 'record_v4']\n",
    "#     du.save_hdf5(tuple_save, label_save, save_dir_scratch, 'SPC_to_lead{}_72km_all_old.hdf'.format(lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16a8789d-15cc-4644-943d-b839292add74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545, 65, 93, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torn_grid_v4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e05365af-867a-4260-90c9-d848a490896f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8b4dc-c33b-4946-a423-40e8ddc870e6",
   "metadata": {},
   "source": [
    "**Use wind/hail as negatives of tornado**\n",
    "\n",
    "lat/lon locatinos will be defined based on 72-km grid cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e450ca6e-205b-4516-86b9-7c197b15be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4e22fbe-8286-430b-ba3c-bfbc1e423c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridTree = cKDTree(list(zip(lon_72km.ravel(), lat_72km.ravel()))) #KDTree_wraper(xgrid, ygrid)\n",
    "\n",
    "# for xi in range(shape_72km[0]):\n",
    "#     for yi in range(shape_72km[1]):\n",
    "        \n",
    "#         temp_lon = lon_72km[xi, yi]\n",
    "#         temp_lat = lat_72km[xi, yi]\n",
    "        \n",
    "#         dist, indexes = gridTree.query(list(zip(np.array(temp_lon)[None], np.array(temp_lat)[None])))\n",
    "#         indx_3km, indy_3km = np.unravel_index(indexes, grid_shape_hrrr)\n",
    "        \n",
    "#         indx_array[xi, yi] = indx_3km[0]\n",
    "#         indy_array[xi, yi] = indy_3km[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "834aa9b4-1eff-45c9-a551-daed272616fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nan_remove(lon, lat):\n",
    "#     lon_clean = []\n",
    "#     lat_clean = []\n",
    "#     L = len(lon)\n",
    "#     for i in range(L):\n",
    "#         lon_ = lon[i]        \n",
    "#         if np.isnan(lon_):\n",
    "#             break\n",
    "#         else:\n",
    "#             lon_clean.append(lon_)\n",
    "#             lat_clean.append(lat[i])\n",
    "            \n",
    "#     return np.array(lon_clean), np.array(lat_clean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72f0e1d2-94f1-4846-acf1-00efc75696c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def index_match(indx, indy, indx_pool, indy_pool):\n",
    "#     out = []\n",
    "#     for i in range(len(indx)):\n",
    "#         indx0 = indx[i]\n",
    "#         indy0 = indy[i]\n",
    "#         #matchx = np.where(indx0==indx_pool)\n",
    "#         matchx = np.argwhere((indx_pool-indx0) == 0)\n",
    "#         matchx = matchx[:, 0]\n",
    "#         if len(matchx) > 0:\n",
    "#             for j in range(len(matchx)):\n",
    "#                 indy1 = indy_pool[matchx[j]]\n",
    "#                 if indy1 == indy0:\n",
    "#                     out.append(i)\n",
    "#     return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3932fd8b-b51b-4a11-bdc7-af39fbf3bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_v3 = len(date_list_v3)\n",
    "\n",
    "# shape_72km = lon_72km.shape\n",
    "\n",
    "# for lead in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]:\n",
    "\n",
    "#     with h5py.File(save_dir_scratch+'SPC_torn_lead{}.hdf'.format(lead), 'r') as h5io:\n",
    "#         record_v3_torn = h5io['record_v3'][...]\n",
    "\n",
    "#     with h5py.File(save_dir_scratch+'SPC_wind_lead{}.hdf'.format(lead), 'r') as h5io:\n",
    "#         record_v3_wind = h5io['record_v3'][...]\n",
    "\n",
    "#     with h5py.File(save_dir_scratch+'SPC_hail_lead{}.hdf'.format(lead), 'r') as h5io:\n",
    "#         record_v3_hail = h5io['record_v3'][...]\n",
    "\n",
    "#     for day in range(L_v3):\n",
    "\n",
    "#         temp_torn = record_v3_torn[day, :]\n",
    "#         temp_wind = record_v3_wind[day, :]\n",
    "#         temp_hail = record_v3_hail[day, :]\n",
    "\n",
    "#         lon_torn_ = temp_torn[0::3]\n",
    "#         lat_torn_ = temp_torn[1::3]\n",
    "#         lon_torn, lat_torn = nan_remove(lon_torn_, lat_torn_)\n",
    "\n",
    "#         if len(lon_torn) == 0:\n",
    "#             continue;\n",
    "\n",
    "#         dist, indexes = gridTree.query(list(zip(np.array(lon_torn), np.array(lat_torn))))\n",
    "#         indx_torn, indy_torn = np.unravel_index(indexes, shape_72km)\n",
    "\n",
    "#         lon_wind_ = temp_wind[0::3]\n",
    "#         lat_wind_ = temp_wind[1::3]\n",
    "#         lon_wind, lat_wind = nan_remove(lon_wind_, lat_wind_)\n",
    "\n",
    "#         if len(lon_wind) > 0:\n",
    "#             dist, indexes = gridTree.query(list(zip(np.array(lon_wind), np.array(lat_wind))))\n",
    "#             indx_wind, indy_wind = np.unravel_index(indexes, shape_72km)\n",
    "\n",
    "#             ind_match_wind = index_match(indx_wind, indy_wind, indx_torn, indy_torn)\n",
    "#             if len(ind_match_wind) > 0:\n",
    "#                 # print(indx_wind)\n",
    "#                 # print(indy_wind)\n",
    "#                 # print(indx_torn)\n",
    "#                 # print(indy_torn)\n",
    "#                 # print(ind_match_wind)\n",
    "#                 for ind_ in ind_match_wind:\n",
    "#                     # print(temp_wind[3*ind_])\n",
    "#                     # print(temp_wind[3*ind_+1])\n",
    "#                     # print(lon_torn)\n",
    "#                     # print(lat_torn)\n",
    "\n",
    "#                     temp_wind[3*ind_] = np.nan\n",
    "#                     temp_wind[3*ind_+1] = np.nan\n",
    "#                     temp_wind[3*ind_+2] = np.nan\n",
    "#                 record_v3_wind[day, :] = temp_wind\n",
    "\n",
    "#         lon_hail_ = temp_hail[0::3]\n",
    "#         lat_hail_ = temp_hail[1::3]\n",
    "#         lon_hail, lat_hail = nan_remove(lon_hail_, lat_hail_)\n",
    "\n",
    "#         if len(lon_hail) > 0:\n",
    "#             dist, indexes = gridTree.query(list(zip(np.array(lon_hail), np.array(lat_hail))))\n",
    "#             indx_hail, indy_hail = np.unravel_index(indexes, shape_72km)\n",
    "\n",
    "#             ind_match_hail = index_match(indx_hail, indy_hail, indx_torn, indy_torn)\n",
    "#             if len(ind_match_hail) > 0:\n",
    "#                 for ind_ in ind_match_hail:\n",
    "#                     temp_hail[3*ind_] = np.nan\n",
    "#                     temp_hail[3*ind_+1] = np.nan\n",
    "#                     temp_hail[3*ind_+2] = np.nan\n",
    "#                 record_v3_hail[day, :] = temp_hail\n",
    "\n",
    "#     tuple_save = (record_v3_wind,); label_save = ['record_v3',]\n",
    "#     du.save_hdf5(tuple_save, label_save, save_dir_scratch, 'SPC_wind_non-torn_lead{}.hdf'.format(lead))\n",
    "\n",
    "#     tuple_save = (record_v3_hail,); label_save = ['record_v3',]\n",
    "#     du.save_hdf5(tuple_save, label_save, save_dir_scratch, 'SPC_hail_non-torn_lead{}.hdf'.format(lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24137c-db46-47d5-9ede-1b2c583d5aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d066e34-65fc-40cd-a2c9-def416ea0759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20321630-3a13-42ea-9af7-b52083a85d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6616c6-e24a-4092-9581-6a8e70394d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64254d34-b7d3-4347-bb35-35a880eec252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
