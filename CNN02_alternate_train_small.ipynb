{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739d9b65-76f1-4e6c-8c6f-f2ffe86ab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e056a192-f23f-444b-af9d-7141f39d3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeabc4a7-1bfd-4e45-9eb0-274d72a10c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e5963a-bc31-418e-9859-f1e0a9b8d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 13:56:11.772086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# from keras_unet_collection import models as k_models\n",
    "from keras_unet_collection import utils as k_utils\n",
    "# from keras_unet_collection import layer_utils as k_layers\n",
    "# from keras_unet_collection.activations import GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb39d9d-280c-4377-8a35-fe5d877361d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import graph_utils as gu\n",
    "#import convnext_keras as ck\n",
    "\n",
    "from sklearn.metrics import classification_report, auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21364371-260e-4081-9ed5-9ba9deccec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af3ce85-02e8-4178-bba1-dcc408f1a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "def create_model(input_shape=(64, 64, 15)):\n",
    "\n",
    "    depths=[3, 3, 27, 3]\n",
    "    projection_dims=[32, 32, 64, 64]\n",
    "    drop_path_rate=0.0\n",
    "    layer_scale_init_value=1e-6\n",
    "\n",
    "\n",
    "    model_name='Branch64X'\n",
    "    IN64 = layers.Input(shape=input_shape)\n",
    "    X = IN64\n",
    "    # ----- convnext block 0 ----- #\n",
    "\n",
    "    X = layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=\"{}_down0\".format(model_name))(X)\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_norm\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[0]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[0], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[0], name=\"{}_down0_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[0], name=\"{}_down0_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down0_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[0], name=\"{}_down0_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[0], name=\"{}_down0_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "\n",
    "    # ----- convnext block 1 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[1], kernel_size=2, strides=2, name=\"{}_down1\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[1]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[1], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[1], name=\"{}_down1_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[1], name=\"{}_down1_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down1_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[1], name=\"{}_down1_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[1], name=\"{}_down1_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 2 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[2], kernel_size=2, strides=2, name=\"{}_down2\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[2]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[2], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[2], name=\"{}_down2_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[2], name=\"{}_down2_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down2_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[2], name=\"{}_down2_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[2], name=\"{}_down2_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 3 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[3], kernel_size=2, padding='same', name=\"{}_down3\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[3]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[3], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[3], name=\"{}_down3_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[3], name=\"{}_down3_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down3_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[3], name=\"{}_down3_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[3], name=\"{}_down3_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    V1 = X\n",
    "\n",
    "    OUT = layers.GlobalMaxPooling2D(name=\"{}_head_pool64\".format(model_name))(V1)\n",
    "#     OUT = layers.LayerNormalization(epsilon=1e-6, name=\"{}_head_norm64\".format(model_name))(OUT)\n",
    "\n",
    "#     OUT = layers.Dense(64, name=\"{}_dense1\".format(model_name))(OUT)\n",
    "#     OUT = layers.LayerNormalization(epsilon=1e-6, name=\"{}_dense1_norm\".format(model_name))(OUT)\n",
    "#     OUT = layers.Activation(\"gelu\", name=\"{}_dense1_gelu{}\".format(model_name, j))(OUT)\n",
    "\n",
    "#     OUT = layers.Dense(1, name=\"{}_head_out\".format(model_name))(OUT)\n",
    "\n",
    "    model = Model(inputs=IN64, outputs=OUT, name=model_name)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52dc9279-8963-4ee9-809d-8dd913fa4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head():\n",
    "\n",
    "    \n",
    "    IN_vec = keras.Input((64,))    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64367cd7-bd33-40a5-bbc5-4d888967a70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffe8d39-8d0b-49ff-8ad0-84b95e8ba5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred):\n",
    "\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    # AUC = auc(fpr, tpr)\n",
    "    # AUC_metric = 1 - AUC\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    #ll = log_loss(VALID_target.ravel(), Y_pred.ravel())\n",
    "    \n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd31da6f-0913-4a6d-93a3-ec24453bdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_pick_from_batch = [0, 1, 3, 4, 8, 9, 10, 13, 14, 15, 16, 17, 18, 21, 22]\n",
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "L_vars = len(ind_pick_from_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c974f051-af9a-49ef-8015-683fcb942226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3, lead2, pos: 5510, neg: 887822\n",
      "v3, lead3, pos: 4297, neg: 889035\n",
      "v3, lead4, pos: 3242, neg: 890090\n",
      "v3, lead5, pos: 2565, neg: 891795\n",
      "v3, lead6, pos: 2127, neg: 891205\n",
      "v3, lead20, pos: 5407, neg: 887925\n",
      "v3, lead21, pos: 6711, neg: 887649\n",
      "v3, lead22, pos: 7613, neg: 886747\n",
      "v3, lead23, pos: 8009, neg: 886351\n",
      "v4x, lead2, pos: 1995, neg: 348553\n",
      "v4x, lead3, pos: 1618, neg: 349958\n",
      "v4x, lead4, pos: 1218, neg: 350358\n",
      "v4x, lead5, pos: 962, neg: 350614\n",
      "v4x, lead6, pos: 817, neg: 349731\n",
      "v4x, lead20, pos: 1777, neg: 344659\n",
      "v4x, lead21, pos: 2209, neg: 344227\n",
      "v4x, lead22, pos: 2511, neg: 343925\n",
      "v4x, lead23, pos: 2690, neg: 343746\n",
      "v4, lead2, pos: 2174, neg: 402858\n",
      "v4, lead3, pos: 1724, neg: 403308\n",
      "v4, lead4, pos: 1349, neg: 403683\n",
      "v4, lead5, pos: 1034, neg: 403998\n",
      "v4, lead6, pos: 795, neg: 403209\n",
      "v4, lead20, pos: 1941, neg: 403091\n",
      "v4, lead21, pos: 2446, neg: 402586\n",
      "v4, lead22, pos: 2791, neg: 402241\n",
      "v4, lead23, pos: 2903, neg: 402129\n"
     ]
    }
   ],
   "source": [
    "vers = ['v3', 'v4x', 'v4']\n",
    "leads = [2, 3, 4, 5, 6, 20, 21, 22, 23]\n",
    "filenames_pos = {}\n",
    "filenames_neg = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        if ver == 'v3' and lead < 23:\n",
    "            path_ = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "        elif ver == 'v3' and lead == 23:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "        elif ver == 'v4':\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "        else:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "            \n",
    "        filenames_pos['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*pos*lead{}.npy\".format(path_, lead)))\n",
    "        filenames_neg['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*neg_neg_neg*lead{}.npy\".format(path_, lead)))\n",
    "        \n",
    "        print('{}, lead{}, pos: {}, neg: {}'.format(ver, lead, \n",
    "                                                    len(filenames_pos['{}_lead{}'.format(ver, lead)]), \n",
    "                                                    len(filenames_neg['{}_lead{}'.format(ver, lead)])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f7ecbaf-868c-4d76-9f23-93630d4764b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_extract(filenames):\n",
    "    \n",
    "    date_base = datetime(2020, 7, 14)\n",
    "    date_base2 = datetime(2021, 1, 1)\n",
    "    \n",
    "    filename_train = []\n",
    "    filename_valid = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+365+30)]\n",
    "    \n",
    "    base_ref = datetime(2019, 10, 1)\n",
    "    date_list_v4x = [base_ref + timedelta(days=day) for day in range(429)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4x' in name:\n",
    "            date_list = date_list_v4x\n",
    "        elif 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        \n",
    "        if (day - date_base).days < 0:\n",
    "            filename_train.append(name)\n",
    "            \n",
    "        else:\n",
    "            if (day - date_base2).days < 0:\n",
    "                filename_valid.append(name)\n",
    "\n",
    "        \n",
    "    return filename_train, filename_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03bc3af-350a-4f70-a2f0-a8c59b539f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos train: 4856 pos valid: 654 neg train: 742500 neg valid 145322\n",
      "pos train: 3787 pos valid: 510 neg train: 743569 neg valid 145466\n",
      "pos train: 2874 pos valid: 368 neg train: 744482 neg valid 145608\n",
      "pos train: 2289 pos valid: 276 neg train: 746095 neg valid 145700\n",
      "pos train: 1885 pos valid: 242 neg train: 745471 neg valid 145734\n",
      "pos train: 4632 pos valid: 775 neg train: 742724 neg valid 145201\n",
      "pos train: 5774 pos valid: 937 neg train: 742610 neg valid 145039\n",
      "pos train: 6589 pos valid: 1024 neg train: 741795 neg valid 144952\n",
      "pos train: 6952 pos valid: 1057 neg train: 741432 neg valid 144919\n",
      "pos train: 1452 pos valid: 543 neg train: 225736 neg valid 122817\n",
      "pos train: 1187 pos valid: 431 neg train: 226001 neg valid 123957\n",
      "pos train: 898 pos valid: 320 neg train: 226290 neg valid 124068\n",
      "pos train: 721 pos valid: 241 neg train: 226467 neg valid 124147\n",
      "pos train: 598 pos valid: 219 neg train: 225562 neg valid 124169\n",
      "pos train: 1136 pos valid: 641 neg train: 220912 neg valid 123747\n",
      "pos train: 1426 pos valid: 783 neg train: 220622 neg valid 123605\n",
      "pos train: 1656 pos valid: 855 neg train: 220392 neg valid 123533\n",
      "pos train: 1800 pos valid: 890 neg train: 220248 neg valid 123498\n",
      "pos train: 0 pos valid: 13 neg train: 0 neg valid 29799\n",
      "pos train: 0 pos valid: 10 neg train: 0 neg valid 29802\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 7 neg train: 0 neg valid 29805\n",
      "pos train: 0 pos valid: 7 neg train: 0 neg valid 29805\n",
      "pos train: 0 pos valid: 11 neg train: 0 neg valid 29801\n",
      "pos train: 0 pos valid: 10 neg train: 0 neg valid 29802\n"
     ]
    }
   ],
   "source": [
    "filenames_pos_train = {}\n",
    "filenames_neg_train = {}\n",
    "\n",
    "filenames_pos_valid = {}\n",
    "filenames_neg_valid = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        temp_namelist_pos = filenames_pos['{}_lead{}'.format(ver, lead)]\n",
    "        temp_namelist_neg = filenames_neg['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "        pos_train, pos_valid = name_extract(temp_namelist_pos)\n",
    "        neg_train, neg_valid = name_extract(temp_namelist_neg)\n",
    "        \n",
    "        print('pos train: {} pos valid: {} neg train: {} neg valid {}'.format(len(pos_train), \n",
    "                                                                              len(pos_valid), \n",
    "                                                                              len(neg_train), \n",
    "                                                                              len(neg_valid)))\n",
    "        \n",
    "        filenames_pos_train['{}_lead{}'.format(ver, lead)] = pos_train\n",
    "        filenames_neg_train['{}_lead{}'.format(ver, lead)] = neg_train\n",
    "        \n",
    "        filenames_pos_valid['{}_lead{}'.format(ver, lead)] = pos_valid\n",
    "        filenames_neg_valid['{}_lead{}'.format(ver, lead)] = neg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeed7b9-39ed-42c1-a1a8-c947b6bf4c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23e0a8-f175-4bc3-80eb-252818710271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b635504-c7cb-4678-aced-e59f5105f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "pos_valid_all = []\n",
    "neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03fdb6-5dd9-40b2-be3d-096a7993b628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf6e364-ee67-4f76-8a35-1f6728381bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename_valid = neg_valid_all[::200] + pos_valid_all[::20]\n",
    "# print(len(filename_valid))\n",
    "\n",
    "# L_valid = len(filename_valid)\n",
    "\n",
    "# VALID_input_64 = np.empty((L_valid, 64, 64, L_vars))\n",
    "# VALID_target = np.ones(L_valid)\n",
    "\n",
    "# for i, name in enumerate(filename_valid):\n",
    "#     data = np.load(name)\n",
    "#     for k, c in enumerate(ind_pick_from_batch):\n",
    "        \n",
    "#         VALID_input_64[i, ..., k] = data[..., c]\n",
    "\n",
    "#         if 'pos' in name:\n",
    "#             VALID_target[i] = 1.0\n",
    "#         else:\n",
    "#             VALID_target[i] = 0.0\n",
    "            \n",
    "# save_dir = '/glade/work/ksha/NCAR/'\n",
    "# tuple_save = (VALID_input_64, VALID_target)\n",
    "# label_save = ['VALID_input_64', 'VALID_target']\n",
    "# du.save_hdf5(tuple_save, label_save, save_dir, 'CNN_Validation.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1b28db8-cf6c-4001-863b-20a8f3d3fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '/glade/work/ksha/NCAR/'\n",
    "# tuple_save = (VALID_input_64, VALID_target)\n",
    "# label_save = ['VALID_input_64', 'VALID_target']\n",
    "# du.save_hdf5(tuple_save, label_save, save_dir, 'CNN_Validation.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f44fde2-2fee-4675-a7cb-137c39af4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/glade/work/ksha/NCAR/'\n",
    "\n",
    "with h5py.File(save_dir+'CNN_Validation.hdf', 'r') as h5io:\n",
    "    VALID_input_64 = h5io['VALID_input_64'][...]\n",
    "    VALID_target = h5io['VALID_target'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf993e3-6695-4d65-9d71-1c7e3af410af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a8a3f-1c3b-409b-b218-dabcd3f7e4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3a6cb4-f848-43fc-8947-aff34d1dce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_train = 'base'\n",
    "\n",
    "if flag_train == 'head':\n",
    "    flag_weights = 'base'\n",
    "else:\n",
    "    flag_weights = 'head'\n",
    "    \n",
    "weights_round = 0\n",
    "save_round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b05c8-8daa-440d-a100-25e860b05344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d413b66-e165-4461-8e9f-f79ac93a6ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:20:56.542067: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-14 14:20:56.545450: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-14 14:20:56.589806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:8a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-14 14:20:56.589840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-14 14:20:56.753674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-14 14:20:56.753716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-14 14:20:56.798270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-14 14:20:56.823993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-14 14:20:56.877500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-14 14:20:56.912713: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-14 14:20:57.030014: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-14 14:20:57.030715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-14 14:20:57.031212: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 14:20:57.031436: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-14 14:20:57.031832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:8a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-14 14:20:57.031853: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-14 14:20:57.031875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-14 14:20:57.031888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-14 14:20:57.031900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-14 14:20:57.031912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-14 14:20:57.031924: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-14 14:20:57.031936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-14 14:20:57.031948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-14 14:20:57.032551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-14 14:20:57.032582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-14 14:20:57.590853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-14 14:20:57.590911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-02-14 14:20:57.590925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-02-14 14:20:57.592098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "model_head = create_model_head()\n",
    "\n",
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/HY_Lead4/')\n",
    "# model_head.set_weights(W_old)\n",
    "\n",
    "model_base = create_model(input_shape=(64, 64, 15))\n",
    "\n",
    "# W_new = model_base.get_weights()\n",
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_{}{}/'.format(flag_weights, weights_round))\n",
    "\n",
    "# for i in range(len(W_new)):\n",
    "#     if W_new[i].shape == W_old[i].shape:\n",
    "#         W_new[i] = W_old[i]\n",
    "#     else:\n",
    "#         ewraewthws\n",
    "        \n",
    "# model_base.set_weights(W_new)\n",
    "\n",
    "# if flag_train == 'base':\n",
    "#     for layer in model_head.layers: \n",
    "#         layer.trainable = False\n",
    "# elif flag_train == 'head':\n",
    "#     for layer in model_base.layers: \n",
    "#         layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c8244af-fa39-4629-9e40-51ed0137dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = layers.Input(shape=(64, 64, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = Model(inputs=IN, outputs=OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f254b070-6569-4f8d-9fe8-cce37c2b2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_round = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b4f26db-b5b0-459e-8858-8d4aa8a59e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "if weights_round > 0:\n",
    "    W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_small_base{}/'.format(weights_round))\n",
    "    model_final.set_weights(W_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c520d28-36a1-4a1a-8230-c7015dc80420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if flag_train == 'base':\n",
    "#     for layer in model_head.layers: \n",
    "#         layer.trainable = False\n",
    "# elif flag_train == 'head':\n",
    "#     for layer in model_base.layers: \n",
    "#         layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b3e8b29-ad84-4194-92ea-0a7904cd0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_pred = model_final.predict([VALID_input_64])\n",
    "# record_temp = verif_metric(VALID_target, Y_pred)\n",
    "# 0.025381361192679055 0.025381361192679055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d071dd20-3a6a-4fe9-a5c9-06841dcc1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n",
      "2023-02-14 14:22:42.652826: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-02-14 14:22:42.660397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n",
      "2023-02-14 14:22:43.075638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-14 14:22:48.284905: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024687795070154846\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model_final.predict([VALID_input_64])\n",
    "record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f98bc-f492-4176-8057-a8208492ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.024687795070154846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "392c3de0-1452-46d4-a98b-782fd45d2a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024710265210776728"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.024710265210776728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9acd6e-2265-4ec1-90dc-989e63bffd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "650a5752-50d0-4bad-bd2f-ab9b3a0f4d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 0.04038452523918941\n",
      "0.039544503270762914\n",
      "Validation loss improved from 0.04038452523918941 to 0.039544503270762914\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_small_base1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 09:35:26.583886: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seeds = 1567 #3725 #\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 64 #int(len(TRAIN_Y_pick) / batch_size)\n",
    "\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "key = 'RE2_small_{}{}'.format(flag_train, save_round)\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp #0.01840167896363949 #record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        if flag_train == 'base':\n",
    "            N_pos = 20\n",
    "        else:\n",
    "            N_pos = 20\n",
    "            \n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "        file_pick_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "\n",
    "        file_pick_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "\n",
    "        if len(file_pick) != batch_size:\n",
    "            sregwet\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "\n",
    "            for l, c in enumerate(ind_pick_from_batch):\n",
    "                temp = data[..., c] \n",
    "                X_batch_64[k, ..., l] = temp\n",
    "\n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "            elif 'neg_neg_neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "\n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_64 = X_batch_64[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_64, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_64])\n",
    "    # Y_pred[Y_pred<0] = 0\n",
    "    # Y_pred[Y_pred>1] = 1\n",
    "\n",
    "    record_temp = verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     model.save(model_path_backup)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        #print('tol: {}'.format(tol))\n",
    "        # save\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac91e18-a5fd-4975-924f-e521b9c91066",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d227-11df-4ab2-b3e6-399d1c620446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872fe44-4992-44df-8b73-efd3f8ae2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590427b-a292-43d2-bd5e-0dc3862c7971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62389b-486a-40a6-96f3-a8034ee8cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
