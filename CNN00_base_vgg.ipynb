{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e42c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import re\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8339246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== #\n",
    "weights_round = 0\n",
    "save_round = 1\n",
    "seeds = 777\n",
    "model_prefix_load = 'RE2_vgg{}'.format(weights_round) #False\n",
    "model_prefix_save = 'RE2_vgg{}'.format(save_round)\n",
    "N_vars = L_vars = 15\n",
    "# ==================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91701da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------- #\n",
    "# Collect pos and neg batch filenames\n",
    "\n",
    "vers = ['v3', 'v4x', 'v4'] # HRRR v4, v4x, v4\n",
    "leads = [2, 3, 4, 5, 6, 20, 21, 22, 23]\n",
    "\n",
    "filenames_pos = {}\n",
    "filenames_neg = {}\n",
    "\n",
    "# Identify and separate pos / neg batch files\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        if ver == 'v3':\n",
    "            path_ = path_batch_v3\n",
    "        elif ver == 'v4':\n",
    "            path_ = path_batch_v4\n",
    "        else:\n",
    "            path_ = path_batch_v4x\n",
    "            \n",
    "        filenames_pos['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*pos*lead{}.npy\".format(path_, lead)))\n",
    "        filenames_neg['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*neg_neg_neg*lead{}.npy\".format(path_, lead)))\n",
    "        \n",
    "        print('{}, lead{}, pos: {}, neg: {}'.format(ver, lead, len(filenames_pos['{}_lead{}'.format(ver, lead)]), \n",
    "                                             len(filenames_neg['{}_lead{}'.format(ver, lead)])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf75ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------- #\n",
    "# Separate train and valid from pos / neg batches\n",
    "filenames_pos_train = {}\n",
    "filenames_neg_train = {}\n",
    "\n",
    "filenames_pos_valid = {}\n",
    "filenames_neg_valid = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        temp_namelist_pos = filenames_pos['{}_lead{}'.format(ver, lead)]\n",
    "        temp_namelist_neg = filenames_neg['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "        pos_train, pos_valid = mu.name_extract(temp_namelist_pos)\n",
    "        neg_train, neg_valid = mu.name_extract(temp_namelist_neg)\n",
    "        \n",
    "        print('pos train: {} pos valid: {} neg train: {} neg valid {}'.format(len(pos_train), len(pos_valid), len(neg_train),len(neg_valid)))\n",
    "        \n",
    "        filenames_pos_train['{}_lead{}'.format(ver, lead)] = pos_train\n",
    "        filenames_neg_train['{}_lead{}'.format(ver, lead)] = neg_train\n",
    "        \n",
    "        filenames_pos_valid['{}_lead{}'.format(ver, lead)] = pos_valid\n",
    "        filenames_neg_valid['{}_lead{}'.format(ver, lead)] = neg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# Merge train/valid and pos/neg batch files from multiple lead times\n",
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "pos_valid_all = []\n",
    "neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "# Load valid files for model training\n",
    "\n",
    "filename_valid = neg_valid_all[::130] + pos_valid_all[::13]\n",
    "L_valid = len(filename_valid)\n",
    "print('number of validation batches: {}'.format(L_valid))\n",
    "\n",
    "VALID_input_64 = np.empty((L_valid, 64, 64, L_vars))\n",
    "VALID_target = np.ones(L_valid)\n",
    "\n",
    "for i, name in enumerate(filename_valid):\n",
    "    data = np.load(name)\n",
    "    for k, c in enumerate(ind_pick_from_batch):\n",
    "        \n",
    "        VALID_input_64[i, ..., k] = data[..., c]\n",
    "\n",
    "        if 'pos' in name:\n",
    "            VALID_target[i] = 1.0\n",
    "        else:\n",
    "            VALID_target[i] = 0.0\n",
    "            \n",
    "# Save and load validation set to speed-up retraining\n",
    "tuple_save = (VALID_input_64, VALID_target)\n",
    "label_save = ['VALID_input_64', 'VALID_target']\n",
    "du.save_hdf5(tuple_save, label_save, save_dir, 'CNN_Validation_basic.hdf')\n",
    "\n",
    "# with h5py.File(save_dir+'CNN_Validation_basic.hdf', 'r') as h5io:\n",
    "#     VALID_input_64 = h5io['VALID_input_64'][...]\n",
    "#     VALID_target = h5io['VALID_target'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_vgg():\n",
    "\n",
    "    input_tensor = keras.Input((None, 64, 64, 15))\n",
    "\n",
    "    X = input_tensor\n",
    "\n",
    "    # (64, 64)\n",
    "\n",
    "    X = keras.layers.Conv2D(64, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(64, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.MaxPooling2D((2,2), strides=(2,2))(X)\n",
    "\n",
    "    # (32, 32)\n",
    "\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(128, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(128, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.MaxPooling2D((2,2), strides=(2,2))(X)\n",
    "\n",
    "    # (16, 16)\n",
    "\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(256, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(256, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(256, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.MaxPooling2D((2,2), strides=(2,2))(X)\n",
    "\n",
    "    # (8, 8)\n",
    "\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(512, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(512, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(512, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.MaxPooling2D((2,2), strides=(2,2))(X)\n",
    "\n",
    "    # (4, 4)\n",
    "\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(512, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(512, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.ZeroPadding2D((1,1))(X)\n",
    "    X = keras.layers.Conv2D(512, (3, 3), padding='same', activation='gelu')(X)\n",
    "    X = keras.layers.MaxPooling2D((2,2), strides=(2,2))(X)\n",
    "    \n",
    "    X = keras.layers.GlobalMaxPooling2D()(X)\n",
    "    \n",
    "    model = keras.Model(inputs=input_tensor, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc93a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head(input_shape=(128,), N_node=64):\n",
    "    \n",
    "    IN_vec = keras.Input(input_shape)    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(N_node)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178aa636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "# model and weights\n",
    "model_head = create_model_head(input_shape=(512,), N_node=64)\n",
    "model_base = create_model(input_shape=(64, 64, 15), depths=[3, 3, 27, 3], projection_dims=[32, 64, 96, 128])\n",
    "\n",
    "IN = layers.Input(shape=(64, 64, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = Model(inputs=IN, outputs=OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= #\n",
    "# Weights\n",
    "\n",
    "if weights_round == 0:\n",
    "    W_old = model_final.get_weights() #mu.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_base5/')\n",
    "else:\n",
    "    if model_prefix_load:\n",
    "        W_old = mu.dummy_loader('/glade/work/ksha/NCAR/Keras_models/{}/'.format(model_prefix_load))\n",
    "    \n",
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "model_final.set_weights(W_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f192336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "# model training loop\n",
    "Y_pred = model_final.predict([VALID_input_64])\n",
    "record_temp = mu.verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f29dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "epochs = 500\n",
    "L_train = 64\n",
    "min_del = 0.0\n",
    "max_tol = 100 # early stopping with patience\n",
    "batch_size = 200\n",
    "\n",
    "# Allocate batch files\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "# Model check-point info\n",
    "model_name = model_prefix_save\n",
    "model_path = temp_dir + model_name\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "tol = 0 # initial tol\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "mu.set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        N_pos = 20\n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "        \n",
    "        # neg batches from this training rotation \n",
    "        file_pick_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "        # pos batches from this training rotation \n",
    "        file_pick_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "            \n",
    "        # get all the batch filenames for checking labels\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "\n",
    "#         if len(file_pick) != batch_size:\n",
    "#             sregwet # number of available files = batch size\n",
    "\n",
    "        # Assign labels based on batch filenames\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "            for l, c in enumerate(N_vars):\n",
    "                temp = data[..., c] \n",
    "                X_batch_64[k, ..., l] = temp\n",
    "\n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "            elif 'neg_neg_neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "        # ------------------------------------------------- #\n",
    "        # batch input and label from this training rotation \n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_64 = X_batch_64[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_64, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_64])\n",
    "    record_temp = mu.verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "def focal_loss(alpha, gamma):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * y_true * tf.math.pow(1 - y_pred, gamma)\n",
    "        loss = ce * weight\n",
    "        return tf.math.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b2c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
