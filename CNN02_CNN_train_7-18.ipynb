{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739d9b65-76f1-4e6c-8c6f-f2ffe86ab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e056a192-f23f-444b-af9d-7141f39d3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeabc4a7-1bfd-4e45-9eb0-274d72a10c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e5963a-bc31-418e-9859-f1e0a9b8d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 18:49:32.990223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from keras_unet_collection import utils as k_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb39d9d-280c-4377-8a35-fe5d877361d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import graph_utils as gu\n",
    "#import convnext_keras as ck\n",
    "\n",
    "from sklearn.metrics import classification_report, auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21364371-260e-4081-9ed5-9ba9deccec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af3ce85-02e8-4178-bba1-dcc408f1a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "def create_model(input_shape=(64, 64, 15)):\n",
    "\n",
    "    depths=[3, 3, 27, 3]\n",
    "    projection_dims=[32, 64, 96, 128]\n",
    "    drop_path_rate=0.0\n",
    "    layer_scale_init_value=1e-6\n",
    "\n",
    "\n",
    "    model_name='Branch64X'\n",
    "    IN64 = layers.Input(shape=input_shape)\n",
    "    X = IN64\n",
    "    # ----- convnext block 0 ----- #\n",
    "\n",
    "    X = layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=\"{}_down0\".format(model_name))(X)\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_norm\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[0]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[0], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[0], name=\"{}_down0_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[0], name=\"{}_down0_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down0_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[0], name=\"{}_down0_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[0], name=\"{}_down0_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "\n",
    "    # ----- convnext block 1 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[1], kernel_size=2, strides=2, name=\"{}_down1\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[1]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[1], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[1], name=\"{}_down1_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[1], name=\"{}_down1_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down1_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[1], name=\"{}_down1_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[1], name=\"{}_down1_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 2 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[2], kernel_size=2, strides=2, name=\"{}_down2\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[2]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[2], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[2], name=\"{}_down2_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[2], name=\"{}_down2_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down2_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[2], name=\"{}_down2_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[2], name=\"{}_down2_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 3 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[3], kernel_size=2, padding='same', name=\"{}_down3\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[3]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[3], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[3], name=\"{}_down3_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[3], name=\"{}_down3_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down3_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[3], name=\"{}_down3_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[3], name=\"{}_down3_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    V1 = X\n",
    "\n",
    "    OUT = layers.GlobalMaxPooling2D(name=\"{}_head_pool64\".format(model_name))(V1)\n",
    "    model = Model(inputs=IN64, outputs=OUT, name=model_name)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52dc9279-8963-4ee9-809d-8dd913fa4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head():\n",
    "\n",
    "    \n",
    "    IN_vec = keras.Input((128,))    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64367cd7-bd33-40a5-bbc5-4d888967a70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffe8d39-8d0b-49ff-8ad0-84b95e8ba5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred):\n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd31da6f-0913-4a6d-93a3-ec24453bdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "L_vars = len(ind_pick_from_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c974f051-af9a-49ef-8015-683fcb942226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3, lead7, pos: 1779, neg: 891553\n",
      "v3, lead8, pos: 1492, neg: 892868\n",
      "v3, lead9, pos: 1272, neg: 893088\n",
      "v3, lead10, pos: 1099, neg: 893261\n",
      "v3, lead11, pos: 984, neg: 893376\n",
      "v3, lead12, pos: 898, neg: 893462\n",
      "v3, lead13, pos: 871, neg: 892461\n",
      "v3, lead14, pos: 889, neg: 892443\n",
      "v3, lead15, pos: 1034, neg: 892298\n",
      "v3, lead16, pos: 1326, neg: 893034\n",
      "v3, lead17, pos: 1897, neg: 892463\n",
      "v3, lead18, pos: 2777, neg: 891583\n",
      "v4, lead7, pos: 644, neg: 404388\n",
      "v4, lead8, pos: 497, neg: 404535\n",
      "v4, lead9, pos: 439, neg: 403565\n",
      "v4, lead10, pos: 384, neg: 404648\n",
      "v4, lead11, pos: 349, neg: 404683\n",
      "v4, lead12, pos: 332, neg: 403672\n",
      "v4, lead13, pos: 308, neg: 404724\n",
      "v4, lead14, pos: 299, neg: 404733\n",
      "v4, lead15, pos: 353, neg: 403651\n",
      "v4, lead16, pos: 450, neg: 404582\n",
      "v4, lead17, pos: 659, neg: 403345\n",
      "v4, lead18, pos: 978, neg: 403026\n"
     ]
    }
   ],
   "source": [
    "vers = ['v3', 'v4']\n",
    "leads = [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "filenames_pos = {}\n",
    "filenames_neg = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        if ver == 'v3':\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "        elif ver == 'v4':\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "        else:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "            \n",
    "        filenames_pos['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*pos*lead{}.npy\".format(path_, lead)))\n",
    "        filenames_neg['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*neg_neg_neg*lead{}.npy\".format(path_, lead)))\n",
    "        \n",
    "        print('{}, lead{}, pos: {}, neg: {}'.format(ver, lead, \n",
    "                                                    len(filenames_pos['{}_lead{}'.format(ver, lead)]), \n",
    "                                                    len(filenames_neg['{}_lead{}'.format(ver, lead)])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7ecbaf-868c-4d76-9f23-93630d4764b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_extract(filenames):\n",
    "    \n",
    "    date_base = datetime(2020, 7, 14)\n",
    "    date_base2 = datetime(2021, 1, 1)\n",
    "    \n",
    "    filename_train = []\n",
    "    filename_valid = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+365+30)]\n",
    "    \n",
    "    base_ref = datetime(2019, 10, 1)\n",
    "    date_list_v4x = [base_ref + timedelta(days=day) for day in range(429)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4x' in name:\n",
    "            date_list = date_list_v4x\n",
    "        elif 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        \n",
    "        if (day - date_base).days < 0:\n",
    "            filename_train.append(name)\n",
    "            \n",
    "        else:\n",
    "            if (day - date_base2).days < 0:\n",
    "                filename_valid.append(name)\n",
    "\n",
    "        \n",
    "    return filename_train, filename_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03bc3af-350a-4f70-a2f0-a8c59b539f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos train: 1574 pos valid: 205 neg train: 745782 neg valid 145771\n",
      "pos train: 1314 pos valid: 178 neg train: 747070 neg valid 145798\n",
      "pos train: 1110 pos valid: 162 neg train: 747274 neg valid 145814\n",
      "pos train: 977 pos valid: 122 neg train: 747407 neg valid 145854\n",
      "pos train: 883 pos valid: 101 neg train: 747501 neg valid 145875\n",
      "pos train: 805 pos valid: 93 neg train: 747579 neg valid 145883\n",
      "pos train: 777 pos valid: 94 neg train: 746579 neg valid 145882\n",
      "pos train: 785 pos valid: 104 neg train: 746571 neg valid 145872\n",
      "pos train: 906 pos valid: 128 neg train: 746450 neg valid 145848\n",
      "pos train: 1154 pos valid: 172 neg train: 747230 neg valid 145804\n",
      "pos train: 1644 pos valid: 253 neg train: 746740 neg valid 145723\n",
      "pos train: 2397 pos valid: 380 neg train: 745987 neg valid 145596\n",
      "pos train: 0 pos valid: 7 neg train: 0 neg valid 29805\n",
      "pos train: 0 pos valid: 5 neg train: 0 neg valid 29807\n",
      "pos train: 0 pos valid: 2 neg train: 0 neg valid 29810\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n",
      "pos train: 0 pos valid: 0 neg train: 0 neg valid 29812\n"
     ]
    }
   ],
   "source": [
    "filenames_pos_train = {}\n",
    "filenames_neg_train = {}\n",
    "\n",
    "filenames_pos_valid = {}\n",
    "filenames_neg_valid = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        temp_namelist_pos = filenames_pos['{}_lead{}'.format(ver, lead)]\n",
    "        temp_namelist_neg = filenames_neg['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "        pos_train, pos_valid = name_extract(temp_namelist_pos)\n",
    "        neg_train, neg_valid = name_extract(temp_namelist_neg)\n",
    "        \n",
    "        print('pos train: {} pos valid: {} neg train: {} neg valid {}'.format(len(pos_train), \n",
    "                                                                              len(pos_valid), \n",
    "                                                                              len(neg_train), \n",
    "                                                                              len(neg_valid)))\n",
    "        \n",
    "        filenames_pos_train['{}_lead{}'.format(ver, lead)] = pos_train\n",
    "        filenames_neg_train['{}_lead{}'.format(ver, lead)] = neg_train\n",
    "        \n",
    "        filenames_pos_valid['{}_lead{}'.format(ver, lead)] = pos_valid\n",
    "        filenames_neg_valid['{}_lead{}'.format(ver, lead)] = neg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b635504-c7cb-4678-aced-e59f5105f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "pos_valid_all = []\n",
    "neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "233ffb82-ea53-422b-9f6c-8007c48da64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10639\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caf6e364-ee67-4f76-8a35-1f6728381bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to /glade/work/ksha/NCAR/CNN_Validation_lead2.hdf\n"
     ]
    }
   ],
   "source": [
    "filename_valid = neg_valid_all[::200] + pos_valid_all[::20]\n",
    "print(len(filename_valid))\n",
    "\n",
    "L_valid = len(filename_valid)\n",
    "\n",
    "VALID_input_64 = np.empty((L_valid, 64, 64, L_vars))\n",
    "VALID_target = np.ones(L_valid)\n",
    "\n",
    "for i, name in enumerate(filename_valid):\n",
    "    data = np.load(name)\n",
    "    for k, c in enumerate(ind_pick_from_batch):\n",
    "        \n",
    "        VALID_input_64[i, ..., k] = data[..., c]\n",
    "\n",
    "        if 'pos' in name:\n",
    "            VALID_target[i] = 1.0\n",
    "        else:\n",
    "            VALID_target[i] = 0.0\n",
    "            \n",
    "# save_dir = '/glade/work/ksha/NCAR/'\n",
    "# tuple_save = (VALID_input_64, VALID_target)\n",
    "# label_save = ['VALID_input_64', 'VALID_target']\n",
    "# du.save_hdf5(tuple_save, label_save, save_dir, 'CNN_Validation_lead2.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1b28db8-cf6c-4001-863b-20a8f3d3fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '/glade/work/ksha/NCAR/'\n",
    "# tuple_save = (VALID_input_64, VALID_target)\n",
    "# label_save = ['VALID_input_64', 'VALID_target']\n",
    "# du.save_hdf5(tuple_save, label_save, save_dir, 'CNN_Validation.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f44fde2-2fee-4675-a7cb-137c39af4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = '/glade/work/ksha/NCAR/'\n",
    "\n",
    "# with h5py.File(save_dir+'CNN_Validation_lead2.hdf', 'r') as h5io:\n",
    "#     VALID_input_64 = h5io['VALID_input_64'][...]\n",
    "#     VALID_target = h5io['VALID_target'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38d3dae8-6800-402a-bd5b-fdcedf4edc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_round = 0\n",
    "save_round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c8244af-fa39-4629-9e40-51ed0137dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 19:46:07.028758: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-01 19:46:07.035541: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-01 19:46:07.204200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-01 19:46:07.204263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-01 19:46:07.400652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-01 19:46:07.400690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-03-01 19:46:07.491239: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-01 19:46:07.583123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-01 19:46:07.701828: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-01 19:46:07.772341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-01 19:46:07.912686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-01 19:46:07.913797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-03-01 19:46:07.926036: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 19:46:07.926389: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-01 19:46:07.927013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:1a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-01 19:46:07.927035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-01 19:46:07.927056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-01 19:46:07.927068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-03-01 19:46:07.927079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-01 19:46:07.927100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-01 19:46:07.927112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-01 19:46:07.927123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-01 19:46:07.927134: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-01 19:46:07.928064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-03-01 19:46:07.928118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-01 19:46:11.087528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-01 19:46:11.087571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-03-01 19:46:11.087597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-03-01 19:46:11.099348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1a:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "model_head = create_model_head()\n",
    "model_base = create_model(input_shape=(64, 64, 15))\n",
    "\n",
    "IN = layers.Input(shape=(64, 64, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = Model(inputs=IN, outputs=OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b4f26db-b5b0-459e-8858-8d4aa8a59e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n",
      "2023-03-01 19:46:53.452795: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-03-01 19:46:53.454282: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n",
      "2023-03-01 19:46:53.780413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-01 19:46:57.993502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007773574597440406\n"
     ]
    }
   ],
   "source": [
    "if weights_round == 0:\n",
    "    W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_base5/')\n",
    "else:\n",
    "    W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak7_base{}/'.format(weights_round))\n",
    "    \n",
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "model_final.set_weights(W_old)\n",
    "\n",
    "Y_pred = model_final.predict([VALID_input_64])\n",
    "record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "650a5752-50d0-4bad-bd2f-ab9b3a0f4d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 0.00944613933569381\n",
      "0.007209673371499679\n",
      "Validation loss improved from 0.00944613933569381 to 0.007209673371499679\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_peak7_base1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 20:43:28.135876: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_peak7_base1/assets\n",
      "--- 468.7881398200989 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007471492290877191\n",
      "Validation loss 0.007471492290877191 NOT improved\n",
      "0.01033414866377093\n",
      "Validation loss 0.01033414866377093 NOT improved\n",
      "0.0069244856390468295\n",
      "Validation loss improved from 0.007209673371499679 to 0.0069244856390468295\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_peak7_base1\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_peak7_base1/assets\n",
      "--- 453.5581512451172 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011394173256506943\n",
      "Validation loss 0.011394173256506943 NOT improved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x2b0c448b45e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/work/ksha/anaconda3/lib/python3.9/weakref.py\", line 370, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     sregwet\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 61\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_pick\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ind_pick_from_batch):\n\u001b[1;32m     64\u001b[0m         temp \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, c] \n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:417\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    418\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seeds = 1567 #3725 #\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 64 #int(len(TRAIN_Y_pick) / batch_size)\n",
    "\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "key = 'RE2_peak7_base{}'.format(save_round)\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        N_pos = 10\n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "        file_pick_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "\n",
    "        file_pick_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "\n",
    "        if len(file_pick) != batch_size:\n",
    "            sregwet\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "\n",
    "            for l, c in enumerate(ind_pick_from_batch):\n",
    "                temp = data[..., c] \n",
    "                X_batch_64[k, ..., l] = temp\n",
    "\n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "            elif 'neg_neg_neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "\n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_64 = X_batch_64[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_64, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_64])\n",
    "    # Y_pred[Y_pred<0] = 0\n",
    "    # Y_pred[Y_pred>1] = 1\n",
    "\n",
    "    record_temp = verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     model.save(model_path_backup)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        #print('tol: {}'.format(tol))\n",
    "        # save\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62389b-486a-40a6-96f3-a8034ee8cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
