{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7085e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 14:55:54.919572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import re\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from random import shuffle\n",
    "from tensorflow import keras\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac09dcdf-0dc3-4ecc-82a9-64d5d14de6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f765a734-5c8c-422e-ad33-cc9331f49cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3f3d05-1099-4d04-bec6-5820a87b54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred):\n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    metric = BS\n",
    "    return metric\n",
    "\n",
    "def neighbour_leads(lead):\n",
    "    out = [lead-2, lead-1, lead, lead+1]\n",
    "    flag_shift = [0, 0, 0, 0]\n",
    "    \n",
    "    for i in range(4):\n",
    "        if out[i] < 0:\n",
    "            out[i] = 24+out[i]\n",
    "            flag_shift[i] = -1\n",
    "        if out[i] > 23:\n",
    "            out[i] = out[i]-24\n",
    "            flag_shift[i] = +1\n",
    "            \n",
    "    return out, flag_shift\n",
    "\n",
    "\n",
    "def filename_to_loc(filenames):\n",
    "    lead_out = []\n",
    "    indx_out = []\n",
    "    indy_out = []\n",
    "    day_out = []\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        \n",
    "        lead = int(nums[-1])\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "      \n",
    "        indx_out.append(indx)\n",
    "        indy_out.append(indy)\n",
    "        day_out.append(day)\n",
    "        lead_out.append(lead)\n",
    "        \n",
    "    return np.array(indx_out), np.array(indy_out), np.array(day_out), np.array(lead_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573a546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== #\n",
    "weights_round = 0\n",
    "save_round = 1\n",
    "seeds = 711 #777\n",
    "model_prefix_load = 'RE3_v4x_base{}'.format(weights_round) #False\n",
    "model_prefix_save = 'RE3_v4x_base{}'.format(save_round)\n",
    "N_vars = L_vars = 15\n",
    "lr = 1e-4\n",
    "# ==================== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962d4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------- #\n",
    "# Collect pos and neg batch filenames\n",
    "vers = ['v4x', 'v4'] # HRRR v4, v4x, v4\n",
    "leads = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc75b44-23ec-44e5-91ab-122e28c83465",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_pos_train = np.load(save_dir_campaign+'HRRR_filenames_pos_train.npy', allow_pickle=True)[()]\n",
    "filenames_neg_train = np.load(save_dir_campaign+'HRRR_filenames_neg_train.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2cbcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# Merge train/valid and pos/neg batch files from multiple lead times\n",
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "# pos_valid_all = []\n",
    "# neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        # pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        # neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "930092a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_v3 = []\n",
    "neg_train_v3 = []\n",
    "\n",
    "pos_train_v4x = []\n",
    "neg_train_v4x = []\n",
    "\n",
    "pos_train_v4 = []\n",
    "neg_train_v4 = []\n",
    "\n",
    "for lead in leads:\n",
    "    pos_train_v3 += filenames_pos_train['{}_lead{}'.format('v3', lead)]\n",
    "    neg_train_v3 += filenames_neg_train['{}_lead{}'.format('v3', lead)]\n",
    "    \n",
    "    pos_train_v4x += filenames_pos_train['{}_lead{}'.format('v4x', lead)]\n",
    "    neg_train_v4x += filenames_neg_train['{}_lead{}'.format('v4x', lead)]\n",
    "    \n",
    "    pos_train_v4 += filenames_pos_train['{}_lead{}'.format('v4', lead)]\n",
    "    neg_train_v4 += filenames_neg_train['{}_lead{}'.format('v4', lead)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ccf294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n",
      "Collect HRRR v3 labels ...\n",
      "Collect HRRR v4x labels ...\n",
      "Collect HRRR v4 labels ...\n",
      "... Done\n"
     ]
    }
   ],
   "source": [
    "label_smooth_v3 = ()\n",
    "label_smooth_v4x = ()\n",
    "label_smooth_v4 = ()\n",
    "\n",
    "for lead in leads:\n",
    "\n",
    "    lead_window, flag_shift = neighbour_leads(lead)\n",
    "    \n",
    "    print('Collect HRRR v3 labels ...')\n",
    "    \n",
    "    record_all = ()\n",
    "\n",
    "    for i, lead_temp in enumerate(lead_window):\n",
    "\n",
    "        flag_ = flag_shift[i]\n",
    "\n",
    "        with h5py.File(save_dir_scratch+'SPC_to_lead{}_72km_all.hdf'.format(lead_temp), 'r') as h5io:\n",
    "            record_temp = h5io['record_v3'][...]\n",
    "\n",
    "        if flag_shift[i] == 0:\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "        if flag_shift[i] == -1:\n",
    "            record_temp[1:, ...] = record_temp[:-1, ...]\n",
    "            record_temp[0, ...] = np.nan\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "        if flag_shift[i] == +1:\n",
    "            record_temp[:-1, ...] = record_temp[1:, ...]\n",
    "            record_temp[-1, ...] = np.nan\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "\n",
    "    shape_record = record_temp.shape      \n",
    "    record_v3 = np.empty(shape_record)\n",
    "    record_v3[...] = 0.0 #np.nan\n",
    "\n",
    "    for i in range(4):\n",
    "        record_temp = record_all[i]\n",
    "        for day in range(shape_record[0]):\n",
    "            for ix in range(shape_record[1]):\n",
    "                for iy in range(shape_record[2]):\n",
    "                    for event in range(shape_record[3]):\n",
    "                        if record_temp[day, ix, iy, event] > 0:\n",
    "                            record_v3[day, ix, iy, event] = 1.0\n",
    "                        elif record_v3[day, ix, iy, event] == 1.0:\n",
    "                            record_v3[day, ix, iy, event] = 1.0\n",
    "                        else:\n",
    "                            record_v3[day, ix, iy, event] = 0.0\n",
    "    \n",
    "    label_smooth_v3 += (record_v3[None, ...],)\n",
    "    \n",
    "    print('Collect HRRR v4x labels ...')\n",
    "    \n",
    "    record_all = ()\n",
    "\n",
    "    for i, lead_temp in enumerate(lead_window):\n",
    "\n",
    "        flag_ = flag_shift[i]\n",
    "\n",
    "        with h5py.File(save_dir_scratch+'SPC_to_lead{}_72km_v4x.hdf'.format(lead_temp), 'r') as h5io:\n",
    "            record_temp = h5io['record_v4x'][...]\n",
    "\n",
    "        if flag_shift[i] == 0:\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "        if flag_shift[i] == -1:\n",
    "            record_temp[1:, ...] = record_temp[:-1, ...]\n",
    "            record_temp[0, ...] = np.nan\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "        if flag_shift[i] == +1:\n",
    "            record_temp[:-1, ...] = record_temp[1:, ...]\n",
    "            record_temp[-1, ...] = np.nan\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "\n",
    "    shape_record = record_temp.shape      \n",
    "    record_v4x = np.empty(shape_record)\n",
    "    record_v4x[...] = np.nan\n",
    "\n",
    "    for i in range(4):\n",
    "        record_temp = record_all[i]\n",
    "        for day in range(shape_record[0]):\n",
    "            for ix in range(shape_record[1]):\n",
    "                for iy in range(shape_record[2]):\n",
    "                    for event in range(shape_record[3]):\n",
    "                        if record_temp[day, ix, iy, event] > 0:\n",
    "                            record_v4x[day, ix, iy, event] = 1.0\n",
    "                        elif record_v4x[day, ix, iy, event] == 1.0:\n",
    "                            record_v4x[day, ix, iy, event] = 1.0\n",
    "                        else:\n",
    "                            record_v4x[day, ix, iy, event] = 0.0\n",
    "    \n",
    "    label_smooth_v4x += (record_v4x[None, ...],)\n",
    "    \n",
    "    print('Collect HRRR v4 labels ...')\n",
    "    \n",
    "    record_all = ()\n",
    "    \n",
    "    for i, lead_temp in enumerate(lead_window):\n",
    "\n",
    "        flag_ = flag_shift[i]\n",
    "\n",
    "        with h5py.File(save_dir_scratch+'SPC_to_lead{}_72km_all.hdf'.format(lead_temp), 'r') as h5io:\n",
    "            record_temp = h5io['record_v4'][...]\n",
    "\n",
    "        if flag_shift[i] == 0:\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "        if flag_shift[i] == -1:\n",
    "            record_temp[1:, ...] = record_temp[:-1, ...]\n",
    "            record_temp[0, ...] = np.nan\n",
    "            record_all = record_all + (record_temp,)\n",
    "\n",
    "        if flag_shift[i] == +1:\n",
    "            record_temp[:-1, ...] = record_temp[1:, ...]\n",
    "            record_temp[-1, ...] = np.nan\n",
    "            record_all = record_all + (record_temp,)\n",
    "            \n",
    "            \n",
    "    shape_record = record_temp.shape      \n",
    "    record_v4 = np.empty(shape_record)\n",
    "    record_v4[...] = 0.0 #np.nan\n",
    "\n",
    "    for i in range(4):\n",
    "        record_temp = record_all[i]\n",
    "        for day in range(shape_record[0]):\n",
    "            for ix in range(shape_record[1]):\n",
    "                for iy in range(shape_record[2]):\n",
    "                    for event in range(shape_record[3]):\n",
    "                        if record_temp[day, ix, iy, event] > 0:\n",
    "                            record_v4[day, ix, iy, event] = 1.0\n",
    "                        elif record_v4[day, ix, iy, event] == 1.0:\n",
    "                            record_v4[day, ix, iy, event] = 1.0\n",
    "                        else:\n",
    "                            record_v4[day, ix, iy, event] = 0.0\n",
    "                            \n",
    "    label_smooth_v4 += (record_v4[None, ...],)\n",
    "    \n",
    "    print('... Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701f9c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d768b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_concat_v3 = np.concatenate(label_smooth_v3, axis=0)\n",
    "label_concat_v4x = np.concatenate(label_smooth_v4x, axis=0)\n",
    "label_concat_v4 = np.concatenate(label_smooth_v4, axis=0)\n",
    "\n",
    "label_concat_v3 = np.sum(label_concat_v3, axis=-1)\n",
    "label_concat_v3[label_concat_v3>1] = 1\n",
    "\n",
    "label_concat_v4 = np.sum(label_concat_v4, axis=-1)\n",
    "label_concat_v4[label_concat_v4>1] = 1\n",
    "\n",
    "label_concat_v4x = np.sum(label_concat_v4x, axis=-1)\n",
    "label_concat_v4x[label_concat_v4x>1] = 1\n",
    "\n",
    "shape_label_v3 = label_concat_v3.shape\n",
    "shape_label_v4 = label_concat_v4.shape\n",
    "shape_label_v4x = label_concat_v4x.shape\n",
    "\n",
    "label_final_v3 = np.empty(shape_label_v3)\n",
    "label_final_v4 = np.empty(shape_label_v4)\n",
    "label_final_v4x = np.empty(shape_label_v4x)\n",
    "\n",
    "for i in range(shape_label_v3[0]):\n",
    "    for j in range(shape_label_v3[1]):\n",
    "        label_final_v3[i, j, ...] = scipy.ndimage.gaussian_filter(label_concat_v3[i, j], sigma=2.5)\n",
    "        \n",
    "for i in range(shape_label_v4[0]):\n",
    "    for j in range(shape_label_v4[1]):\n",
    "        label_final_v4[i, j, ...] = scipy.ndimage.gaussian_filter(label_concat_v4[i, j], sigma=2.5)\n",
    "        \n",
    "for i in range(shape_label_v4x[0]):\n",
    "    for j in range(shape_label_v4x[1]):\n",
    "        label_final_v4x[i, j, ...] = scipy.ndimage.gaussian_filter(label_concat_v4x[i, j], sigma=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b000840-1a94-4bbe-a196-cd82d7215cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_pos_train_v3, indy_pos_train_v3, day_pos_train_v3, lead_pos_train_v3 = filename_to_loc(pos_train_v3)\n",
    "indx_neg_train_v3, indy_neg_train_v3, day_neg_train_v3, lead_neg_train_v3 = filename_to_loc(neg_train_v3)\n",
    "\n",
    "lead_pos_train_v3 = lead_pos_train_v3 - 2\n",
    "lead_neg_train_v3 = lead_neg_train_v3 - 2\n",
    "\n",
    "y_pos_train_v3 = label_final_v3[lead_pos_train_v3, day_pos_train_v3, indx_pos_train_v3, indy_pos_train_v3]\n",
    "y_neg_train_v3 = label_final_v3[lead_neg_train_v3, day_neg_train_v3, indx_neg_train_v3, indy_neg_train_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f510e17-fd5d-4fb7-8cee-5ac072d953a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx_pos_train_v4x, indy_pos_train_v4x, day_pos_train_v4x, lead_pos_train_v4x = filename_to_loc(pos_train_v4x)\n",
    "indx_neg_train_v4x, indy_neg_train_v4x, day_neg_train_v4x, lead_neg_train_v4x = filename_to_loc(neg_train_v4x)\n",
    "\n",
    "lead_pos_train_v4x = lead_pos_train_v4x - 2\n",
    "lead_neg_train_v4x = lead_neg_train_v4x - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fc6406f-9f50-41d1-9c30-812468531012",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos_train_v4x = label_final_v4x[lead_pos_train_v4x, day_pos_train_v4x, indx_pos_train_v4x, indy_pos_train_v4x]\n",
    "y_neg_train_v4x = label_final_v4x[lead_neg_train_v4x, day_neg_train_v4x, indx_neg_train_v4x, indy_neg_train_v4x]\n",
    "\n",
    "y_pos_train_all = np.concatenate((y_pos_train_v3, y_pos_train_v4x,), axis=0)\n",
    "y_neg_train_all = np.concatenate((y_neg_train_v3, y_neg_train_v4x,), axis=0)\n",
    "\n",
    "y_pos_train_all_adjust = np.copy(y_pos_train_all) + 0.75\n",
    "y_pos_train_all_adjust[y_pos_train_all_adjust>0.99] = 0.99\n",
    "\n",
    "# y_neg_train_all_adjust = np.copy(y_neg_train_all)\n",
    "# y_neg_train_all_adjust[y_neg_train_all_adjust>0.49] = 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92fad923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f89cb2da-9cc8-4269-94a2-dc9707e08c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "393b102a-9c1d-4a3f-b691-a1afb6dab77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14f54bab-2b69-44a6-9726-e1ca82512366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b33159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "with h5py.File(save_dir+'CNN_Validation_basic.hdf', 'r') as h5io:\n",
    "    VALID_input_64 = h5io['VALID_input_64'][...]\n",
    "    VALID_target = h5io['VALID_target'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d2f6e-224a-4875-bb62-c341a180610f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e74a5-01cc-41e9-ad45-23780306ebef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bff6cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 13:40:42.497134: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-28 13:40:42.955221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-28 13:40:43.078077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-04-28 13:40:43.078180: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-28 13:40:43.278781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-28 13:40:43.279702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-04-28 13:40:43.353810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-28 13:40:43.427395: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-28 13:40:43.544046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-28 13:40:43.631228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-28 13:40:43.793579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-28 13:40:43.795706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-04-28 13:40:43.820395: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 13:40:43.821082: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-28 13:40:43.822313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:b3:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-04-28 13:40:43.822649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-28 13:40:43.822862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-28 13:40:43.822880: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-04-28 13:40:43.822893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-28 13:40:43.822917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-28 13:40:43.822930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-28 13:40:43.822942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-28 13:40:43.822976: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-28 13:40:43.824098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-04-28 13:40:43.824151: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-28 13:40:47.981537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-28 13:40:47.981704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-04-28 13:40:47.981737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-04-28 13:40:47.985575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "# model and weights\n",
    "model_head = mu.create_model_head(input_shape=(128,), N_node=64)\n",
    "model_base = mu.create_model_base(input_shape=(64, 64, 15), depths=[3, 3, 27, 3], projection_dims=[32, 64, 96, 128], first_pool=4)\n",
    "\n",
    "IN = keras.layers.Input(shape=(64, 64, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = keras.models.Model(inputs=IN, outputs=OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c79db583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= #\n",
    "# Weights\n",
    "\n",
    "if weights_round > 0:\n",
    "    if model_prefix_load:\n",
    "        W_old = mu.dummy_loader('/glade/work/ksha/NCAR/Keras_models/{}/'.format(model_prefix_load))\n",
    "        model_final.set_weights(W_old)\n",
    "    \n",
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46875f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc105378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "# model training loop\n",
    "Y_pred = model_final.predict([VALID_input_64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbd1d1c5-72ef-45ae-bc33-4bacc5d6e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c17de5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04704005223131508\n"
     ]
    }
   ],
   "source": [
    "# Change based on smoothed labels\n",
    "print(record_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf128f-e1ca-4050-89f1-d6b5fede6672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0eb08b-820a-482b-b6d6-dd939ba46a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc4c72-9270-4a0e-ab1f-6cf7b6476e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e218ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 0.023274358203234598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m     model_final\u001b[38;5;241m.\u001b[39mtrain_on_batch(X_batch_64, Y_batch);\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# epoch end operations\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mVALID_input_64\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m record_temp \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mverif_metric(VALID_target, Y_pred)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (record \u001b[38;5;241m-\u001b[39m record_temp \u001b[38;5;241m>\u001b[39m min_del):\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1598\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1592\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1593\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1594\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1595\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1596\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1598\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1100\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1099\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1115\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mget_dataset()\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:263\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    253\u001b[0m              x,\n\u001b[1;32m    254\u001b[0m              y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m   \u001b[38;5;28msuper\u001b[39m(TensorLikeDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 263\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m   sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    265\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[1;32m    267\u001b[0m   \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1016\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1016\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_numpy_and_scipy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/nest.py:659\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/nest.py:659\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1011\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1010\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1011\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor_v2_with_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m scipy_sparse \u001b[38;5;129;01mand\u001b[39;00m scipy_sparse\u001b[38;5;241m.\u001b[39missparse(x):\n\u001b[1;32m   1013\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1404\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1343\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1344\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \n\u001b[1;32m   1346\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1410\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1409\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1410\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1540\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1535\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1537\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1540\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1543\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:264\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    275\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    279\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    300\u001b[0m   \u001b[38;5;124;03m\"\"\"Implementation of eager constant.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "epochs = 500\n",
    "L_train = 64\n",
    "min_del = 0.0\n",
    "max_tol = 100 # early stopping with patience\n",
    "batch_size = 200\n",
    "\n",
    "# Allocate batch files\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "# Model check-point info\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "model_name = model_prefix_save\n",
    "model_path = temp_dir + model_name\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "tol = 0 # initial tol\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "mu.set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        N_pos = 20\n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "        \n",
    "        # neg batches from this training rotation \n",
    "        file_pick_neg = []\n",
    "        file_label_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "            file_label_neg.append(y_neg_train_all_adjust[ind_temp])\n",
    "            \n",
    "        # pos batches from this training rotation \n",
    "        file_pick_pos = []\n",
    "        file_label_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "            file_label_pos.append(y_pos_train_all_adjust[ind_temp])\n",
    "            \n",
    "        # get all the batch filenames for checking labels\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "        file_label = file_label_neg + file_label_pos\n",
    "        Y_batch = np.array(file_label)[:, None]\n",
    "        \n",
    "#         if len(file_pick) != batch_size:\n",
    "#             sregwet # number of available files = batch size\n",
    "\n",
    "        # Assign labels based on batch filenames\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "            for l, c in enumerate(ind_pick_from_batch):\n",
    "                temp = data[..., c] \n",
    "                X_batch_64[k, ..., l] = temp\n",
    "\n",
    "#             if 'pos' in file_pick[k]:\n",
    "#                 Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "#             elif 'neg_neg_neg' in file_pick[k]:\n",
    "#                 Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "#             else:\n",
    "#                 werhgaer\n",
    "        # ------------------------------------------------- #\n",
    "        # batch input and label from this training rotation \n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_64 = X_batch_64[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_64, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_64])\n",
    "    record_temp = mu.verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920441c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcc4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2be21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65a950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
