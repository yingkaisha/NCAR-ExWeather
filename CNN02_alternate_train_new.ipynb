{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739d9b65-76f1-4e6c-8c6f-f2ffe86ab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e056a192-f23f-444b-af9d-7141f39d3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeabc4a7-1bfd-4e45-9eb0-274d72a10c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e5963a-bc31-418e-9859-f1e0a9b8d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 09:29:05.616296: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# from keras_unet_collection import models as k_models\n",
    "from keras_unet_collection import utils as k_utils\n",
    "# from keras_unet_collection import layer_utils as k_layers\n",
    "# from keras_unet_collection.activations import GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb39d9d-280c-4377-8a35-fe5d877361d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import graph_utils as gu\n",
    "#import convnext_keras as ck\n",
    "\n",
    "from sklearn.metrics import classification_report, auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21364371-260e-4081-9ed5-9ba9deccec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af3ce85-02e8-4178-bba1-dcc408f1a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "def create_model(input_shape=(64, 64, 15)):\n",
    "\n",
    "    depths=[3, 3, 27, 3]\n",
    "    projection_dims=[32, 64, 96, 128]\n",
    "    drop_path_rate=0.0\n",
    "    layer_scale_init_value=1e-6\n",
    "\n",
    "\n",
    "    model_name='Branch64X'\n",
    "    IN64 = layers.Input(shape=input_shape)\n",
    "    X = IN64\n",
    "    # ----- convnext block 0 ----- #\n",
    "\n",
    "    X = layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=\"{}_down0\".format(model_name))(X)\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_norm\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[0]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[0], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[0], name=\"{}_down0_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[0], name=\"{}_down0_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down0_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[0], name=\"{}_down0_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[0], name=\"{}_down0_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "\n",
    "    # ----- convnext block 1 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[1], kernel_size=2, strides=2, name=\"{}_down1\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[1]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[1], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[1], name=\"{}_down1_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[1], name=\"{}_down1_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down1_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[1], name=\"{}_down1_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[1], name=\"{}_down1_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 2 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[2], kernel_size=2, strides=2, name=\"{}_down2\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[2]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[2], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[2], name=\"{}_down2_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[2], name=\"{}_down2_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down2_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[2], name=\"{}_down2_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[2], name=\"{}_down2_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 3 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[3], kernel_size=2, padding='same', name=\"{}_down3\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[3]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[3], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[3], name=\"{}_down3_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[3], name=\"{}_down3_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down3_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[3], name=\"{}_down3_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[3], name=\"{}_down3_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    V1 = X\n",
    "\n",
    "    OUT = layers.GlobalMaxPooling2D(name=\"{}_head_pool64\".format(model_name))(V1)\n",
    "#     OUT = layers.LayerNormalization(epsilon=1e-6, name=\"{}_head_norm64\".format(model_name))(OUT)\n",
    "\n",
    "#     OUT = layers.Dense(64, name=\"{}_dense1\".format(model_name))(OUT)\n",
    "#     OUT = layers.LayerNormalization(epsilon=1e-6, name=\"{}_dense1_norm\".format(model_name))(OUT)\n",
    "#     OUT = layers.Activation(\"gelu\", name=\"{}_dense1_gelu{}\".format(model_name, j))(OUT)\n",
    "\n",
    "#     OUT = layers.Dense(1, name=\"{}_head_out\".format(model_name))(OUT)\n",
    "\n",
    "    model = Model(inputs=IN64, outputs=OUT, name=model_name)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52dc9279-8963-4ee9-809d-8dd913fa4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head():\n",
    "\n",
    "    \n",
    "    IN_vec = keras.Input((128,))    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64367cd7-bd33-40a5-bbc5-4d888967a70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffe8d39-8d0b-49ff-8ad0-84b95e8ba5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred):\n",
    "\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    # AUC = auc(fpr, tpr)\n",
    "    # AUC_metric = 1 - AUC\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    #ll = log_loss(VALID_target.ravel(), Y_pred.ravel())\n",
    "    \n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd31da6f-0913-4a6d-93a3-ec24453bdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_pick_from_batch = [0, 1, 3, 4, 8, 9, 10, 13, 14, 15, 16, 17, 18, 21, 22]\n",
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "L_vars = len(ind_pick_from_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c974f051-af9a-49ef-8015-683fcb942226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3, lead2, pos: 5510, neg: 887822\n",
      "v3, lead3, pos: 4297, neg: 889035\n",
      "v3, lead4, pos: 3242, neg: 890090\n",
      "v3, lead5, pos: 2565, neg: 891795\n",
      "v3, lead6, pos: 2127, neg: 891205\n",
      "v3, lead20, pos: 5407, neg: 887925\n",
      "v3, lead21, pos: 6711, neg: 887649\n",
      "v3, lead22, pos: 7613, neg: 886747\n",
      "v3, lead23, pos: 8009, neg: 886351\n",
      "v4x, lead2, pos: 1995, neg: 348553\n",
      "v4x, lead3, pos: 1618, neg: 349958\n",
      "v4x, lead4, pos: 1218, neg: 350358\n",
      "v4x, lead5, pos: 962, neg: 350614\n",
      "v4x, lead6, pos: 817, neg: 349731\n",
      "v4x, lead20, pos: 1777, neg: 344659\n",
      "v4x, lead21, pos: 2209, neg: 344227\n",
      "v4x, lead22, pos: 2511, neg: 343925\n",
      "v4x, lead23, pos: 2690, neg: 343746\n",
      "v4, lead2, pos: 1995, neg: 348553\n",
      "v4, lead3, pos: 1618, neg: 349958\n",
      "v4, lead4, pos: 1218, neg: 350358\n",
      "v4, lead5, pos: 962, neg: 350614\n",
      "v4, lead6, pos: 817, neg: 349731\n",
      "v4, lead20, pos: 1777, neg: 344659\n",
      "v4, lead21, pos: 2209, neg: 344227\n",
      "v4, lead22, pos: 2511, neg: 343925\n",
      "v4, lead23, pos: 2690, neg: 343746\n"
     ]
    }
   ],
   "source": [
    "vers = ['v3', 'v4x', 'v4']\n",
    "leads = [2, 3, 4, 5, 6, 20, 21, 22, 23]\n",
    "filenames_pos = {}\n",
    "filenames_neg = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        if ver == 'v3' and lead < 23:\n",
    "            path_ = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "        elif ver == 'v3' and lead == 23:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "        else:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "            \n",
    "        filenames_pos['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*pos*lead{}.npy\".format(path_, lead)))\n",
    "        filenames_neg['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*neg_neg_neg*lead{}.npy\".format(path_, lead)))\n",
    "        \n",
    "        print('{}, lead{}, pos: {}, neg: {}'.format(ver, lead, \n",
    "                                                    len(filenames_pos['{}_lead{}'.format(ver, lead)]), \n",
    "                                                    len(filenames_neg['{}_lead{}'.format(ver, lead)])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7ecbaf-868c-4d76-9f23-93630d4764b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_extract(filenames):\n",
    "    \n",
    "    date_base = datetime(2020, 7, 14)\n",
    "    \n",
    "    filename_train = []\n",
    "    filename_valid = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+365+30)]\n",
    "    \n",
    "    base_ref = datetime(2019, 10, 1)\n",
    "    date_list_v4x = [base_ref + timedelta(days=day) for day in range(429)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4x' in name:\n",
    "            date_list = date_list_v4x\n",
    "        elif 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        \n",
    "        if (day - date_base).days > 0:\n",
    "            filename_valid.append(name)\n",
    "        else:\n",
    "            filename_train.append(name)\n",
    "\n",
    "        \n",
    "    return filename_train, filename_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03bc3af-350a-4f70-a2f0-a8c59b539f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos train: 4874 pos valid: 636 neg train: 743510 neg valid 144312\n",
      "pos train: 3803 pos valid: 494 neg train: 744581 neg valid 144454\n",
      "pos train: 2888 pos valid: 354 neg train: 745496 neg valid 144594\n",
      "pos train: 2295 pos valid: 270 neg train: 747117 neg valid 144678\n",
      "pos train: 1888 pos valid: 239 neg train: 746496 neg valid 144709\n",
      "pos train: 4637 pos valid: 770 neg train: 743747 neg valid 144178\n",
      "pos train: 5779 pos valid: 932 neg train: 743633 neg valid 144016\n",
      "pos train: 6595 pos valid: 1018 neg train: 742817 neg valid 143930\n",
      "pos train: 6960 pos valid: 1049 neg train: 742452 neg valid 143899\n",
      "pos train: 1470 pos valid: 525 neg train: 226746 neg valid 121807\n",
      "pos train: 1203 pos valid: 415 neg train: 227013 neg valid 122945\n",
      "pos train: 912 pos valid: 306 neg train: 227304 neg valid 123054\n",
      "pos train: 727 pos valid: 235 neg train: 227489 neg valid 123125\n",
      "pos train: 601 pos valid: 216 neg train: 226587 neg valid 123144\n",
      "pos train: 1141 pos valid: 636 neg train: 221935 neg valid 122724\n",
      "pos train: 1431 pos valid: 778 neg train: 221645 neg valid 122582\n",
      "pos train: 1662 pos valid: 849 neg train: 221414 neg valid 122511\n",
      "pos train: 1808 pos valid: 882 neg train: 221268 neg valid 122478\n",
      "pos train: 1470 pos valid: 525 neg train: 226746 neg valid 121807\n",
      "pos train: 1203 pos valid: 415 neg train: 227013 neg valid 122945\n",
      "pos train: 912 pos valid: 306 neg train: 227304 neg valid 123054\n",
      "pos train: 727 pos valid: 235 neg train: 227489 neg valid 123125\n",
      "pos train: 601 pos valid: 216 neg train: 226587 neg valid 123144\n",
      "pos train: 1141 pos valid: 636 neg train: 221935 neg valid 122724\n",
      "pos train: 1431 pos valid: 778 neg train: 221645 neg valid 122582\n",
      "pos train: 1662 pos valid: 849 neg train: 221414 neg valid 122511\n",
      "pos train: 1808 pos valid: 882 neg train: 221268 neg valid 122478\n"
     ]
    }
   ],
   "source": [
    "filenames_pos_train = {}\n",
    "filenames_neg_train = {}\n",
    "\n",
    "filenames_pos_valid = {}\n",
    "filenames_neg_valid = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        temp_namelist_pos = filenames_pos['{}_lead{}'.format(ver, lead)]\n",
    "        temp_namelist_neg = filenames_neg['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "        pos_train, pos_valid = name_extract(temp_namelist_pos)\n",
    "        neg_train, neg_valid = name_extract(temp_namelist_neg)\n",
    "        \n",
    "        print('pos train: {} pos valid: {} neg train: {} neg valid {}'.format(len(pos_train), \n",
    "                                                                              len(pos_valid), \n",
    "                                                                              len(neg_train), \n",
    "                                                                              len(neg_valid)))\n",
    "        \n",
    "        filenames_pos_train['{}_lead{}'.format(ver, lead)] = pos_train\n",
    "        filenames_neg_train['{}_lead{}'.format(ver, lead)] = neg_train\n",
    "        \n",
    "        filenames_pos_valid['{}_lead{}'.format(ver, lead)] = pos_valid\n",
    "        filenames_neg_valid['{}_lead{}'.format(ver, lead)] = neg_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b635504-c7cb-4678-aced-e59f5105f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "pos_valid_all = []\n",
    "neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03fdb6-5dd9-40b2-be3d-096a7993b628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf6e364-ee67-4f76-8a35-1f6728381bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename_valid = neg_valid_all[::200] + pos_valid_all[::20]\n",
    "# print(len(filename_valid))\n",
    "\n",
    "# L_valid = len(filename_valid)\n",
    "\n",
    "# VALID_input_64 = np.empty((L_valid, 64, 64, L_vars))\n",
    "# VALID_target = np.ones(L_valid)\n",
    "\n",
    "# for i, name in enumerate(filename_valid):\n",
    "#     data = np.load(name)\n",
    "#     for k, c in enumerate(ind_pick_from_batch):\n",
    "        \n",
    "#         VALID_input_64[i, ..., k] = data[..., c]\n",
    "\n",
    "#         if 'pos' in name:\n",
    "#             VALID_target[i] = 1.0\n",
    "#         else:\n",
    "#             VALID_target[i] = 0.0\n",
    "            \n",
    "# save_dir = '/glade/work/ksha/NCAR/'\n",
    "# tuple_save = (VALID_input_64, VALID_target)\n",
    "# label_save = ['VALID_input_64', 'VALID_target']\n",
    "# du.save_hdf5(tuple_save, label_save, save_dir, 'CNN_Validation.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f44fde2-2fee-4675-a7cb-137c39af4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/glade/work/ksha/NCAR/'\n",
    "\n",
    "with h5py.File(save_dir+'CNN_Validation.hdf', 'r') as h5io:\n",
    "    VALID_input_64 = h5io['VALID_input_64'][...]\n",
    "    VALID_target = h5io['VALID_target'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf993e3-6695-4d65-9d71-1c7e3af410af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f47b34d7-8f15-4e7b-972c-1dedf1cafd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_train = 'head'\n",
    "#flag_train = 'base'\n",
    "\n",
    "if flag_train == 'head':\n",
    "    flag_weights = 'base'\n",
    "else:\n",
    "    flag_weights = 'head'\n",
    "    \n",
    "weights_round = 1\n",
    "save_round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a8a3f-1c3b-409b-b218-dabcd3f7e4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a6cb4-f848-43fc-8947-aff34d1dce4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b05c8-8daa-440d-a100-25e860b05344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d413b66-e165-4461-8e9f-f79ac93a6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_head = create_model_head()\n",
    "\n",
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/HY_Lead4/')\n",
    "# model_head.set_weights(W_old)\n",
    "\n",
    "model_base = create_model(input_shape=(64, 64, 15))\n",
    "\n",
    "# W_new = model_base.get_weights()\n",
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/{}/'.format('RE1_15p_full'))\n",
    "\n",
    "# for i in range(len(W_new)):\n",
    "#     if W_new[i].shape == W_old[i].shape:\n",
    "#         W_new[i] = W_old[i]\n",
    "#     else:\n",
    "#         ewraewthws\n",
    "        \n",
    "# model_base.set_weights(W_new)\n",
    "\n",
    "if flag_train == 'base':\n",
    "    for layer in model_head.layers: \n",
    "        layer.trainable = False\n",
    "elif flag_train == 'head':\n",
    "    for layer in model_base.layers: \n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8244af-fa39-4629-9e40-51ed0137dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = layers.Input(shape=(64, 64, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = Model(inputs=IN, outputs=OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b4f26db-b5b0-459e-8858-8d4aa8a59e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_{}{}/'.format(flag_weights, weights_round))\n",
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "model_final.set_weights(W_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c520d28-36a1-4a1a-8230-c7015dc80420",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_train == 'base':\n",
    "    for layer in model_head.layers: \n",
    "        layer.trainable = False\n",
    "elif flag_train == 'head':\n",
    "    for layer in model_base.layers: \n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e8b29-ad84-4194-92ea-0a7904cd0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_final.predict([VALID_input_64])\n",
    "record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071dd20-3a6a-4fe9-a5c9-06841dcc1399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77702b7c-71eb-4f9e-ac83-b0ea6d6e4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE1_15p_base/')\n",
    "# model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "# model_final.set_weights(W_old)\n",
    "\n",
    "# Y_pred = model_final.predict([VALID_input_64])\n",
    "# record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996920bf-e9c7-4c08-805c-153eaf81581e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ca53b-7cc5-4a30-af69-2e5a4155f127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5f4ab-39bb-4f0c-932b-594c63af5625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE1_15p_full/')\n",
    "# model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "# model_final.set_weights(W_old)\n",
    "\n",
    "# Y_pred = model_final.predict([VALID_input_64])\n",
    "# record_temp = verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "#0.03020596704790702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dc43d-fcad-4a7e-b5ca-eef77c21da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_pred)\n",
    "plt.plot(VALID_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9acd6e-2265-4ec1-90dc-989e63bffd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a5752-50d0-4bad-bd2f-ab9b3a0f4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = 3725 #1567\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 10 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 64 #int(len(TRAIN_Y_pick) / batch_size)\n",
    "\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "key = 'RE2_peak_{}{}'.format(flag_train, save_round)\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp #0.01840167896363949 #record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        if flag_train == 'base':\n",
    "            N_pos = 20\n",
    "        else:\n",
    "            N_pos = 100\n",
    "            \n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "        file_pick_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "\n",
    "        file_pick_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "\n",
    "        if len(file_pick) != batch_size:\n",
    "            sregwet\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "\n",
    "            for l, c in enumerate(ind_pick_from_batch):\n",
    "                temp = data[..., c] \n",
    "                X_batch_64[k, ..., l] = temp\n",
    "\n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "            elif 'neg_neg_neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "\n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_64 = X_batch_64[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_64, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_64])\n",
    "    # Y_pred[Y_pred<0] = 0\n",
    "    # Y_pred[Y_pred>1] = 1\n",
    "\n",
    "    record_temp = verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     model.save(model_path_backup)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        #print('tol: {}'.format(tol))\n",
    "        # save\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac91e18-a5fd-4975-924f-e521b9c91066",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d227-11df-4ab2-b3e6-399d1c620446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872fe44-4992-44df-8b73-efd3f8ae2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590427b-a292-43d2-bd5e-0dc3862c7971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62389b-486a-40a6-96f3-a8034ee8cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
