{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060452ac-b15c-4c3f-aa00-1638cc0597d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 08:40:30.528836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('lead1', help='lead1')\n",
    "# parser.add_argument('lead2', help='lead2')\n",
    "# parser.add_argument('lead3', help='lead3')\n",
    "# parser.add_argument('lead4', help='lead4')\n",
    "\n",
    "# parser.add_argument('lead_name', help='lead_name')\n",
    "# parser.add_argument('model_tag', help='model_tag')\n",
    "\n",
    "# args = vars(parser.parse_args())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc5b3f8-670d-4b93-8041-c21cbf465095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== #\n",
    "\n",
    "lead1 = 19\n",
    "lead2 = 20 #int(args['lead2'])\n",
    "lead3 = 21 #int(args['lead3'])\n",
    "lead4 = 22 #int(args['lead4'])\n",
    "\n",
    "lead_name = 21 #args['lead_name']\n",
    "model_tag = 'peak19' #args['model_tag']\n",
    "\n",
    "L_vec = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c882f4-a372-4b9a-80dd-c4eb0106e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "# Functions\n",
    "\n",
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def verif_metric(VALID_target, Y_pred, ref):\n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    metric = BS\n",
    "\n",
    "    return metric / ref\n",
    "\n",
    "def feature_extract(filenames, lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max):\n",
    "    \n",
    "    lon_out = []\n",
    "    lat_out = []\n",
    "    elev_out = []\n",
    "    mon_out = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+180-151)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        month = day.month\n",
    "        \n",
    "        month_norm = (month - 1)/(12-1)\n",
    "        \n",
    "        lon = lon_80km[indx, indy]\n",
    "        lat = lat_80km[indx, indy]\n",
    "\n",
    "        lon = (lon - lon_minmax[0])/(lon_minmax[1] - lon_minmax[0])\n",
    "        lat = (lat - lat_minmax[0])/(lat_minmax[1] - lat_minmax[0])\n",
    "\n",
    "        elev = elev_80km[indx, indy]\n",
    "        elev = elev / elev_max\n",
    "        \n",
    "        lon_out.append(lon)\n",
    "        lat_out.append(lat)\n",
    "        elev_out.append(elev)\n",
    "        mon_out.append(month_norm)\n",
    "        \n",
    "    return np.array(lon_out), np.array(lat_out), np.array(elev_out), np.array(mon_out)\n",
    "\n",
    "def name_to_ind(filenames):\n",
    "    \n",
    "    indx_out = []\n",
    "    indy_out = []\n",
    "    day_out = []\n",
    "    flag_out = []\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        \n",
    "        indx_out.append(indx)\n",
    "        indy_out.append(indy)\n",
    "        day_out.append(day)\n",
    "        \n",
    "        if \"pos\" in name:\n",
    "            flag_out.append(True)\n",
    "        else:\n",
    "            flag_out.append(False)\n",
    "        \n",
    "    return np.array(indx_out), np.array(indy_out), np.array(day_out), np.array(flag_out)\n",
    "\n",
    "def feature_extract(filenames, lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max):\n",
    "    \n",
    "    lon_out = []\n",
    "    lat_out = []\n",
    "    elev_out = []\n",
    "    mon_out = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+180-151)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        month = day.month\n",
    "        \n",
    "        month_norm = (month - 1)/(12-1)\n",
    "        \n",
    "        lon = lon_80km[indx, indy]\n",
    "        lat = lat_80km[indx, indy]\n",
    "\n",
    "        lon = (lon - lon_minmax[0])/(lon_minmax[1] - lon_minmax[0])\n",
    "        lat = (lat - lat_minmax[0])/(lat_minmax[1] - lat_minmax[0])\n",
    "\n",
    "        elev = elev_80km[indx, indy]\n",
    "        elev = elev / elev_max\n",
    "        \n",
    "        lon_out.append(lon)\n",
    "        lat_out.append(lat)\n",
    "        elev_out.append(elev)\n",
    "        mon_out.append(month_norm)\n",
    "        \n",
    "    return np.array(lon_out), np.array(lat_out), np.array(elev_out), np.array(mon_out)\n",
    "\n",
    "def name_to_ind(filenames):\n",
    "    \n",
    "    indx_out = []\n",
    "    indy_out = []\n",
    "    day_out = []\n",
    "    flag_out = []\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        \n",
    "        indx_out.append(indx)\n",
    "        indy_out.append(indy)\n",
    "        day_out.append(day)\n",
    "        \n",
    "        if \"pos\" in name:\n",
    "            flag_out.append(True)\n",
    "        else:\n",
    "            flag_out.append(False)\n",
    "        \n",
    "    return np.array(indx_out), np.array(indy_out), np.array(day_out), np.array(flag_out)\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    IN = keras.Input((L_vec, 128))\n",
    "    X = IN\n",
    "    X = keras.layers.Conv1D(128, kernel_size=2, strides=1, padding='valid')(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "    \n",
    "    #\n",
    "    IN_vec = keras.Input((2,))\n",
    "    \n",
    "    X = keras.layers.GlobalMaxPool1D()(X) #X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.Concatenate()([X, IN_vec])\n",
    "    \n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=[IN, IN_vec], outputs=OUT)\n",
    "    return model\n",
    "\n",
    "# ================================================================ #\n",
    "# Geographical information\n",
    "\n",
    "with h5py.File(save_dir+'HRRR_domain.hdf', 'r') as h5io:\n",
    "    lon_3km = h5io['lon_3km'][...]\n",
    "    lat_3km = h5io['lat_3km'][...]\n",
    "    lon_80km = h5io['lon_80km'][...]\n",
    "    lat_80km = h5io['lat_80km'][...]\n",
    "    elev_3km = h5io['elev_3km'][...]\n",
    "    land_mask_80km = h5io['land_mask_80km'][...]\n",
    "    \n",
    "grid_shape = land_mask_80km.shape\n",
    "\n",
    "elev_80km = du.interp2d_wraper(lon_3km, lat_3km, elev_3km, lon_80km, lat_80km, method='linear')\n",
    "\n",
    "elev_80km[np.isnan(elev_80km)] = 0\n",
    "elev_80km[elev_80km<0] = 0\n",
    "elev_max = np.max(elev_80km)\n",
    "\n",
    "lon_80km_mask = lon_80km[land_mask_80km]\n",
    "lat_80km_mask = lat_80km[land_mask_80km]\n",
    "\n",
    "lon_minmax = [np.min(lon_80km_mask), np.max(lon_80km_mask)]\n",
    "lat_minmax = [np.min(lat_80km_mask), np.max(lat_80km_mask)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b318872-5672-49cf-85e3-579f7407d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================ #\n",
    "# File path\n",
    "\n",
    "filepath_vec = \"/glade/work/ksha/NCAR/\"\n",
    "\n",
    "path_name1_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "path_name2_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/' \n",
    "path_name3_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "path_name4_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "\n",
    "path_name1_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name2_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name3_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name4_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "\n",
    "path_name1_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name2_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name3_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name4_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "\n",
    "# ========================================================================= #\n",
    "# Read batch file names (npy)\n",
    "\n",
    "filename_train_lead1_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_train_lead2_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name2_v3, lead2)))\n",
    "filename_train_lead3_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name3_v3, lead3)))\n",
    "filename_train_lead4_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name4_v3, lead4)))\n",
    "\n",
    "filename_valid_lead1_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_valid_lead2_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name2_v3, lead2)))\n",
    "filename_valid_lead3_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name3_v3, lead3)))\n",
    "filename_valid_lead4_v3 = sorted(glob(\"{}VALID*lead{}.npy\".format(path_name4_v3, lead4)))\n",
    "\n",
    "# ============================================================ #\n",
    "# Consistency check indices\n",
    "\n",
    "IND_TRAIN_lead = np.load('/glade/work/ksha/NCAR/IND_TRAIN_lead_full.npy', allow_pickle=True)[()]\n",
    "TRAIN_ind1_v3 = IND_TRAIN_lead['lead{}'.format(lead1)]\n",
    "TRAIN_ind2_v3 = IND_TRAIN_lead['lead{}'.format(lead2)]\n",
    "TRAIN_ind3_v3 = IND_TRAIN_lead['lead{}'.format(lead3)]\n",
    "TRAIN_ind4_v3 = IND_TRAIN_lead['lead{}'.format(lead4)]\n",
    "\n",
    "IND_VALID_lead = np.load('/glade/work/ksha/NCAR/IND_VALID_lead_full.npy', allow_pickle=True)[()]\n",
    "VALID_ind1_v3 = IND_VALID_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2_v3 = IND_VALID_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3_v3 = IND_VALID_lead['lead{}'.format(lead3)]\n",
    "VALID_ind4_v3 = IND_VALID_lead['lead{}'.format(lead4)]\n",
    "\n",
    "# ============================================================== #\n",
    "# Load feature vectors (HRRR v3, training)\n",
    "\n",
    "data_lead1_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead2_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead3_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead4_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "TRAIN_lead1_v3 = np.concatenate((data_lead1_p0['y_vector'], data_lead1_p1['y_vector'], data_lead1_p2['y_vector']), axis=0)\n",
    "TRAIN_lead2_v3 = np.concatenate((data_lead2_p0['y_vector'], data_lead2_p1['y_vector'], data_lead2_p2['y_vector']), axis=0)\n",
    "TRAIN_lead3_v3 = np.concatenate((data_lead3_p0['y_vector'], data_lead3_p1['y_vector'], data_lead3_p2['y_vector']), axis=0)\n",
    "TRAIN_lead4_v3 = np.concatenate((data_lead4_p0['y_vector'], data_lead4_p1['y_vector'], data_lead4_p2['y_vector']), axis=0)\n",
    "\n",
    "TRAIN_lead1_y_v3 = np.concatenate((data_lead1_p0['y_true'], data_lead1_p1['y_true'], data_lead1_p2['y_true']), axis=0)\n",
    "TRAIN_lead2_y_v3 = np.concatenate((data_lead2_p0['y_true'], data_lead2_p1['y_true'], data_lead2_p2['y_true']), axis=0)\n",
    "TRAIN_lead3_y_v3 = np.concatenate((data_lead3_p0['y_true'], data_lead3_p1['y_true'], data_lead3_p2['y_true']), axis=0)\n",
    "TRAIN_lead4_y_v3 = np.concatenate((data_lead4_p0['y_true'], data_lead4_p1['y_true'], data_lead4_p2['y_true']), axis=0)\n",
    "\n",
    "# =========================================================== #\n",
    "# Load feature vectors (HRRR v3, validation)\n",
    "\n",
    "data_lead1_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_valid = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "VALID_lead1_v3 = data_lead1_valid['y_vector']\n",
    "VALID_lead2_v3 = data_lead2_valid['y_vector']\n",
    "VALID_lead3_v3 = data_lead3_valid['y_vector']\n",
    "VALID_lead4_v3 = data_lead4_valid['y_vector']\n",
    "\n",
    "VALID_lead1_y_v3 = data_lead1_valid['y_true']\n",
    "VALID_lead2_y_v3 = data_lead2_valid['y_true']\n",
    "VALID_lead3_y_v3 = data_lead3_valid['y_true']\n",
    "VALID_lead4_y_v3 = data_lead4_valid['y_true']\n",
    "\n",
    "# ================================================================= #\n",
    "# Collect feature vectors from all batch files (HRRR v3, validation)\n",
    "\n",
    "L = len(TRAIN_ind2_v3)\n",
    "\n",
    "filename_train1_pick_v3 = []\n",
    "filename_train2_pick_v3 = []\n",
    "filename_train3_pick_v3 = []\n",
    "filename_train4_pick_v3 = []\n",
    "\n",
    "TRAIN_X_lead1 = np.empty((L, 128))\n",
    "TRAIN_X_lead2 = np.empty((L, 128))\n",
    "TRAIN_X_lead3 = np.empty((L, 128))\n",
    "TRAIN_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "TRAIN_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(TRAIN_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(TRAIN_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(TRAIN_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(TRAIN_ind4_v3[i])\n",
    "    \n",
    "    filename_train1_pick_v3.append(filename_train_lead1_v3[ind_lead1_v3])\n",
    "    filename_train2_pick_v3.append(filename_train_lead2_v3[ind_lead2_v3])\n",
    "    filename_train3_pick_v3.append(filename_train_lead3_v3[ind_lead3_v3])\n",
    "    filename_train4_pick_v3.append(filename_train_lead4_v3[ind_lead4_v3])\n",
    "    \n",
    "    TRAIN_X_lead1[i, :] = TRAIN_lead1_v3[ind_lead1_v3, :]\n",
    "    TRAIN_X_lead2[i, :] = TRAIN_lead2_v3[ind_lead2_v3, :]\n",
    "    TRAIN_X_lead3[i, :] = TRAIN_lead3_v3[ind_lead3_v3, :]\n",
    "    TRAIN_X_lead4[i, :] = TRAIN_lead4_v3[ind_lead4_v3, :]\n",
    "    \n",
    "    TRAIN_Y_v3[i] = TRAIN_lead3_y_v3[ind_lead3_v3]\n",
    "    \n",
    "# ================================================================== #\n",
    "# Collect feature vectors from all batch files (HRRR v3, validation)\n",
    "L = len(VALID_ind2_v3)\n",
    "\n",
    "filename_valid1_pick_v3 = []\n",
    "filename_valid2_pick_v3 = []\n",
    "filename_valid3_pick_v3 = []\n",
    "filename_valid4_pick_v3 = []\n",
    "\n",
    "VALID_X_lead1 = np.empty((L, 128))\n",
    "VALID_X_lead2 = np.empty((L, 128))\n",
    "VALID_X_lead3 = np.empty((L, 128))\n",
    "VALID_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "VALID_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(VALID_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(VALID_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(VALID_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(VALID_ind4_v3[i])\n",
    "    \n",
    "    filename_valid1_pick_v3.append(filename_valid_lead1_v3[ind_lead1_v3])\n",
    "    filename_valid2_pick_v3.append(filename_valid_lead2_v3[ind_lead2_v3])\n",
    "    filename_valid3_pick_v3.append(filename_valid_lead3_v3[ind_lead3_v3])\n",
    "    filename_valid4_pick_v3.append(filename_valid_lead4_v3[ind_lead4_v3])\n",
    "    \n",
    "    VALID_X_lead1[i, :] = VALID_lead1_v3[ind_lead1_v3, :]\n",
    "    VALID_X_lead2[i, :] = VALID_lead2_v3[ind_lead2_v3, :]\n",
    "    VALID_X_lead3[i, :] = VALID_lead3_v3[ind_lead3_v3, :]\n",
    "    VALID_X_lead4[i, :] = VALID_lead4_v3[ind_lead4_v3, :]\n",
    "    \n",
    "    VALID_Y_v3[i] = VALID_lead3_y_v3[ind_lead3_v3]\n",
    "\n",
    "# ================================================================== #\n",
    "# extract location information for nearby-grid-cell-based training\n",
    "\n",
    "indx_train, indy_train, days_train, flags_train = name_to_ind(filename_train3_pick_v3)\n",
    "indx_valid, indy_valid, days_valid, flags_valid = name_to_ind(filename_valid3_pick_v3)\n",
    "grid_shape = lon_80km.shape\n",
    "\n",
    "# ============================================= #\n",
    "# Merge feature vectors on multiple lead times\n",
    "\n",
    "N_days_train = np.max(days_train) + 1\n",
    "N_days_valid = (np.max(days_valid) - np.min(days_valid) + 1) + 1\n",
    "\n",
    "ALL_VEC = np.empty((N_days_train+N_days_valid, 4,)+grid_shape+(128,))\n",
    "ALL_VEC[...] = np.nan\n",
    "\n",
    "for i in range(len(indx_train)):\n",
    "    indx_temp = indx_train[i]\n",
    "    indy_temp = indy_train[i]\n",
    "    days_temp = days_train[i]\n",
    "    \n",
    "    ALL_VEC[days_temp, 0, indx_temp, indy_temp, :] = TRAIN_X_lead1[i, :]\n",
    "    ALL_VEC[days_temp, 1, indx_temp, indy_temp, :] = TRAIN_X_lead2[i, :]\n",
    "    ALL_VEC[days_temp, 2, indx_temp, indy_temp, :] = TRAIN_X_lead3[i, :]\n",
    "    ALL_VEC[days_temp, 3, indx_temp, indy_temp, :] = TRAIN_X_lead4[i, :]\n",
    "\n",
    "for i in range(len(indx_valid)):\n",
    "    indx_temp = indx_valid[i]\n",
    "    indy_temp = indy_valid[i]\n",
    "    days_temp = days_valid[i]\n",
    "    \n",
    "    ALL_VEC[days_temp, 0, indx_temp, indy_temp, :] = VALID_X_lead1[i, :]\n",
    "    ALL_VEC[days_temp, 1, indx_temp, indy_temp, :] = VALID_X_lead2[i, :]\n",
    "    ALL_VEC[days_temp, 2, indx_temp, indy_temp, :] = VALID_X_lead3[i, :]\n",
    "    ALL_VEC[days_temp, 3, indx_temp, indy_temp, :] = VALID_X_lead4[i, :]\n",
    "\n",
    "# ================================================#\n",
    "# Combine training and validation set loation info\n",
    "\n",
    "indx = np.concatenate((indx_train, indx_valid), axis=0)\n",
    "indy = np.concatenate((indy_train, indy_valid), axis=0)\n",
    "days = np.concatenate((days_train, days_valid), axis=0)\n",
    "flags = np.concatenate((flags_train, flags_valid), axis=0)\n",
    "\n",
    "# ======================================================== #\n",
    "# Separate pos and neg samples for balanced training\n",
    "\n",
    "TRAIN_Y = np.concatenate((TRAIN_Y_v3, VALID_Y_v3), axis=0)\n",
    "\n",
    "TRAIN_pos_indx = indx[TRAIN_Y==1]\n",
    "TRAIN_pos_indy = indy[TRAIN_Y==1]\n",
    "TRAIN_pos_days = days[TRAIN_Y==1]\n",
    "\n",
    "TRAIN_neg_indx = indx[TRAIN_Y==0]\n",
    "TRAIN_neg_indy = indy[TRAIN_Y==0]\n",
    "TRAIN_neg_days = days[TRAIN_Y==0]\n",
    "\n",
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = feature_extract(filename_train3_pick_v3, \n",
    "                                                                      lon_80km, lon_minmax, \n",
    "                                                                      lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "TRAIN_stn_v3 = np.concatenate((lon_norm_v3[:, None], \n",
    "                               lat_norm_v3[:, None]), axis=1)\n",
    "\n",
    "\n",
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = feature_extract(filename_valid3_pick_v3, \n",
    "                                                                      lon_80km, lon_minmax, \n",
    "                                                                      lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "VALID_stn_v3 = np.concatenate((lon_norm_v3[:, None], \n",
    "                               lat_norm_v3[:, None]), axis=1)\n",
    "\n",
    "ALL_stn = np.concatenate((TRAIN_stn_v3, VALID_stn_v3))\n",
    "\n",
    "TRAIN_stn_pos = ALL_stn[TRAIN_Y==1]\n",
    "TRAIN_stn_neg = ALL_stn[TRAIN_Y==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f12f65-9386-4d6e-b4d0-a8b684d5c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ====================================================== #\n",
    "# HRRR v4x validation set\n",
    "# ====================================================== #\n",
    "# Read batch file names (npy)\n",
    "\n",
    "filename_valid_lead1 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead1)))\n",
    "filename_valid_lead2 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name2_v4_test, lead2)))\n",
    "filename_valid_lead3 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name3_v4_test, lead3)))\n",
    "filename_valid_lead4 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name4_v4_test, lead4)))\n",
    "\n",
    "# =============================== #\n",
    "# Load feature vectors\n",
    "\n",
    "valid_lead1 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "valid_lead2 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "valid_lead3 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "valid_lead4 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "VALID_lead1 = valid_lead1['y_vector']\n",
    "VALID_lead2 = valid_lead2['y_vector']\n",
    "VALID_lead3 = valid_lead3['y_vector']\n",
    "VALID_lead4 = valid_lead4['y_vector']\n",
    "\n",
    "VALID_lead1_y = valid_lead1['y_true']\n",
    "VALID_lead2_y = valid_lead2['y_true']\n",
    "VALID_lead3_y = valid_lead3['y_true']\n",
    "VALID_lead4_y = valid_lead4['y_true']\n",
    "\n",
    "# ============================================================ #\n",
    "# Consistency check indices\n",
    "\n",
    "IND_TEST_lead = np.load('/glade/work/ksha/NCAR/IND_TEST_lead_v4.npy', allow_pickle=True)[()]\n",
    "\n",
    "VALID_ind1 = IND_TEST_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2 = IND_TEST_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3 = IND_TEST_lead['lead{}'.format(lead3)]\n",
    "VALID_ind4 = IND_TEST_lead['lead{}'.format(lead4)]\n",
    "\n",
    "# ================================================================== #\n",
    "# Collect feature vectors from all batch files\n",
    "\n",
    "L = len(VALID_ind2)\n",
    "\n",
    "filename_valid1_pick = []\n",
    "filename_valid2_pick = []\n",
    "filename_valid3_pick = []\n",
    "filename_valid4_pick = []\n",
    "\n",
    "VALID_X_lead1 = np.empty((L, 128))\n",
    "VALID_X_lead2 = np.empty((L, 128))\n",
    "VALID_X_lead3 = np.empty((L, 128))\n",
    "VALID_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "VALID_Y = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1 = int(VALID_ind1[i])\n",
    "    ind_lead2 = int(VALID_ind2[i])\n",
    "    ind_lead3 = int(VALID_ind3[i])\n",
    "    ind_lead4 = int(VALID_ind4[i])\n",
    "    \n",
    "    filename_valid1_pick.append(filename_valid_lead1[ind_lead1])\n",
    "    filename_valid2_pick.append(filename_valid_lead2[ind_lead2])\n",
    "    filename_valid3_pick.append(filename_valid_lead3[ind_lead3])\n",
    "    filename_valid4_pick.append(filename_valid_lead4[ind_lead4])\n",
    "    \n",
    "    VALID_X_lead1[i, :] = VALID_lead1[ind_lead1, :]\n",
    "    VALID_X_lead2[i, :] = VALID_lead2[ind_lead2, :]\n",
    "    VALID_X_lead3[i, :] = VALID_lead3[ind_lead3, :]\n",
    "    VALID_X_lead4[i, :] = VALID_lead4[ind_lead4, :]\n",
    "    \n",
    "    VALID_Y[i] = VALID_lead3_y[ind_lead3]\n",
    "\n",
    "# ================================================================== #\n",
    "# extract location information\n",
    "indx, indy, days, flags = name_to_ind(filename_valid3_pick)\n",
    "\n",
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = feature_extract(filename_valid3_pick, \n",
    "                                                                      lon_80km, lon_minmax, \n",
    "                                                                      lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "VALID_stn_v3 = np.concatenate((lon_norm_v3[:, None], \n",
    "                               lat_norm_v3[:, None]), axis=1)\n",
    "\n",
    "# ================================================================== #\n",
    "# Collect feature vectors from all batch files\n",
    "\n",
    "grid_shape = lon_80km.shape\n",
    "N_days = np.max(days)-np.min(days)+1\n",
    "\n",
    "VALID_VEC = np.empty((N_days, 4,)+grid_shape+(128,))\n",
    "VALID_VEC[...] = np.nan\n",
    "\n",
    "for i in range(len(indx)):\n",
    "    indx_temp = indx[i]\n",
    "    indy_temp = indy[i]\n",
    "    days_temp = days[i]-np.min(days)\n",
    "    \n",
    "    if days_temp <0:\n",
    "        eqrgetwqh\n",
    "    \n",
    "    VALID_VEC[days_temp, 0, indx_temp, indy_temp, :] = VALID_X_lead1[i, :]\n",
    "    VALID_VEC[days_temp, 1, indx_temp, indy_temp, :] = VALID_X_lead2[i, :]\n",
    "    VALID_VEC[days_temp, 2, indx_temp, indy_temp, :] = VALID_X_lead3[i, :]\n",
    "    VALID_VEC[days_temp, 3, indx_temp, indy_temp, :] = VALID_X_lead4[i, :]\n",
    "\n",
    "# ========================================================================= #\n",
    "# Generate v4 validation set with nearby grid cells\n",
    "# ========================================================================= #\n",
    "\n",
    "grid_shape = lon_80km.shape\n",
    "day_min = np.min(days)\n",
    "VALID_Input = np.empty((len(VALID_Y), L_vec, 128))\n",
    "VALID_Input[...] = np.nan\n",
    "\n",
    "VALID_Input_stn = np.empty((len(VALID_Y), 2))\n",
    "\n",
    "for i in range(len(VALID_Y)):\n",
    "    \n",
    "    vec_merge = ()\n",
    "    \n",
    "    indx_temp = indx[i]\n",
    "    indy_temp = indy[i]\n",
    "    days_temp = days[i] - day_min\n",
    "    \n",
    "    indx_left = np.max([indx_temp - 1, 0])\n",
    "    indx_right = np.min([indx_temp + 1, grid_shape[0]])\n",
    "    \n",
    "    indy_bot = np.max([indy_temp - 1, 0])\n",
    "    indy_top = np.min([indy_temp + 1, grid_shape[1]])\n",
    "    \n",
    "    count = 0\n",
    "    for ix in [indx_temp, indx_left, indx_right]:\n",
    "        for iy in [indy_temp, indy_bot, indy_top]:\n",
    "            for il in range(4):\n",
    "                vec_temp = VALID_VEC[days_temp, il, ix, iy, :]\n",
    "                if np.sum(np.isnan(vec_temp)) == 0 and count < L_vec:\n",
    "                    vec_merge += (vec_temp[None, ...],)\n",
    "                    count += 1\n",
    "                    \n",
    "    VEC_merge = np.concatenate(vec_merge, axis=0)\n",
    "    VALID_Input[i, ...] = VEC_merge\n",
    "    VALID_Input_stn[i, ...] = VALID_stn_v3[days_temp, :]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a6cb53b-ca39-4277-89ce-d106fdeda683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 1.1\n",
      "Training round 0\n",
      "Validation loss improved from 1.1 to 0.9968025366865402\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 38.67487668991089 seconds ---\n",
      "Validation loss improved from 0.9968025366865402 to 0.9929829981941929\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 13.013433933258057 seconds ---\n",
      "Validation loss improved from 0.9929829981941929 to 0.9893377514055938\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 13.741854429244995 seconds ---\n",
      "Validation loss 0.9926382231041732 NOT improved\n",
      "Validation loss 0.9906001840354325 NOT improved\n",
      "Validation loss 0.9898613960527176 NOT improved\n",
      "Validation loss improved from 0.9893377514055938 to 0.9839671080372944\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 16.914819955825806 seconds ---\n",
      "Validation loss improved from 0.9839671080372944 to 0.9719337182253603\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 10.421988725662231 seconds ---\n",
      "Validation loss improved from 0.9719337182253603 to 0.9691840467333158\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 11.112192153930664 seconds ---\n",
      "Validation loss improved from 0.9691840467333158 to 0.9577755800931067\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 10.25260305404663 seconds ---\n",
      "Validation loss improved from 0.9577755800931067 to 0.9377046060867766\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 9.997673034667969 seconds ---\n",
      "Validation loss improved from 0.9377046060867766 to 0.9136237684741393\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 10.557398080825806 seconds ---\n",
      "Validation loss 0.9432446824432471 NOT improved\n",
      "Validation loss improved from 0.9136237684741393 to 0.8930471555326939\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 9.88445520401001 seconds ---\n",
      "Validation loss 0.9033305479824907 NOT improved\n",
      "Validation loss improved from 0.8930471555326939 to 0.8863122785469987\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 10.561375617980957 seconds ---\n",
      "Validation loss improved from 0.8863122785469987 to 0.8849109421972863\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 38.27407193183899 seconds ---\n",
      "Validation loss improved from 0.8849109421972863 to 0.8802080165835194\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 11.926106214523315 seconds ---\n",
      "Validation loss 0.8972092700405884 NOT improved\n",
      "Validation loss 0.9537392543125752 NOT improved\n",
      "Validation loss 0.9162990421613774 NOT improved\n",
      "Validation loss 1.1623717745089672 NOT improved\n",
      "Early stopping\n",
      "Training round 1\n",
      "Validation loss 0.9997809647672969 NOT improved\n",
      "Validation loss 0.9996620382605195 NOT improved\n",
      "Validation loss 0.9995338219771267 NOT improved\n",
      "Validation loss 0.9993027971102993 NOT improved\n",
      "Validation loss 0.9990146515724648 NOT improved\n",
      "Validation loss 0.9984495659900583 NOT improved\n",
      "Validation loss 0.9980659536998291 NOT improved\n",
      "Validation loss 0.9979603497392296 NOT improved\n",
      "Validation loss 0.997432678340276 NOT improved\n",
      "Validation loss 0.9971260823405964 NOT improved\n",
      "Validation loss 0.9941490287393158 NOT improved\n",
      "Validation loss 0.9906947743728286 NOT improved\n",
      "Validation loss 0.9858871099808033 NOT improved\n",
      "Validation loss 0.9776855186365331 NOT improved\n",
      "Validation loss 0.9771244199352414 NOT improved\n",
      "Validation loss 0.9578639427453329 NOT improved\n",
      "Validation loss 0.9409694988424954 NOT improved\n",
      "Validation loss 0.9421099342935991 NOT improved\n",
      "Validation loss 0.950471099224123 NOT improved\n",
      "Validation loss 0.9134110476359546 NOT improved\n",
      "Validation loss 0.9075479324291748 NOT improved\n",
      "Validation loss 0.8951889818667638 NOT improved\n",
      "Validation loss 1.0650958081607187 NOT improved\n",
      "Validation loss 1.1936629716117173 NOT improved\n",
      "Early stopping\n",
      "Training round 2\n",
      "Validation loss 0.9997850336486591 NOT improved\n",
      "Validation loss 0.9994480113254883 NOT improved\n",
      "Validation loss 0.9985504772437069 NOT improved\n",
      "Validation loss 0.9975289989492453 NOT improved\n",
      "Validation loss 0.997951664111273 NOT improved\n",
      "Validation loss 0.9976033373730547 NOT improved\n",
      "Validation loss 0.9974727769304251 NOT improved\n",
      "Validation loss 0.9965934510567 NOT improved\n",
      "Validation loss 0.9957717943531847 NOT improved\n",
      "Validation loss 0.9948242702082285 NOT improved\n",
      "Validation loss 0.9931998951098958 NOT improved\n",
      "Validation loss 0.9922995294785584 NOT improved\n",
      "Validation loss 0.9930680942587023 NOT improved\n",
      "Validation loss 0.9867580347693141 NOT improved\n",
      "Validation loss 0.9734962695265883 NOT improved\n",
      "Validation loss 0.9688815030245215 NOT improved\n",
      "Validation loss 0.9540557801379659 NOT improved\n",
      "Validation loss 0.9367355617392517 NOT improved\n",
      "Validation loss 0.9377629324124686 NOT improved\n",
      "Validation loss 0.9058655718089954 NOT improved\n",
      "Validation loss 0.9064008502943716 NOT improved\n",
      "Validation loss 0.8841395853228676 NOT improved\n",
      "Validation loss 0.8869752349965461 NOT improved\n",
      "Validation loss 0.8827357961922978 NOT improved\n",
      "Validation loss improved from 0.8802080165835194 to 0.8748284867790554\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 11.91104793548584 seconds ---\n",
      "Validation loss 0.8822174211139739 NOT improved\n",
      "Validation loss improved from 0.8748284867790554 to 0.8709127980919995\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/peak19_lead21/assets\n",
      "--- 10.148513793945312 seconds ---\n",
      "Validation loss 0.8837076210086003 NOT improved\n",
      "Validation loss 0.896068582486039 NOT improved\n",
      "Validation loss 0.9040368271928163 NOT improved\n",
      "Validation loss 0.8902532546353648 NOT improved\n",
      "Validation loss 0.8994411165729606 NOT improved\n",
      "Validation loss 0.9035382651729056 NOT improved\n",
      "Validation loss 0.8743675606459448 NOT improved\n",
      "Validation loss 0.9859491608729593 NOT improved\n",
      "Validation loss 0.958220242032335 NOT improved\n",
      "Validation loss 1.0039974385687858 NOT improved\n",
      "Validation loss 3.470761284131598 NOT improved\n",
      "Early stopping\n",
      "Training round 3\n",
      "Validation loss 0.9995413984367156 NOT improved\n",
      "Validation loss 0.9990029032442084 NOT improved\n",
      "Validation loss 0.9977075226608318 NOT improved\n",
      "Validation loss 0.996603876993889 NOT improved\n",
      "Validation loss 0.9957695730704748 NOT improved\n",
      "Validation loss 0.9952621470649949 NOT improved\n",
      "Validation loss 0.9943660428601592 NOT improved\n",
      "Validation loss 0.9929011090058457 NOT improved\n",
      "Validation loss 0.9935836756489943 NOT improved\n",
      "Validation loss 0.9900909107255467 NOT improved\n",
      "Validation loss 0.9837829684503917 NOT improved\n",
      "Validation loss 0.9801484621131917 NOT improved\n",
      "Validation loss 0.975949798458634 NOT improved\n",
      "Validation loss 0.938551895594726 NOT improved\n",
      "Validation loss 0.936697835327365 NOT improved\n",
      "Validation loss 0.943580150249788 NOT improved\n",
      "Validation loss 0.8966939109104576 NOT improved\n",
      "Validation loss 0.8986379882318013 NOT improved\n",
      "Validation loss 0.8801269896932552 NOT improved\n",
      "Validation loss 0.8747463296720401 NOT improved\n",
      "Validation loss 0.8726916243175147 NOT improved\n",
      "Validation loss 0.8722785812977967 NOT improved\n",
      "Validation loss 0.8977768902205411 NOT improved\n",
      "Validation loss 0.8985401226408147 NOT improved\n",
      "Validation loss 0.9441935272661676 NOT improved\n",
      "Validation loss 2.094454822529241 NOT improved\n",
      "Early stopping\n",
      "Training round 4\n",
      "Validation loss 0.9995136543630424 NOT improved\n",
      "Validation loss 0.9985078583797429 NOT improved\n",
      "Validation loss 0.9978445151234336 NOT improved\n",
      "Validation loss 0.9976059649880457 NOT improved\n",
      "Validation loss 0.9971054154987733 NOT improved\n",
      "Validation loss 0.9967617840024977 NOT improved\n",
      "Validation loss 0.9939880326217292 NOT improved\n",
      "Validation loss 0.98662534281388 NOT improved\n",
      "Validation loss 0.9670699130748391 NOT improved\n",
      "Validation loss 0.9482733192603057 NOT improved\n",
      "Validation loss 0.9456753992867792 NOT improved\n",
      "Validation loss 0.9210833488366366 NOT improved\n",
      "Validation loss 0.9118112186345764 NOT improved\n",
      "Validation loss 0.906108152948159 NOT improved\n",
      "Validation loss 0.881605255081269 NOT improved\n",
      "Validation loss 0.8800300039892809 NOT improved\n",
      "Validation loss 0.8820472051952419 NOT improved\n",
      "Validation loss 0.8806390837184153 NOT improved\n",
      "Validation loss 0.9789569738766183 NOT improved\n",
      "Validation loss 0.9433272424253472 NOT improved\n",
      "Validation loss 1.026288432399936 NOT improved\n",
      "Validation loss 1.1070360911051458 NOT improved\n",
      "Early stopping\n",
      "Training round 5\n",
      "Validation loss 0.9998950504925763 NOT improved\n",
      "Validation loss 0.9997439073144391 NOT improved\n",
      "Validation loss 0.9994737312694216 NOT improved\n",
      "Validation loss 0.9986046168240056 NOT improved\n",
      "Validation loss 0.9975963884786603 NOT improved\n",
      "Validation loss 0.9964153593722138 NOT improved\n",
      "Validation loss 0.9961787136458701 NOT improved\n",
      "Validation loss 0.9952563038909339 NOT improved\n",
      "Validation loss 0.9945587432616648 NOT improved\n",
      "Validation loss 0.9931284184511195 NOT improved\n",
      "Validation loss 0.9901416242680748 NOT improved\n",
      "Validation loss 0.9912128786607477 NOT improved\n",
      "Validation loss 0.9802164905891101 NOT improved\n",
      "Validation loss 0.9805605108283227 NOT improved\n",
      "Validation loss 0.9649826131695799 NOT improved\n",
      "Validation loss 0.9640649093832576 NOT improved\n",
      "Validation loss 0.9680588983626707 NOT improved\n",
      "Validation loss 0.9437343863884838 NOT improved\n",
      "Validation loss 0.9259751271793233 NOT improved\n",
      "Validation loss 0.9175761828656722 NOT improved\n",
      "Validation loss 0.9018701461663166 NOT improved\n",
      "Validation loss 0.8797880562511995 NOT improved\n",
      "Validation loss 0.8820430061579311 NOT improved\n",
      "Validation loss 0.8796213271466851 NOT improved\n",
      "Validation loss 0.8791752860088529 NOT improved\n",
      "Validation loss 0.8774952132782237 NOT improved\n",
      "Validation loss 0.8784048048333155 NOT improved\n",
      "Validation loss 0.8809083286927065 NOT improved\n",
      "Validation loss 0.8982623969360198 NOT improved\n",
      "Validation loss 0.9626625329255158 NOT improved\n",
      "Validation loss 0.9678684684741029 NOT improved\n",
      "Validation loss 1.034606977069355 NOT improved\n",
      "Validation loss 0.9855124356732304 NOT improved\n",
      "Validation loss 1.0512218862741614 NOT improved\n",
      "Validation loss 1.0234899986808528 NOT improved\n",
      "Validation loss 1.0638958212884282 NOT improved\n",
      "Validation loss 0.9935599194642878 NOT improved\n",
      "Validation loss 1.1697218205587538 NOT improved\n",
      "Early stopping\n",
      "Training round 6\n",
      "Validation loss 0.9994532473429729 NOT improved\n",
      "Validation loss 0.998984681400645 NOT improved\n",
      "Validation loss 0.9985411576213823 NOT improved\n",
      "Validation loss 0.9983194487746677 NOT improved\n",
      "Validation loss 0.9976126509682052 NOT improved\n",
      "Validation loss 0.996880928199762 NOT improved\n",
      "Validation loss 0.9977309704675292 NOT improved\n",
      "Validation loss 0.9952370786420636 NOT improved\n",
      "Validation loss 0.9955933759296473 NOT improved\n",
      "Validation loss 0.9925730919601743 NOT improved\n",
      "Validation loss 0.9902761308067187 NOT improved\n",
      "Validation loss 0.9826121942500764 NOT improved\n",
      "Validation loss 0.981347917226792 NOT improved\n",
      "Validation loss 0.9769381834925488 NOT improved\n",
      "Validation loss 0.9483184713807665 NOT improved\n",
      "Validation loss 0.9076640361009622 NOT improved\n",
      "Validation loss 0.8865915182473016 NOT improved\n",
      "Validation loss 0.8759102551259377 NOT improved\n",
      "Validation loss 0.8759369519179623 NOT improved\n",
      "Validation loss 0.8800941712665086 NOT improved\n",
      "Validation loss 0.8760443703754596 NOT improved\n",
      "Validation loss 0.8857574592697758 NOT improved\n",
      "Validation loss 0.9059271987934365 NOT improved\n",
      "Validation loss 0.8901210845165425 NOT improved\n",
      "Validation loss 1.20929511501792 NOT improved\n",
      "Early stopping\n",
      "Training round 7\n",
      "Validation loss 0.9987743466596809 NOT improved\n",
      "Validation loss 0.9975044183757145 NOT improved\n",
      "Validation loss 0.9971483434995586 NOT improved\n",
      "Validation loss 0.9957416262684411 NOT improved\n",
      "Validation loss 0.9922357265740909 NOT improved\n",
      "Validation loss 0.9892418017772628 NOT improved\n",
      "Validation loss 0.9827010291427501 NOT improved\n",
      "Validation loss 0.9816739859858983 NOT improved\n",
      "Validation loss 0.9298014454004085 NOT improved\n",
      "Validation loss 0.8804529205049995 NOT improved\n",
      "Validation loss 0.879618743595744 NOT improved\n",
      "Validation loss 0.8772814682093734 NOT improved\n",
      "Validation loss 0.8780027594834909 NOT improved\n",
      "Validation loss 0.8779311520764286 NOT improved\n",
      "Validation loss 0.8964464504542465 NOT improved\n",
      "Validation loss 0.8833495902914524 NOT improved\n",
      "Validation loss 0.9808528232458582 NOT improved\n",
      "Validation loss 0.9683364769424414 NOT improved\n",
      "Validation loss 1.0560344042471896 NOT improved\n",
      "Validation loss 0.9792821400422753 NOT improved\n",
      "Validation loss 1.3057435539858553 NOT improved\n",
      "Early stopping\n",
      "Training round 8\n",
      "Validation loss 0.9954301431330854 NOT improved\n",
      "Validation loss 0.9924633846439447 NOT improved\n",
      "Validation loss 0.9932089954701753 NOT improved\n",
      "Validation loss 0.9950499677425986 NOT improved\n",
      "Validation loss 0.9967200025364757 NOT improved\n",
      "Validation loss 0.9957038257342636 NOT improved\n",
      "Validation loss 0.9931662865324073 NOT improved\n",
      "Validation loss 0.9916316239652853 NOT improved\n",
      "Validation loss 0.9916655491761854 NOT improved\n",
      "Validation loss 0.9884449248394451 NOT improved\n",
      "Validation loss 0.9794311535196244 NOT improved\n",
      "Validation loss 0.9808673579108123 NOT improved\n",
      "Validation loss 0.9373726213956874 NOT improved\n",
      "Validation loss 0.9149555173663629 NOT improved\n",
      "Validation loss 0.8964428378937359 NOT improved\n",
      "Validation loss 0.8749890006304536 NOT improved\n",
      "Validation loss 0.8887541315928877 NOT improved\n",
      "Validation loss 0.8897474554142868 NOT improved\n",
      "Validation loss 0.959795316197156 NOT improved\n",
      "Validation loss 0.904794543558088 NOT improved\n",
      "Validation loss 0.9255831213057327 NOT improved\n",
      "Validation loss 0.9522879406184588 NOT improved\n",
      "Validation loss 1.0801833345393845 NOT improved\n",
      "Validation loss 1.138927003991464 NOT improved\n",
      "Early stopping\n",
      "Training round 9\n",
      "Validation loss 0.9996653366816884 NOT improved\n",
      "Validation loss 0.9991555269625645 NOT improved\n",
      "Validation loss 0.9982316361901746 NOT improved\n",
      "Validation loss 0.9970838580400426 NOT improved\n",
      "Validation loss 0.9967475680019314 NOT improved\n",
      "Validation loss 0.9968950870771458 NOT improved\n",
      "Validation loss 0.9970005489251954 NOT improved\n",
      "Validation loss 0.994381948155284 NOT improved\n",
      "Validation loss 0.990849661614162 NOT improved\n",
      "Validation loss 0.9777147371891481 NOT improved\n",
      "Validation loss 0.9836071679722691 NOT improved\n",
      "Validation loss 0.9350593510550911 NOT improved\n",
      "Validation loss 0.8983047824790906 NOT improved\n",
      "Validation loss 0.8758074118136336 NOT improved\n",
      "Validation loss 0.8735821453895726 NOT improved\n",
      "Validation loss 0.8803543410027003 NOT improved\n",
      "Validation loss 0.9571718534250283 NOT improved\n",
      "Validation loss 0.9216363595762597 NOT improved\n",
      "Validation loss 0.9520677731149835 NOT improved\n",
      "Validation loss 0.8866741918259228 NOT improved\n",
      "Validation loss 1.0230138328696803 NOT improved\n",
      "Validation loss 1.0829207048480234 NOT improved\n",
      "Validation loss 1.1238689192277787 NOT improved\n",
      "Early stopping\n",
      "Training round 10\n",
      "Validation loss 0.9993790677509437 NOT improved\n",
      "Validation loss 0.9983405497134304 NOT improved\n",
      "Validation loss 0.9972006893366782 NOT improved\n",
      "Validation loss 0.9960061823454253 NOT improved\n",
      "Validation loss 0.9918282617610634 NOT improved\n",
      "Validation loss 0.9766704546328241 NOT improved\n",
      "Validation loss 0.9766904382961643 NOT improved\n",
      "Validation loss 0.9759104349037148 NOT improved\n",
      "Validation loss 0.9594502626382398 NOT improved\n",
      "Validation loss 0.9638394559672975 NOT improved\n",
      "Validation loss 0.9571021142834972 NOT improved\n",
      "Validation loss 0.9225460118288443 NOT improved\n",
      "Validation loss 0.9061961951789947 NOT improved\n",
      "Validation loss 0.8773878420196929 NOT improved\n",
      "Validation loss 0.877262997180946 NOT improved\n",
      "Validation loss 0.8784131370261742 NOT improved\n",
      "Validation loss 0.9017235062003843 NOT improved\n",
      "Validation loss 0.8950169963167512 NOT improved\n",
      "Validation loss 0.9851968787551781 NOT improved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain_on_batch([X_batch, X_batch_stn], Y_batch);\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# epoch end operations\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mVALID_Input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVALID_Input_stn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m Y_pred[Y_pred\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    180\u001b[0m Y_pred[Y_pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1629\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1628\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1629\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1630\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1631\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:862\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "   \n",
    "# ============================================================================== #\n",
    "# Set randmo seeds\n",
    "\n",
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695, \n",
    "         2463725, 4734, 23234, 884, 2341, 362, 5, 234, 483, 785356, 23425, 3621, \n",
    "         58461, 80968765, 123, 425633, 5646, 67635, 76785, 34214, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "\n",
    "training_rounds = len(seeds)\n",
    "\n",
    "# ============================================== #\n",
    "# Model training\n",
    "ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "grid_shape = lon_80km.shape\n",
    "day_min = np.min(TRAIN_neg_days)\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = '{}_lead{}'.format(model_tag, lead_name)\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(TRAIN_pos_indx)\n",
    "L_neg = len(TRAIN_neg_indx)\n",
    "\n",
    "record = 1.1\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "L_train = 16\n",
    "\n",
    "N_pos = 32\n",
    "N_neg = batch_size - N_pos\n",
    "\n",
    "buffer = 10\n",
    "N_pos += buffer\n",
    "N_neg += buffer\n",
    "\n",
    "N_neg_true = N_neg - buffer\n",
    "N_pos_true = N_pos - buffer\n",
    "\n",
    "NEG_Input = np.empty((N_neg, L_vec, 128))\n",
    "NEG_Input[...] = np.nan\n",
    "\n",
    "POS_Input = np.empty((N_pos, L_vec, 128))\n",
    "POS_Input[...] = np.nan\n",
    "\n",
    "NEG_Input_stn = np.empty((N_neg, 2))\n",
    "POS_Input_stn = np.empty((N_neg, 2))\n",
    "\n",
    "VEC_merge = np.empty((L_vec, 128))\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    if r == 0:\n",
    "        tol = 0\n",
    "    else:\n",
    "        tol = -200\n",
    "\n",
    "    model = create_model()\n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "    \n",
    "    set_seeds(int(seeds[r]))\n",
    "    print('Training round {}'.format(r))\n",
    "\n",
    "    \n",
    "    for i in range(epochs):            \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            \n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "            ind_neg_pick = ind_neg[:N_neg]\n",
    "            ind_pos_pick = ind_pos[:N_pos]\n",
    "            \n",
    "            # ========== #\n",
    "            count_save = 0\n",
    "            for ineg, neg_pick in enumerate(ind_neg_pick):\n",
    "                \n",
    "                count = 0\n",
    "                \n",
    "                vec_merge = ()\n",
    "\n",
    "                indx_temp = TRAIN_neg_indx[neg_pick]\n",
    "                indy_temp = TRAIN_neg_indy[neg_pick]\n",
    "                days_temp = TRAIN_neg_days[neg_pick] - day_min\n",
    "\n",
    "                indx_left = np.max([indx_temp - 1, 0])\n",
    "                indx_right = np.min([indx_temp + 1, grid_shape[0]])\n",
    "\n",
    "                indy_bot = np.max([indy_temp - 1, 0])\n",
    "                indy_top = np.min([indy_temp + 1, grid_shape[1]])\n",
    "                \n",
    "                for ix in [indx_temp, indx_left, indx_right]:\n",
    "                    for iy in [indy_temp, indy_bot, indy_top]:\n",
    "                        for il in range(4):\n",
    "                            vec_temp = ALL_VEC[days_temp, il, ix, iy, :]\n",
    "                            if np.sum(np.isnan(vec_temp)) == 0 and count < L_vec:\n",
    "                                vec_merge += (vec_temp[None, ...],)\n",
    "                                count += 1\n",
    "                if count < L_vec:\n",
    "                    continue\n",
    "                else:\n",
    "                    VEC_merge = np.concatenate(vec_merge, axis=0)\n",
    "                    NEG_Input[count_save, ...] = VEC_merge\n",
    "                    NEG_Input_stn[count_save, ...] = TRAIN_stn_neg[neg_pick]\n",
    "                    count_save += 1\n",
    "                    \n",
    "            count_save = 0\n",
    "            for ipos, pos_pick in enumerate(ind_pos_pick):\n",
    "                count = 0\n",
    "                \n",
    "                vec_merge = ()\n",
    "\n",
    "                indx_temp = TRAIN_pos_indx[pos_pick]\n",
    "                indy_temp = TRAIN_pos_indy[pos_pick]\n",
    "                days_temp = TRAIN_pos_days[pos_pick] - day_min\n",
    "\n",
    "                indx_left = np.max([indx_temp - 1, 0])\n",
    "                indx_right = np.min([indx_temp + 1, grid_shape[0]])\n",
    "\n",
    "                indy_bot = np.max([indy_temp - 1, 0])\n",
    "                indy_top = np.min([indy_temp + 1, grid_shape[1]])\n",
    "                \n",
    "                for ix in [indx_temp, indx_left, indx_right]:\n",
    "                    for iy in [indy_temp, indy_bot, indy_top]:\n",
    "                        for il in range(4):\n",
    "                            vec_temp = ALL_VEC[days_temp, il, ix, iy, :]\n",
    "                            if np.sum(np.isnan(vec_temp)) == 0 and count < L_vec:\n",
    "                                vec_merge += (vec_temp[None, ...],)\n",
    "                                count += 1\n",
    "                if count < L_vec:\n",
    "                    continue\n",
    "                else:\n",
    "                    VEC_merge = np.concatenate(vec_merge, axis=0)\n",
    "                    POS_Input[count_save, ...] = VEC_merge\n",
    "                    POS_Input_stn[count_save, ...] = TRAIN_stn_pos[pos_pick]\n",
    "                    count_save += 1\n",
    "            # ========== #\n",
    "            \n",
    "            X_batch = np.concatenate((NEG_Input[:N_neg_true, ...], POS_Input[:N_pos_true, ...]), axis=0)\n",
    "            X_batch_stn = np.concatenate((NEG_Input_stn[:N_neg_true, ...], POS_Input_stn[:N_pos_true, ...]), axis=0)\n",
    "            \n",
    "            if np.sum(np.isnan(X_batch)) > 0:\n",
    "                continue;\n",
    "            \n",
    "            Y_batch = np.ones([batch_size,])\n",
    "            Y_batch[:N_neg] = 0.0\n",
    "            \n",
    "            # Y_batch = np.zeros([batch_size,])\n",
    "            # Y_batch[:N_neg] = np.random.uniform(low=0.0, high=0.1, size=N_neg)\n",
    "            # Y_batch[N_neg:] = np.random.uniform(low=0.95, high=1.0, size=N_pos)\n",
    "\n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "\n",
    "            X_batch = X_batch[ind_, :]\n",
    "            Y_batch = Y_batch[ind_]\n",
    "            \n",
    "            model.train_on_batch([X_batch, X_batch_stn], Y_batch);\n",
    "\n",
    "        # epoch end operations\n",
    "        Y_pred = model.predict([VALID_Input, VALID_Input_stn])\n",
    "\n",
    "        Y_pred[Y_pred<0] = 0\n",
    "        Y_pred[Y_pred>1] = 1\n",
    "\n",
    "        record_temp = verif_metric(VALID_Y, Y_pred, ref)\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     model.save(model_path_backup)\n",
    "\n",
    "        if (record - record_temp > min_del):\n",
    "            print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "            record = record_temp\n",
    "            tol = 0\n",
    "            \n",
    "            #print('tol: {}'.format(tol))\n",
    "            # save\n",
    "            print('save to: {}'.format(model_path))\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            print('Validation loss {} NOT improved'.format(record_temp))\n",
    "            if record_temp > 1.1:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d536c6-eb59-454c-b16c-68c55176d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================================ #\n",
    "# # Inference\n",
    "    \n",
    "# model = create_model()\n",
    "\n",
    "# model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#               optimizer=keras.optimizers.Adam(lr=0))\n",
    "\n",
    "# W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/{}_lead{}'.format(model_tag, lead_name))\n",
    "# model.set_weights(W_old)\n",
    "        \n",
    "# Y_pred_valid = model.predict([VALID_Input, VALID_Input_stn])\n",
    "\n",
    "# # Save results\n",
    "# save_dict = {}\n",
    "# save_dict['Y_pred_valid'] = Y_pred_valid\n",
    "# save_dict['VALID_Y'] = VALID_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0af48-93b4-4025-8b42-e0eb4f1338f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedeeb71-540a-4ca8-8d4b-5043743ab658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
