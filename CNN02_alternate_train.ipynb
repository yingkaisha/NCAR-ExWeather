{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739d9b65-76f1-4e6c-8c6f-f2ffe86ab015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e5963a-bc31-418e-9859-f1e0a9b8d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 21:45:51.497690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# from keras_unet_collection import models as k_models\n",
    "from keras_unet_collection import utils as k_utils\n",
    "# from keras_unet_collection import layer_utils as k_layers\n",
    "# from keras_unet_collection.activations import GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb39d9d-280c-4377-8a35-fe5d877361d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import graph_utils as gu\n",
    "#import convnext_keras as ck\n",
    "\n",
    "from sklearn.metrics import classification_report, auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21364371-260e-4081-9ed5-9ba9deccec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd86bf98-75d2-4c93-81b9-938ead5699c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_head():\n",
    "    \n",
    "    IN_vec = keras.Input((128,))    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffe8d39-8d0b-49ff-8ad0-84b95e8ba5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred, ref):\n",
    "\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    # AUC = auc(fpr, tpr)\n",
    "    # AUC_metric = 1 - AUC\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    #ll = log_loss(VALID_target.ravel(), Y_pred.ravel())\n",
    "    \n",
    "    #print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric / ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc162f9-2b2b-4da2-84ac-bc302c8aa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead1 = 2\n",
    "lead2 = 3\n",
    "lead3 = 4\n",
    "lead4 = 5\n",
    "lead5 = 6\n",
    "lead6 = 20\n",
    "lead7 = 21\n",
    "lead8 = 22\n",
    "lead9 = 23\n",
    "\n",
    "lead_name = 4\n",
    "model_tag = 're'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e336972a-af84-428b-bf3b-992379eb85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_vec = \"/glade/work/ksha/NCAR/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8c683-754a-496c-aaad-453b8f548439",
   "metadata": {},
   "source": [
    "**Training set (feature vectors)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184a75a3-b012-4696-a440-57e491264991",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lead1_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead2_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead3_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead4_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead5_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead5, model_tag), allow_pickle=True)[()]\n",
    "data_lead5_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead5, model_tag), allow_pickle=True)[()]\n",
    "data_lead5_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead5, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead6_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead6, model_tag), allow_pickle=True)[()]\n",
    "data_lead6_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead6, model_tag), allow_pickle=True)[()]\n",
    "data_lead6_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead6, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead7_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead7, model_tag), allow_pickle=True)[()]\n",
    "data_lead7_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead7, model_tag), allow_pickle=True)[()]\n",
    "data_lead7_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead7, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead8_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead8, model_tag), allow_pickle=True)[()]\n",
    "data_lead8_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead8, model_tag), allow_pickle=True)[()]\n",
    "data_lead8_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead8, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead9_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead9, model_tag), allow_pickle=True)[()]\n",
    "data_lead9_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead9, model_tag), allow_pickle=True)[()]\n",
    "data_lead9_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead9, model_tag), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d40654-7a14-415a-85bd-c4fc32703ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_lead1_v3 = np.concatenate((data_lead1_p0['y_vector'], data_lead1_p1['y_vector'], data_lead1_p2['y_vector']), axis=0)\n",
    "TRAIN_lead2_v3 = np.concatenate((data_lead2_p0['y_vector'], data_lead2_p1['y_vector'], data_lead2_p2['y_vector']), axis=0)\n",
    "TRAIN_lead3_v3 = np.concatenate((data_lead3_p0['y_vector'], data_lead3_p1['y_vector'], data_lead3_p2['y_vector']), axis=0)\n",
    "TRAIN_lead4_v3 = np.concatenate((data_lead4_p0['y_vector'], data_lead4_p1['y_vector'], data_lead4_p2['y_vector']), axis=0)\n",
    "TRAIN_lead5_v3 = np.concatenate((data_lead5_p0['y_vector'], data_lead5_p1['y_vector'], data_lead5_p2['y_vector']), axis=0)\n",
    "TRAIN_lead6_v3 = np.concatenate((data_lead6_p0['y_vector'], data_lead6_p1['y_vector'], data_lead6_p2['y_vector']), axis=0)\n",
    "TRAIN_lead7_v3 = np.concatenate((data_lead7_p0['y_vector'], data_lead7_p1['y_vector'], data_lead7_p2['y_vector']), axis=0)\n",
    "TRAIN_lead8_v3 = np.concatenate((data_lead8_p0['y_vector'], data_lead8_p1['y_vector'], data_lead8_p2['y_vector']), axis=0)\n",
    "TRAIN_lead9_v3 = np.concatenate((data_lead9_p0['y_vector'], data_lead9_p1['y_vector'], data_lead9_p2['y_vector']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd3609b-66df-4cee-ae02-7df83bfda2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_lead1_y_v3 = np.concatenate((data_lead1_p0['y_true'], data_lead1_p1['y_true'], data_lead1_p2['y_true']), axis=0)\n",
    "TRAIN_lead2_y_v3 = np.concatenate((data_lead2_p0['y_true'], data_lead2_p1['y_true'], data_lead2_p2['y_true']), axis=0)\n",
    "TRAIN_lead3_y_v3 = np.concatenate((data_lead3_p0['y_true'], data_lead3_p1['y_true'], data_lead3_p2['y_true']), axis=0)\n",
    "TRAIN_lead4_y_v3 = np.concatenate((data_lead4_p0['y_true'], data_lead4_p1['y_true'], data_lead4_p2['y_true']), axis=0)\n",
    "TRAIN_lead5_y_v3 = np.concatenate((data_lead5_p0['y_true'], data_lead5_p1['y_true'], data_lead5_p2['y_true']), axis=0)\n",
    "TRAIN_lead6_y_v3 = np.concatenate((data_lead6_p0['y_true'], data_lead6_p1['y_true'], data_lead6_p2['y_true']), axis=0)\n",
    "TRAIN_lead7_y_v3 = np.concatenate((data_lead7_p0['y_true'], data_lead7_p1['y_true'], data_lead7_p2['y_true']), axis=0)\n",
    "TRAIN_lead8_y_v3 = np.concatenate((data_lead8_p0['y_true'], data_lead8_p1['y_true'], data_lead8_p2['y_true']), axis=0)\n",
    "TRAIN_lead9_y_v3 = np.concatenate((data_lead9_p0['y_true'], data_lead9_p1['y_true'], data_lead9_p2['y_true']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45522a00-0973-4a10-847c-e8697ee4fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lead1_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead5_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead5, model_tag), allow_pickle=True)[()]\n",
    "data_lead6_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead6, model_tag), allow_pickle=True)[()]\n",
    "data_lead7_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead7, model_tag), allow_pickle=True)[()]\n",
    "data_lead8_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead8, model_tag), allow_pickle=True)[()]\n",
    "data_lead9_px = np.load('{}TRAIN_v4x_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead9, model_tag), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f3b7a0c-3d71-4528-967a-1e3ea3e573e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_X = np.concatenate((TRAIN_lead1_v3, TRAIN_lead2_v3, TRAIN_lead3_v3, \n",
    "                          TRAIN_lead4_v3, TRAIN_lead5_v3, TRAIN_lead6_v3, \n",
    "                          TRAIN_lead7_v3, TRAIN_lead8_v3, TRAIN_lead9_v3,\n",
    "                          data_lead1_px['y_vector'], data_lead2_px['y_vector'],\n",
    "                          data_lead3_px['y_vector'], data_lead4_px['y_vector'],\n",
    "                          data_lead5_px['y_vector'], data_lead6_px['y_vector'],\n",
    "                          data_lead7_px['y_vector'], data_lead8_px['y_vector'],\n",
    "                          data_lead9_px['y_vector']), axis=0)\n",
    "\n",
    "\n",
    "TRAIN_Y = np.concatenate((TRAIN_lead1_y_v3, TRAIN_lead2_y_v3, TRAIN_lead3_y_v3, \n",
    "                          TRAIN_lead4_y_v3, TRAIN_lead5_y_v3, TRAIN_lead6_y_v3, \n",
    "                          TRAIN_lead7_y_v3, TRAIN_lead8_y_v3, TRAIN_lead9_y_v3,\n",
    "                          data_lead1_px['y_true'], data_lead2_px['y_true'],\n",
    "                          data_lead3_px['y_true'], data_lead4_px['y_true'],\n",
    "                          data_lead5_px['y_true'], data_lead6_px['y_true'],\n",
    "                          data_lead7_px['y_true'], data_lead8_px['y_true'],\n",
    "                          data_lead9_px['y_true']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57707066-a1a9-4954-8135-dc0dbfb50640",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_X_pos = TRAIN_X[TRAIN_Y == 1]\n",
    "TRAIN_X_neg = TRAIN_X[TRAIN_Y == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172f190-e42a-4bca-8010-4e864e2b4999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19232302-4c40-4009-b0c0-b2e907296a81",
   "metadata": {},
   "source": [
    "**Validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5f182f8-388c-444e-90bf-8bf2776df67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lead1 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "valid_lead2 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "valid_lead3 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "valid_lead4 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "valid_lead5 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead5, model_tag), allow_pickle=True)[()]\n",
    "valid_lead6 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead6, model_tag), allow_pickle=True)[()]\n",
    "valid_lead7 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead7, model_tag), allow_pickle=True)[()]\n",
    "valid_lead8 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead8, model_tag), allow_pickle=True)[()]\n",
    "valid_lead9 = np.load('{}VALID_v3_vec_lead{}_{}.npy'.format(filepath_vec, lead9, model_tag), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "295a430c-f029-4c06-a6db-006a1347b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lead1_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "valid_lead2_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "valid_lead3_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "valid_lead4_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "valid_lead5_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead5, model_tag), allow_pickle=True)[()]\n",
    "valid_lead6_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead6, model_tag), allow_pickle=True)[()]\n",
    "valid_lead7_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead7, model_tag), allow_pickle=True)[()]\n",
    "valid_lead8_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead8, model_tag), allow_pickle=True)[()]\n",
    "valid_lead9_px = np.load('{}VALID_v4x_vec_lead{}_{}.npy'.format(filepath_vec, lead9, model_tag), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63ba607c-4b73-47b8-bdb0-52dc56257ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_X = np.concatenate((valid_lead1['y_vector'], valid_lead2['y_vector'],\n",
    "                          valid_lead3['y_vector'], valid_lead4['y_vector'],\n",
    "                          valid_lead5['y_vector'], valid_lead6['y_vector'],\n",
    "                          valid_lead7['y_vector'], valid_lead8['y_vector'],\n",
    "                          valid_lead9['y_vector'], valid_lead1_px['y_vector'], \n",
    "                          valid_lead2_px['y_vector'], valid_lead3_px['y_vector'], \n",
    "                          valid_lead4_px['y_vector'], valid_lead5_px['y_vector'], \n",
    "                          valid_lead6_px['y_vector'], valid_lead7_px['y_vector'],\n",
    "                          valid_lead8_px['y_vector'], valid_lead9_px['y_vector']), axis=0)\n",
    "\n",
    "VALID_Y = np.concatenate((valid_lead1['y_true'], valid_lead2['y_true'],\n",
    "                          valid_lead3['y_true'], valid_lead4['y_true'],\n",
    "                          valid_lead5['y_true'], valid_lead6['y_true'],\n",
    "                          valid_lead7['y_true'], valid_lead8['y_true'],\n",
    "                          valid_lead9['y_true'], valid_lead1_px['y_true'], \n",
    "                          valid_lead2_px['y_true'], valid_lead3_px['y_true'], \n",
    "                          valid_lead4_px['y_true'], valid_lead5_px['y_true'], \n",
    "                          valid_lead6_px['y_true'], valid_lead7_px['y_true'],\n",
    "                          valid_lead8_px['y_true'], valid_lead9_px['y_true']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3bdbf30-6db1-4aaf-9a30-af06d16a2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID_X = np.concatenate((valid_lead1['y_vector'], valid_lead2['y_vector'],\n",
    "#                           valid_lead3['y_vector'], valid_lead4['y_vector'],\n",
    "#                           valid_lead5['y_vector'], valid_lead6['y_vector'],\n",
    "#                           valid_lead7['y_vector'], valid_lead8['y_vector'],\n",
    "#                           valid_lead9['y_vector'],), axis=0)\n",
    "\n",
    "# VALID_Y = np.concatenate((valid_lead1['y_true'], valid_lead2['y_true'],\n",
    "#                           valid_lead3['y_true'], valid_lead4['y_true'],\n",
    "#                           valid_lead5['y_true'], valid_lead6['y_true'],\n",
    "#                           valid_lead7['y_true'], valid_lead8['y_true'],\n",
    "#                           valid_lead9['y_true'],), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c64731-ef68-4b14-a1cc-ccb05505eea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4624972, 128)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALID_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd6cc65-e316-45aa-9194-10880492f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 21:46:22.042037: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-06 21:46:22.278929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-06 21:46:22.354861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-06 21:46:22.356483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-06 21:46:22.440275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-06 21:46:22.441486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-06 21:46:22.514133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-06 21:46:22.559885: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-06 21:46:22.719390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-06 21:46:22.721582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-06 21:46:22.766325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-06 21:46:22.767567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-06 21:46:22.781415: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-06 21:46:22.781678: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-06 21:46:22.782143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-06 21:46:22.782372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-06 21:46:22.782631: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-06 21:46:22.782649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-06 21:46:22.782660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-06 21:46:22.782676: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-06 21:46:22.782687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-06 21:46:22.782698: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-06 21:46:22.782765: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-06 21:46:22.783348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-06 21:46:22.783391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-06 21:46:25.428877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-06 21:46:25.428918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-02-06 21:46:25.428932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-02-06 21:46:25.431113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:61:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "model_head = create_model_head()\n",
    "# W_new = model_head.get_weights()\n",
    "\n",
    "W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_{}{}/'.format('base', 3))\n",
    "\n",
    "W_new = W_old[-8:]\n",
    "\n",
    "model_head.set_weights(W_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "426ec144-cf12-4c7c-b687-bdbb931a1e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n",
      "2023-02-06 21:46:55.865568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-02-06 21:46:55.870156: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2600000000 Hz\n",
      "2023-02-06 21:46:56.013241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model_head.predict([VALID_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "799fdd90-6e0d-4954-8216-41b26431b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred):\n",
    "\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    # AUC = auc(fpr, tpr)\n",
    "    # AUC_metric = 1 - AUC\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    #ll = log_loss(VALID_target.ravel(), Y_pred.ravel())\n",
    "    \n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a254650c-a640-4929-982f-d824ae41b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013216430070799631\n"
     ]
    }
   ],
   "source": [
    "#ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "record_temp = verif_metric(VALID_Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "381cd368-65c3-492b-807f-5819f461b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_start = record_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc871677-bf41-42cc-a9e4-cb1b57723e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695, \n",
    "         2463725, 4734, 23234, 884, 2341, 362, 5, 234, 483, 785356, 23425, 3621, \n",
    "         58461, 80968765, 123, 425633, 5646, 67635, 76785, 34214]\n",
    "\n",
    "training_rounds = len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723cdc9-51c7-4ab1-ac02-b4590417c479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c44327b2-6472-42cc-bd3c-602ba86701c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 0.013216430070799631\n",
      "Training round 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "verif_metric() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m Y_pred[Y_pred\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     84\u001b[0m Y_pred[Y_pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 86\u001b[0m record_temp \u001b[38;5;241m=\u001b[39m \u001b[43mverif_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVALID_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# if i % 10 == 0:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#     model.save(model_path_backup)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (record \u001b[38;5;241m-\u001b[39m record_temp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_del):\n",
      "\u001b[0;31mTypeError\u001b[0m: verif_metric() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = 'HEAD_base2'\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(TRAIN_X_pos)\n",
    "L_neg = len(TRAIN_X_neg)\n",
    "\n",
    "record = record_start\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 16\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    if r == 0:\n",
    "        tol = 0\n",
    "    else:\n",
    "        tol = -200\n",
    "\n",
    "    model_head = create_model_head()\n",
    "    # W_new = model_head.get_weights()\n",
    "\n",
    "    W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_peak_{}{}/'.format('base', 3))\n",
    "\n",
    "    W_new = W_old[-8:]\n",
    "\n",
    "    model_head.set_weights(W_new)\n",
    "    \n",
    "    model_head.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "    \n",
    "    set_seeds(int(seeds[r]))\n",
    "    print('Training round {}'.format(r))\n",
    "\n",
    "    for i in range(epochs):            \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            N_pos = 20\n",
    "            N_neg = batch_size - N_pos\n",
    "\n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "            ind_neg_pick = ind_neg[:N_neg]\n",
    "            ind_pos_pick = ind_pos[:N_pos]\n",
    "\n",
    "            X_batch_neg = TRAIN_X_neg[ind_neg_pick, :]\n",
    "            X_batch_pos = TRAIN_X_pos[ind_pos_pick, :]\n",
    "            X_batch = np.concatenate((X_batch_neg, X_batch_pos), axis=0)\n",
    "            \n",
    "            Y_batch = np.ones([batch_size,])\n",
    "            Y_batch[:N_neg] = 0.0\n",
    "\n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "\n",
    "            X_batch = X_batch[ind_, :]\n",
    "            Y_batch = Y_batch[ind_]\n",
    "\n",
    "            # train on batch\n",
    "            model_head.train_on_batch(X_batch, Y_batch);\n",
    "            \n",
    "        if i > 8:\n",
    "            # epoch end operations\n",
    "            Y_pred = model_head.predict(VALID_X)\n",
    "\n",
    "            Y_pred[Y_pred<0] = 0\n",
    "            Y_pred[Y_pred>1] = 1\n",
    "\n",
    "            record_temp = verif_metric(VALID_Y, Y_pred, ref)\n",
    "\n",
    "            # if i % 10 == 0:\n",
    "            #     model.save(model_path_backup)\n",
    "\n",
    "            if (record - record_temp >= min_del):\n",
    "                print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "                record = record_temp\n",
    "                tol = 0\n",
    "\n",
    "                #print('tol: {}'.format(tol))\n",
    "                # save\n",
    "                print('save to: {}'.format(model_path))\n",
    "                model_head.save(model_path)\n",
    "            else:\n",
    "                print('Validation loss {} NOT improved'.format(record_temp))\n",
    "                if record_temp > 1.0:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    tol += 1\n",
    "                    if tol >= max_tol:\n",
    "                        print('Early stopping')\n",
    "                        break;\n",
    "                    else:\n",
    "                        continue;\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d227-11df-4ab2-b3e6-399d1c620446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872fe44-4992-44df-8b73-efd3f8ae2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590427b-a292-43d2-bd5e-0dc3862c7971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62389b-486a-40a6-96f3-a8034ee8cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
