{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8cf1bf-e4cb-4507-81db-babea061b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:09:49.955593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import keras_unet_collection\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcce49e-85db-4e4d-ace7-5925510773ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2e710d-0cd6-451c-96ba-08179f7366a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def create_model(input_shape=(32, 32, 15)):\n",
    "\n",
    "    channels = [32, 64, 96, 128]\n",
    "\n",
    "    Input_shape=(32, 32, 15)\n",
    "    IN = layers.Input(shape=Input_shape)\n",
    "\n",
    "    X = IN\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[0], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[0], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    # pooling\n",
    "    X = keras.layers.Conv2D(channels[1], kernel_size=2, strides=(2, 2), padding='valid', use_bias=True)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[1], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[1], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    # pooling\n",
    "    X = keras.layers.Conv2D(channels[2], kernel_size=2, strides=(2, 2), padding='valid', use_bias=True)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[2], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[2], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    # pooling\n",
    "    X = keras.layers.Conv2D(channels[3], kernel_size=2, strides=(2, 2), padding='valid', use_bias=True)(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[3], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    X = keras.layers.Conv2D(channels[3], kernel_size=3, padding='same', use_bias=False)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "\n",
    "    V1 = X\n",
    "    OUT = keras.layers.GlobalMaxPooling2D()(V1)\n",
    "    model = Model(inputs=IN, outputs=OUT)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_head():\n",
    "\n",
    "    \n",
    "    IN_vec = keras.Input((128,))    \n",
    "    X = IN_vec\n",
    "    #\n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN_vec, outputs=OUT)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def verif_metric(VALID_target, Y_pred):\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    \n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric\n",
    "\n",
    "def name_extract(filenames):\n",
    "    \n",
    "    date_base = datetime(2020, 7, 14)\n",
    "    date_base2 = datetime(2021, 1, 1)\n",
    "    \n",
    "    filename_train = []\n",
    "    filename_valid = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+365+30)]\n",
    "    \n",
    "    base_ref = datetime(2019, 10, 1)\n",
    "    date_list_v4x = [base_ref + timedelta(days=day) for day in range(429)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4x' in name:\n",
    "            date_list = date_list_v4x\n",
    "        elif 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        \n",
    "        if (day - date_base).days < 0:\n",
    "            filename_train.append(name)\n",
    "            \n",
    "        else:\n",
    "            if (day - date_base2).days < 0:\n",
    "                filename_valid.append(name)\n",
    "\n",
    "        \n",
    "    return filename_train, filename_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b495138a-6ebd-4be1-a1b8-f1d3de3a562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3, lead2, pos: 5510, neg: 887822\n",
      "v3, lead3, pos: 4297, neg: 889035\n",
      "v3, lead4, pos: 3242, neg: 890090\n",
      "v3, lead5, pos: 2565, neg: 891795\n",
      "v3, lead6, pos: 2127, neg: 891205\n",
      "v3, lead20, pos: 5407, neg: 887925\n",
      "v3, lead21, pos: 6711, neg: 887649\n",
      "v3, lead22, pos: 7613, neg: 886747\n",
      "v3, lead23, pos: 8009, neg: 886351\n",
      "v4x, lead2, pos: 1995, neg: 348553\n",
      "v4x, lead3, pos: 1618, neg: 349958\n",
      "v4x, lead4, pos: 1218, neg: 350358\n",
      "v4x, lead5, pos: 962, neg: 350614\n",
      "v4x, lead6, pos: 817, neg: 349731\n",
      "v4x, lead20, pos: 1777, neg: 344659\n",
      "v4x, lead21, pos: 2209, neg: 344227\n",
      "v4x, lead22, pos: 2511, neg: 343925\n",
      "v4x, lead23, pos: 2690, neg: 343746\n",
      "v4, lead2, pos: 2174, neg: 402858\n",
      "v4, lead3, pos: 1724, neg: 403308\n",
      "v4, lead4, pos: 1349, neg: 403683\n",
      "v4, lead5, pos: 1034, neg: 403998\n",
      "v4, lead6, pos: 795, neg: 403209\n",
      "v4, lead20, pos: 1941, neg: 403091\n",
      "v4, lead21, pos: 2446, neg: 402586\n",
      "v4, lead22, pos: 2791, neg: 402241\n",
      "v4, lead23, pos: 2903, neg: 402129\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "L_vars = len(ind_pick_from_batch)\n",
    "\n",
    "vers = ['v3', 'v4x', 'v4']\n",
    "leads = [2, 3, 4, 5, 6, 20, 21, 22, 23]\n",
    "filenames_pos = {}\n",
    "filenames_neg = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        if ver == 'v3' and lead < 23:\n",
    "            path_ = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "        elif ver == 'v3' and lead == 23:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "        elif ver == 'v4':\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "        else:\n",
    "            path_ = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "            \n",
    "        filenames_pos['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*pos*lead{}.npy\".format(path_, lead)))\n",
    "        filenames_neg['{}_lead{}'.format(ver, lead)] = sorted(glob(\"{}*neg_neg_neg*lead{}.npy\".format(path_, lead)))\n",
    "        \n",
    "        print('{}, lead{}, pos: {}, neg: {}'.format(ver, lead, \n",
    "                                                    len(filenames_pos['{}_lead{}'.format(ver, lead)]), \n",
    "                                                    len(filenames_neg['{}_lead{}'.format(ver, lead)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1e4d3a-847f-4908-8492-0367ee007ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos train: 4856 pos valid: 654 neg train: 742500 neg valid 145322\n",
      "pos train: 3787 pos valid: 510 neg train: 743569 neg valid 145466\n",
      "pos train: 2874 pos valid: 368 neg train: 744482 neg valid 145608\n",
      "pos train: 2289 pos valid: 276 neg train: 746095 neg valid 145700\n",
      "pos train: 1885 pos valid: 242 neg train: 745471 neg valid 145734\n",
      "pos train: 4632 pos valid: 775 neg train: 742724 neg valid 145201\n",
      "pos train: 5774 pos valid: 937 neg train: 742610 neg valid 145039\n",
      "pos train: 6589 pos valid: 1024 neg train: 741795 neg valid 144952\n",
      "pos train: 6952 pos valid: 1057 neg train: 741432 neg valid 144919\n",
      "pos train: 1452 pos valid: 543 neg train: 225736 neg valid 122817\n",
      "pos train: 1187 pos valid: 431 neg train: 226001 neg valid 123957\n",
      "pos train: 898 pos valid: 320 neg train: 226290 neg valid 124068\n",
      "pos train: 721 pos valid: 241 neg train: 226467 neg valid 124147\n",
      "pos train: 598 pos valid: 219 neg train: 225562 neg valid 124169\n",
      "pos train: 1136 pos valid: 641 neg train: 220912 neg valid 123747\n",
      "pos train: 1426 pos valid: 783 neg train: 220622 neg valid 123605\n",
      "pos train: 1656 pos valid: 855 neg train: 220392 neg valid 123533\n",
      "pos train: 1800 pos valid: 890 neg train: 220248 neg valid 123498\n",
      "pos train: 0 pos valid: 13 neg train: 0 neg valid 29799\n",
      "pos train: 0 pos valid: 10 neg train: 0 neg valid 29802\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 8 neg train: 0 neg valid 29804\n",
      "pos train: 0 pos valid: 7 neg train: 0 neg valid 29805\n",
      "pos train: 0 pos valid: 7 neg train: 0 neg valid 29805\n",
      "pos train: 0 pos valid: 11 neg train: 0 neg valid 29801\n",
      "pos train: 0 pos valid: 10 neg train: 0 neg valid 29802\n"
     ]
    }
   ],
   "source": [
    "filenames_pos_train = {}\n",
    "filenames_neg_train = {}\n",
    "\n",
    "filenames_pos_valid = {}\n",
    "filenames_neg_valid = {}\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        temp_namelist_pos = filenames_pos['{}_lead{}'.format(ver, lead)]\n",
    "        temp_namelist_neg = filenames_neg['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "        pos_train, pos_valid = name_extract(temp_namelist_pos)\n",
    "        neg_train, neg_valid = name_extract(temp_namelist_neg)\n",
    "        \n",
    "        print('pos train: {} pos valid: {} neg train: {} neg valid {}'.format(len(pos_train), \n",
    "                                                                              len(pos_valid), \n",
    "                                                                              len(neg_train), \n",
    "                                                                              len(neg_valid)))\n",
    "        \n",
    "        filenames_pos_train['{}_lead{}'.format(ver, lead)] = pos_train\n",
    "        filenames_neg_train['{}_lead{}'.format(ver, lead)] = neg_train\n",
    "        \n",
    "        filenames_pos_valid['{}_lead{}'.format(ver, lead)] = pos_valid\n",
    "        filenames_neg_valid['{}_lead{}'.format(ver, lead)] = neg_valid\n",
    "        \n",
    "pos_train_all = []\n",
    "neg_train_all = []\n",
    "pos_valid_all = []\n",
    "neg_valid_all = []\n",
    "\n",
    "for ver in vers:\n",
    "    for lead in leads:\n",
    "        pos_train_all += filenames_pos_train['{}_lead{}'.format(ver, lead)]\n",
    "        neg_train_all += filenames_neg_train['{}_lead{}'.format(ver, lead)]\n",
    "        pos_valid_all += filenames_pos_valid['{}_lead{}'.format(ver, lead)]\n",
    "        neg_valid_all += filenames_neg_valid['{}_lead{}'.format(ver, lead)]\n",
    "        \n",
    "save_dir = '/glade/work/ksha/NCAR/'\n",
    "\n",
    "with h5py.File(save_dir+'CNN_Validation.hdf', 'r') as h5io:\n",
    "    VALID_input_32 = h5io['VALID_input_64'][:, 16:-16, 16:-16, ...]\n",
    "    VALID_target = h5io['VALID_target'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88952a89-03ca-48c3-827c-ec3d605d2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 13:21:43.980858: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-23 13:21:43.982358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-23 13:21:44.038088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:8a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-23 13:21:44.038121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-23 13:21:44.103672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-23 13:21:44.103706: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-23 13:21:44.151090: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-23 13:21:44.172062: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-23 13:21:44.208343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-23 13:21:44.265387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-23 13:21:44.324033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-23 13:21:44.324652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-23 13:21:44.325107: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 13:21:44.325345: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-23 13:21:44.325719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:8a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-23 13:21:44.325738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-23 13:21:44.325752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-23 13:21:44.325762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-23 13:21:44.325771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-23 13:21:44.325780: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-23 13:21:44.325789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-23 13:21:44.325798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-23 13:21:44.325806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-23 13:21:44.326297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-23 13:21:44.326320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-23 13:21:44.858937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-23 13:21:44.858978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-02-23 13:21:44.858989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-02-23 13:21:44.859971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "flag_train = 'base'\n",
    "\n",
    "if flag_train == 'head':\n",
    "    flag_weights = 'base'\n",
    "else:\n",
    "    flag_weights = 'head'\n",
    "    \n",
    "model_head = create_model_head()\n",
    "model_base = create_model(input_shape=(32, 32, 15))\n",
    "\n",
    "IN = layers.Input(shape=(32, 32, 15))\n",
    "\n",
    "VEC = model_base(IN)\n",
    "OUT = model_head(VEC)\n",
    "\n",
    "model_final = Model(inputs=IN, outputs=OUT)\n",
    "model_final.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d038858-5e99-4f1e-b69d-de35d0dc2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/RE2_vgg_base{}/'.format(2))\n",
    "model_final.set_weights(W_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af96e00b-dd52-4853-9c34-8ea3e57cdbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n",
      "2023-02-23 13:36:17.002028: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-02-23 13:36:17.002463: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2600000000 Hz\n",
      "2023-02-23 13:36:17.019329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-23 13:36:18.332716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025294339000915635\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model_final.predict([VALID_input_32])\n",
    "record_temp = verif_metric(VALID_target, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3c1ad14-4c3b-4fcb-83db-d7fbe81783e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_round = 1\n",
    "seeds = 711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae5b40c2-b2a1-40e0-9030-5ff48887fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 0.04038640111228885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039531235440255545\n",
      "Validation loss improved from 0.04038640111228885 to 0.039531235440255545\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1/assets\n",
      "--- 317.5968677997589 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03649287790712893\n",
      "Validation loss improved from 0.039531235440255545 to 0.03649287790712893\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1/assets\n",
      "--- 313.91480016708374 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03064535618095853\n",
      "Validation loss improved from 0.03649287790712893 to 0.03064535618095853\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1/assets\n",
      "--- 386.40570640563965 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03033785999633634\n",
      "Validation loss improved from 0.03064535618095853 to 0.03033785999633634\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/RE2_vgg_base1/assets\n",
      "--- 466.7732915878296 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ksha/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039937552980324496\n",
      "Validation loss 0.039937552980324496 NOT improved\n",
      "0.03831762728701112\n",
      "Validation loss 0.03831762728701112 NOT improved\n",
      "0.04535715821299042\n",
      "Validation loss 0.04535715821299042 NOT improved\n",
      "0.03171125491545084\n",
      "Validation loss 0.03171125491545084 NOT improved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     sregwet\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 63\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_pick\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ind_pick_from_batch):\n\u001b[1;32m     66\u001b[0m         temp \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m, c] \n",
      "File \u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py:424\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    422\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    423\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 424\u001b[0m magic \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    427\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 64 #int(len(TRAIN_Y_pick) / batch_size)\n",
    "\n",
    "X_batch_32 = np.empty((batch_size, 32, 32, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "\n",
    "X_batch_32[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "key = 'RE2_vgg_{}{}'.format(flag_train, save_round)\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "filename_pos_train = pos_train_all\n",
    "filename_neg_train = neg_train_all\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = record_temp #0.01840167896363949 #record_temp\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "set_seeds(seeds)\n",
    "    \n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        if flag_train == 'base':\n",
    "            N_pos = 20\n",
    "        else:\n",
    "            N_pos = 20\n",
    "            \n",
    "        N_neg = batch_size - N_pos\n",
    "\n",
    "        ind_neg = du.shuffle_ind(L_neg)\n",
    "        ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "        file_pick_neg = []\n",
    "        for ind_temp in ind_neg[:N_neg]:\n",
    "            file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "\n",
    "        file_pick_pos = []\n",
    "        for ind_temp in ind_pos[:N_pos]:\n",
    "            file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "\n",
    "        file_pick = file_pick_neg + file_pick_pos\n",
    "\n",
    "        if len(file_pick) != batch_size:\n",
    "            sregwet\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            data = np.load(file_pick[k])\n",
    "\n",
    "            for l, c in enumerate(ind_pick_from_batch):\n",
    "                temp = data[:, 16:-16, 16:-16, c] \n",
    "                X_batch_32[k, ..., l] = temp\n",
    "\n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "            elif 'neg_neg_neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "\n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch_32 = X_batch_32[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "\n",
    "        # train on batch\n",
    "        model_final.train_on_batch(X_batch_32, Y_batch);\n",
    "\n",
    "    # epoch end operations\n",
    "    Y_pred = model_final.predict([VALID_input_32])\n",
    "    # Y_pred[Y_pred<0] = 0\n",
    "    # Y_pred[Y_pred>1] = 1\n",
    "\n",
    "    record_temp = verif_metric(VALID_target, Y_pred)\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     model.save(model_path_backup)\n",
    "\n",
    "    if (record - record_temp > min_del):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        #print('tol: {}'.format(tol))\n",
    "        # save\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model_final.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        if record_temp >= 2.0:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            tol += 1\n",
    "            if tol >= max_tol:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7095257-0eb4-4612-a20a-5fee189fc440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522ebff-4483-48d0-bd26-65ca783f57ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663c633-d7d5-4f77-9d1a-f1509b7e086c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829e930-e09f-4384-b3bf-7863cb6cf975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cebb89-2334-4ce2-92cd-c0a6f2bb5e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96d044-409c-4e33-a800-1a9d30cb40ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2fd30-4f62-46b7-b61f-39856a189bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3ffbe-295d-43e3-9f3f-f9193dc7e128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db707c1-81df-47fa-a073-f22f16ea9607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4dfa8-8f26-4a5c-a382-a9c86b3ed927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb722a8-a3df-4cd3-8586-c04f14007baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c45129-783a-467c-8b5b-68f3df970f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61deef6-5f1e-4447-8829-5c9dc7276cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d9ecc-e8c1-4e24-943c-f82fc6a66a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb930aae-b818-4f32-b6bd-e2089828e068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00721ee-ac30-4180-8bd1-454b394333f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b5ce1-b57d-49a0-8c24-828e390ff692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce3f73-b3fe-4048-aa6a-179a578add5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
