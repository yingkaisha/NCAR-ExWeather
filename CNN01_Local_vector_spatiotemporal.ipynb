{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a0a927-ec50-44a0-b82c-26848002e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a24190-0ecb-4c87-ba41-0b1ec72becf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 18:35:23.570730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from keras_unet_collection import utils as k_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed60309c-e19e-4869-ae18-694ad645527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57dc768a-1ab3-4fc2-912e-3fe1d6f6ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2a25df-e774-4c84-8270-9b7f62d51e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3af42-f1fd-4d09-938f-b5657472ec64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6837e699-714b-4b55-98fc-b6b0a61765c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_v3_s = datetime(2018, 7, 15)\n",
    "base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "base_v4_s = datetime(2020, 12, 3)\n",
    "base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "base_ref = datetime(2010, 1, 1)\n",
    "\n",
    "date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+365+30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c990a7d3-da80-4e44-ac72-17b39d3ba477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 12, 31, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list_v4[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b090ba6-a5fc-4207-b7d3-c00fa1856a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_mixer(TRAIN, L, a0=0, a1=0.2):\n",
    "    data_shape = TRAIN.shape\n",
    "    out = np.empty((L, data_shape[-1]))\n",
    "    \n",
    "    for i in range(L):\n",
    "        inds = np.random.choice(data_shape[0], 2)\n",
    "        a = np.random.uniform(a0, a1)\n",
    "        out[i, :] = a*TRAIN[inds[0], :] + (1-a)*TRAIN[inds[1], :]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23b86cf-3500-471a-bdfe-bad8433bbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc32d24-a9ce-486a-9da6-fa401ca03cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_metric(VALID_target, Y_pred, ref):\n",
    "\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    # AUC = auc(fpr, tpr)\n",
    "    # AUC_metric = 1 - AUC\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    #ll = log_loss(VALID_target.ravel(), Y_pred.ravel())\n",
    "    \n",
    "    #print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric / ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912aef91-0827-49ae-95d4-28d4f8bfb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def set_seeds(seed):\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "#     np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bbe697-79dc-4c49-a499-474a478441fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(filenames, lon_80km, lon_minmax, lat_80km, lat_minmax, elev_80km, elev_max):\n",
    "    \n",
    "    lon_out = []\n",
    "    lat_out = []\n",
    "    elev_out = []\n",
    "    mon_out = []\n",
    "    \n",
    "    base_v3_s = datetime(2018, 7, 15)\n",
    "    base_v3_e = datetime(2020, 12, 2)\n",
    "\n",
    "    base_v4_s = datetime(2020, 12, 3)\n",
    "    base_v4_e = datetime(2022, 7, 15)\n",
    "\n",
    "    base_ref = datetime(2010, 1, 1)\n",
    "    \n",
    "    date_list_v3 = [base_v3_s + timedelta(days=day) for day in range(365+365+142)]\n",
    "    date_list_v4 = [base_v4_s + timedelta(days=day) for day in range(365+180-151)]\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        \n",
    "        if 'v4' in name:\n",
    "            date_list = date_list_v4\n",
    "        else:\n",
    "            date_list = date_list_v3\n",
    "        \n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        day = date_list[day]\n",
    "        month = day.month\n",
    "        \n",
    "        month_norm = (month - 1)/(12-1)\n",
    "        \n",
    "        lon = lon_80km[indx, indy]\n",
    "        lat = lat_80km[indx, indy]\n",
    "\n",
    "        lon = (lon - lon_minmax[0])/(lon_minmax[1] - lon_minmax[0])\n",
    "        lat = (lat - lat_minmax[0])/(lat_minmax[1] - lat_minmax[0])\n",
    "\n",
    "        elev = elev_80km[indx, indy]\n",
    "        elev = elev / elev_max\n",
    "        \n",
    "        lon_out.append(lon)\n",
    "        lat_out.append(lat)\n",
    "        elev_out.append(elev)\n",
    "        mon_out.append(month_norm)\n",
    "        \n",
    "    return np.array(lon_out), np.array(lat_out), np.array(elev_out), np.array(mon_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b1c36e5-4aee-49f2-b8b0-261600e25e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(save_dir+'HRRR_domain.hdf', 'r') as h5io:\n",
    "    lon_3km = h5io['lon_3km'][...]\n",
    "    lat_3km = h5io['lat_3km'][...]\n",
    "    lon_80km = h5io['lon_80km'][...]\n",
    "    lat_80km = h5io['lat_80km'][...]\n",
    "    elev_3km = h5io['elev_3km'][...]\n",
    "    land_mask_80km = h5io['land_mask_80km'][...]\n",
    "    \n",
    "grid_shape = land_mask_80km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "528e2e04-fffe-4f1c-a162-482f3c3983f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_80km = du.interp2d_wraper(lon_3km, lat_3km, elev_3km, lon_80km, lat_80km, method='linear')\n",
    "\n",
    "elev_80km[np.isnan(elev_80km)] = 0\n",
    "elev_80km[elev_80km<0] = 0\n",
    "elev_max = np.max(elev_80km)\n",
    "\n",
    "lon_80km_mask = lon_80km[land_mask_80km]\n",
    "lat_80km_mask = lat_80km[land_mask_80km]\n",
    "\n",
    "lon_minmax = [np.min(lon_80km_mask), np.max(lon_80km_mask)]\n",
    "lat_minmax = [np.min(lat_80km_mask), np.max(lat_80km_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707742a-b973-483d-9d60-95a8ef87c7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cb0cd3b-a1da-46d8-918c-2fb336cb93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead1 = 2\n",
    "lead2 = 3\n",
    "lead3 = 4\n",
    "lead4 = 5\n",
    "\n",
    "lead_name = 4\n",
    "model_tag = 'peak'\n",
    "\n",
    "8 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28c6aec5-4f1e-4a53-9a80-366ec8463aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_vec = \"/glade/work/ksha/NCAR/\"\n",
    "\n",
    "if (lead1 < 9) or (lead1 > 18):\n",
    "    path_name1_v3 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name1_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "\n",
    "if (lead2 < 9) or (lead2 > 18):\n",
    "    path_name2_v3 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name2_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "    \n",
    "if (lead3 < 9) or (lead3 > 18):\n",
    "    path_name3_v3 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name3_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "    \n",
    "if (lead4 < 9) or (lead4 > 18):\n",
    "    path_name4_v3 = '/glade/scratch/ksha/DATA/NCAR_batch_v3/'\n",
    "else:\n",
    "    path_name4_v3 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v3/'\n",
    "\n",
    "path_name1_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name2_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name3_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "path_name4_v4 = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4x/'\n",
    "\n",
    "path_name1_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name2_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name3_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'\n",
    "path_name4_v4_test = '/glade/campaign/cisl/aiml/ksha/NCAR_batch_v4_temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e593234c-ee7f-42fc-9384-5f8239efdd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train_lead1_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name1_v3, lead1)))\n",
    "filename_train_lead2_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name2_v3, lead2)))\n",
    "filename_train_lead3_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name3_v3, lead3)))\n",
    "filename_train_lead4_v3 = sorted(glob(\"{}TRAIN*lead{}.npy\".format(path_name4_v3, lead4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ead68ed4-b9aa-4068-87c0-d897c4f2841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IND_TRAIN_lead = np.load('/glade/work/ksha/NCAR/IND_TRAIN_lead_full.npy', allow_pickle=True)[()]\n",
    "TRAIN_ind1_v3 = IND_TRAIN_lead['lead{}'.format(lead1)]\n",
    "TRAIN_ind2_v3 = IND_TRAIN_lead['lead{}'.format(lead2)]\n",
    "TRAIN_ind3_v3 = IND_TRAIN_lead['lead{}'.format(lead3)]\n",
    "TRAIN_ind4_v3 = IND_TRAIN_lead['lead{}'.format(lead4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ff2c0a-5107-4bc9-a721-5ce809972dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lead1_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "data_lead1_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead2_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "data_lead2_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead3_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "data_lead3_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "data_lead4_p0 = np.load('{}TRAIN_v3_vec_lead{}_part0_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p1 = np.load('{}TRAIN_v3_vec_lead{}_part1_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "data_lead4_p2 = np.load('{}TRAIN_v3_vec_lead{}_part2_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af2205b3-1baf-42de-9503-d0b589ccfc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_lead1_v3 = np.concatenate((data_lead1_p0['y_vector'], data_lead1_p1['y_vector'], data_lead1_p2['y_vector']), axis=0)\n",
    "TRAIN_lead2_v3 = np.concatenate((data_lead2_p0['y_vector'], data_lead2_p1['y_vector'], data_lead2_p2['y_vector']), axis=0)\n",
    "TRAIN_lead3_v3 = np.concatenate((data_lead3_p0['y_vector'], data_lead3_p1['y_vector'], data_lead3_p2['y_vector']), axis=0)\n",
    "TRAIN_lead4_v3 = np.concatenate((data_lead4_p0['y_vector'], data_lead4_p1['y_vector'], data_lead4_p2['y_vector']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c58d7f3-f2c4-42e4-ab39-5d26131f3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_lead1_y_v3 = np.concatenate((data_lead1_p0['y_true'], data_lead1_p1['y_true'], data_lead1_p2['y_true']), axis=0)\n",
    "TRAIN_lead2_y_v3 = np.concatenate((data_lead2_p0['y_true'], data_lead2_p1['y_true'], data_lead2_p2['y_true']), axis=0)\n",
    "TRAIN_lead3_y_v3 = np.concatenate((data_lead3_p0['y_true'], data_lead3_p1['y_true'], data_lead3_p2['y_true']), axis=0)\n",
    "TRAIN_lead4_y_v3 = np.concatenate((data_lead4_p0['y_true'], data_lead4_p1['y_true'], data_lead4_p2['y_true']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b21a0ec8-fc11-4ccf-ab5d-4ff6f2129d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(TRAIN_ind2_v3)\n",
    "\n",
    "filename_train1_pick_v3 = []\n",
    "filename_train2_pick_v3 = []\n",
    "filename_train3_pick_v3 = []\n",
    "filename_train4_pick_v3 = []\n",
    "\n",
    "TRAIN_X_lead1 = np.empty((L, 128))\n",
    "TRAIN_X_lead2 = np.empty((L, 128))\n",
    "TRAIN_X_lead3 = np.empty((L, 128))\n",
    "TRAIN_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "TRAIN_Y_v3 = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1_v3 = int(TRAIN_ind1_v3[i])\n",
    "    ind_lead2_v3 = int(TRAIN_ind2_v3[i])\n",
    "    ind_lead3_v3 = int(TRAIN_ind3_v3[i])\n",
    "    ind_lead4_v3 = int(TRAIN_ind4_v3[i])\n",
    "    \n",
    "    filename_train1_pick_v3.append(filename_train_lead1_v3[ind_lead1_v3])\n",
    "    filename_train2_pick_v3.append(filename_train_lead2_v3[ind_lead2_v3])\n",
    "    filename_train3_pick_v3.append(filename_train_lead3_v3[ind_lead3_v3])\n",
    "    filename_train4_pick_v3.append(filename_train_lead4_v3[ind_lead4_v3])\n",
    "    \n",
    "    TRAIN_X_lead1[i, :] = TRAIN_lead1_v3[ind_lead1_v3, :]\n",
    "    TRAIN_X_lead2[i, :] = TRAIN_lead2_v3[ind_lead2_v3, :]\n",
    "    TRAIN_X_lead3[i, :] = TRAIN_lead3_v3[ind_lead3_v3, :]\n",
    "    TRAIN_X_lead4[i, :] = TRAIN_lead4_v3[ind_lead4_v3, :]\n",
    "    \n",
    "    TRAIN_Y_v3[i] = TRAIN_lead3_y_v3[ind_lead3_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8ac4ab4-18e1-42ab-bf04-9e158ed1fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_norm_v3, lat_norm_v3, elev_norm_v3, mon_norm_v3 = feature_extract(filename_train3_pick_v3, \n",
    "                                                                      lon_80km, lon_minmax, \n",
    "                                                                      lat_80km, lat_minmax, elev_80km, elev_max)\n",
    "\n",
    "TRAIN_stn_v3 = np.concatenate((lon_norm_v3[:, None], \n",
    "                               lat_norm_v3[:, None]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d479179-2549-49de-90d6-610670007dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f947c39-b53a-4333-a978-15375ed5ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_ind(filenames):\n",
    "    \n",
    "    indx_out = []\n",
    "    indy_out = []\n",
    "    day_out = []\n",
    "    flag_out = []\n",
    "    \n",
    "    for i, name in enumerate(filenames):\n",
    "        nums = re.findall(r'\\d+', name)\n",
    "        indy = int(nums[-2])\n",
    "        indx = int(nums[-3])\n",
    "        day = int(nums[-4])\n",
    "        \n",
    "        indx_out.append(indx)\n",
    "        indy_out.append(indy)\n",
    "        day_out.append(day)\n",
    "        \n",
    "        if \"pos\" in name:\n",
    "            flag_out.append(True)\n",
    "        else:\n",
    "            flag_out.append(False)\n",
    "        \n",
    "    return np.array(indx_out), np.array(indy_out), np.array(day_out), np.array(flag_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f555f51a-26e7-48b7-9ddc-11491d3ae9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx, indy, days, flags = name_to_ind(filename_train3_pick_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ed1ae69-305b-4e46-b14d-72f6e7200bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_shape = lon_80km.shape\n",
    "N_days = np.max(days)+1\n",
    "\n",
    "TRAIN_VEC = np.empty((N_days, 4,)+grid_shape+(128,))\n",
    "TRAIN_VEC[...] = np.nan\n",
    "\n",
    "for i in range(len(indx)):\n",
    "    indx_temp = indx[i]\n",
    "    indy_temp = indy[i]\n",
    "    days_temp = days[i]\n",
    "    \n",
    "    TRAIN_VEC[days_temp, 0, indx_temp, indy_temp, :] = TRAIN_X_lead1[i, :]\n",
    "    TRAIN_VEC[days_temp, 1, indx_temp, indy_temp, :] = TRAIN_X_lead2[i, :]\n",
    "    TRAIN_VEC[days_temp, 2, indx_temp, indy_temp, :] = TRAIN_X_lead3[i, :]\n",
    "    TRAIN_VEC[days_temp, 3, indx_temp, indy_temp, :] = TRAIN_X_lead4[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59c11c45-42b7-43fd-9e72-8de13f4edc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_pos_indx = indx[TRAIN_Y_v3==1]\n",
    "TRAIN_pos_indy = indy[TRAIN_Y_v3==1]\n",
    "TRAIN_pos_days = days[TRAIN_Y_v3==1]\n",
    "\n",
    "TRAIN_neg_indx = indx[TRAIN_Y_v3==0]\n",
    "TRAIN_neg_indy = indy[TRAIN_Y_v3==0]\n",
    "TRAIN_neg_days = days[TRAIN_Y_v3==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19544dea-1342-4c30-a470-d59b18652f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_stn_pos_v3 = TRAIN_stn_v3[TRAIN_Y_v3==1]\n",
    "TRAIN_stn_neg_v3 = TRAIN_stn_v3[TRAIN_Y_v3==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4a55881-1ba9-4310-ac1c-9cc49954f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_Y = TRAIN_Y_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f3f6b-8860-4506-95e1-42b03d9da78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a30ece5-27c7-49dd-9431-d9608dab439b",
   "metadata": {},
   "source": [
    "**Valid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5cdb3-9700-404f-be2a-b59d46aa20f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77782c59-1e77-4c60-87a9-12f93b51136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_valid_lead1 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name1_v4_test, lead1)))\n",
    "filename_valid_lead2 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name2_v4_test, lead2)))\n",
    "filename_valid_lead3 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name3_v4_test, lead3)))\n",
    "filename_valid_lead4 = sorted(glob(\"{}TEST*lead{}.npy\".format(path_name4_v4_test, lead4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45f705bf-7fdd-4362-ba2c-72227c3f9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lead1 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead1, model_tag), allow_pickle=True)[()]\n",
    "valid_lead2 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead2, model_tag), allow_pickle=True)[()]\n",
    "valid_lead3 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead3, model_tag), allow_pickle=True)[()]\n",
    "valid_lead4 = np.load('{}TEST_v4_vec_lead{}_{}.npy'.format(filepath_vec, lead4, model_tag), allow_pickle=True)[()]\n",
    "\n",
    "VALID_lead1 = valid_lead1['y_vector']\n",
    "VALID_lead2 = valid_lead2['y_vector']\n",
    "VALID_lead3 = valid_lead3['y_vector']\n",
    "VALID_lead4 = valid_lead4['y_vector']\n",
    "\n",
    "VALID_lead1_y = valid_lead1['y_true']\n",
    "VALID_lead2_y = valid_lead2['y_true']\n",
    "VALID_lead3_y = valid_lead3['y_true']\n",
    "VALID_lead4_y = valid_lead4['y_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becef75-4396-4c01-93ea-399a484eb752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "786fd32d-88c4-4017-a798-74d964b2dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IND_TEST_lead = np.load('/glade/work/ksha/NCAR/IND_TEST_lead_v4.npy', allow_pickle=True)[()]\n",
    "\n",
    "VALID_ind1 = IND_TEST_lead['lead{}'.format(lead1)]\n",
    "VALID_ind2 = IND_TEST_lead['lead{}'.format(lead2)]\n",
    "VALID_ind3 = IND_TEST_lead['lead{}'.format(lead3)]\n",
    "VALID_ind4 = IND_TEST_lead['lead{}'.format(lead4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5e7d130-1de3-4541-a638-9158cf1bb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(VALID_ind2)\n",
    "\n",
    "filename_valid1_pick = []\n",
    "filename_valid2_pick = []\n",
    "filename_valid3_pick = []\n",
    "filename_valid4_pick = []\n",
    "\n",
    "VALID_X_lead1 = np.empty((L, 128))\n",
    "VALID_X_lead2 = np.empty((L, 128))\n",
    "VALID_X_lead3 = np.empty((L, 128))\n",
    "VALID_X_lead4 = np.empty((L, 128))\n",
    "\n",
    "VALID_Y = np.empty(L)\n",
    "\n",
    "for i in range(L):\n",
    "    \n",
    "    ind_lead1 = int(VALID_ind1[i])\n",
    "    ind_lead2 = int(VALID_ind2[i])\n",
    "    ind_lead3 = int(VALID_ind3[i])\n",
    "    ind_lead4 = int(VALID_ind4[i])\n",
    "    \n",
    "    filename_valid1_pick.append(filename_valid_lead1[ind_lead1])\n",
    "    filename_valid2_pick.append(filename_valid_lead2[ind_lead2])\n",
    "    filename_valid3_pick.append(filename_valid_lead3[ind_lead3])\n",
    "    filename_valid4_pick.append(filename_valid_lead4[ind_lead4])\n",
    "    \n",
    "    VALID_X_lead1[i, :] = VALID_lead1[ind_lead1, :]\n",
    "    VALID_X_lead2[i, :] = VALID_lead2[ind_lead2, :]\n",
    "    VALID_X_lead3[i, :] = VALID_lead3[ind_lead3, :]\n",
    "    VALID_X_lead4[i, :] = VALID_lead4[ind_lead4, :]\n",
    "    \n",
    "    VALID_Y[i] = VALID_lead3_y[ind_lead3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2947435-f97a-4775-84ea-c8298ef1150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx, indy, days, flags = name_to_ind(filename_valid3_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0395099-86d3-4ca6-9658-d1d5ed6cbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_shape = lon_80km.shape\n",
    "N_days = np.max(days)-np.min(days)+1\n",
    "\n",
    "VALID_VEC = np.empty((N_days, 4,)+grid_shape+(128,))\n",
    "VALID_VEC[...] = np.nan\n",
    "\n",
    "for i in range(len(indx)):\n",
    "    indx_temp = indx[i]\n",
    "    indy_temp = indy[i]\n",
    "    days_temp = days[i]-np.min(days)\n",
    "    \n",
    "    if days_temp <0:\n",
    "        eqrgetwqh\n",
    "    \n",
    "    VALID_VEC[days_temp, 0, indx_temp, indy_temp, :] = VALID_X_lead1[i, :]\n",
    "    VALID_VEC[days_temp, 1, indx_temp, indy_temp, :] = VALID_X_lead2[i, :]\n",
    "    VALID_VEC[days_temp, 2, indx_temp, indy_temp, :] = VALID_X_lead3[i, :]\n",
    "    VALID_VEC[days_temp, 3, indx_temp, indy_temp, :] = VALID_X_lead4[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd616be5-be5d-41a5-931c-72033b93a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_shape = lon_80km.shape\n",
    "day_min = np.min(days)\n",
    "VALID_Input = np.empty((len(VALID_Y), L_vec, 128))\n",
    "VALID_Input[...] = np.nan\n",
    "\n",
    "for i in range(len(VALID_Y)):\n",
    "    \n",
    "    vec_merge = ()\n",
    "    \n",
    "    indx_temp = indx[i]\n",
    "    indy_temp = indy[i]\n",
    "    days_temp = days[i] - day_min\n",
    "    \n",
    "    indx_left = np.max([indx_temp - 1, 0])\n",
    "    indx_right = np.min([indx_temp + 1, grid_shape[0]])\n",
    "    \n",
    "    indy_bot = np.max([indy_temp - 1, 0])\n",
    "    indy_top = np.min([indy_temp + 1, grid_shape[1]])\n",
    "    \n",
    "    count = 0\n",
    "    for ix in [indx_temp, indx_left, indx_right]:\n",
    "        for iy in [indy_temp, indy_bot, indy_top]:\n",
    "            for il in range(4):\n",
    "                vec_temp = VALID_VEC[days_temp, il, ix, iy, :]\n",
    "                if np.sum(np.isnan(vec_temp)) == 0 and count < L_vec:\n",
    "                    vec_merge += (vec_temp[None, ...],)\n",
    "                    count += 1\n",
    "                    \n",
    "    VEC_merge = np.concatenate(vec_merge, axis=0)\n",
    "    VALID_Input[i, ...] = VEC_merge    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1771954-08bc-4ce7-9fe5-c33a2566d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    IN = keras.Input((L_vec, 128))\n",
    "    X = IN\n",
    "    X = keras.layers.Conv1D(128, kernel_size=2, strides=1, padding='valid', use_bias=True)(X)\n",
    "    X = keras.layers.Activation(\"gelu\")(X)\n",
    "    \n",
    "    #X = keras.layers.Flatten()(X)\n",
    "    X = keras.layers.GlobalMaxPool1D()(X)\n",
    "    #X = keras.layers.GlobalAveragePooling1D()(X)\n",
    "    \n",
    "    X = keras.layers.Dense(64)(X)\n",
    "    X = keras.layers.Activation(\"relu\")(X)\n",
    "    X = keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    OUT = X\n",
    "    OUT = keras.layers.Dense(1, activation='sigmoid', bias_initializer=keras.initializers.Constant(-10))(OUT)\n",
    "\n",
    "    model = keras.models.Model(inputs=IN, outputs=OUT)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbee2302-f09b-4442-bc28-7b7a000b3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model()  \n",
    "# model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#               optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "\n",
    "# Y_pred = model.predict(VALID_Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b67c26e-fd5f-41c4-8dd5-a2d1b1691f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695, \n",
    "         2463725, 4734, 23234, 884, 2341, 362, 5, 234, 483, 785356, 23425, 3621, \n",
    "         58461, 80968765, 123, 425633, 5646, 67635, 76785, 34214]\n",
    "\n",
    "training_rounds = len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ba8ffac-6fd0-4a42-8064-779ce5c7b6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial record: 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 18:43:13.288695: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-19 18:43:13.448054: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-19 18:43:13.504188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-19 18:43:13.504281: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-19 18:43:13.586177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-19 18:43:13.586703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-19 18:43:13.626987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-19 18:43:13.668083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-19 18:43:13.713426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-19 18:43:13.765986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-19 18:43:13.814282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-19 18:43:13.815103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-19 18:43:13.830454: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-19 18:43:13.832662: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-02-19 18:43:13.833041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:61:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-02-19 18:43:13.833160: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-19 18:43:13.833207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-19 18:43:13.833219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-02-19 18:43:13.833228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-19 18:43:13.833238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-19 18:43:13.833249: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-19 18:43:13.833258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-19 18:43:13.833274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-19 18:43:13.833775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-02-19 18:43:13.833803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-19 18:43:15.652396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-19 18:43:15.652437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-02-19 18:43:15.652448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-02-19 18:43:15.653809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:61:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 18:43:21.252190: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-02-19 18:43:21.297375: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2600000000 Hz\n",
      "2023-02-19 18:43:22.019738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-19 18:43:22.432168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved from 1.1 to 0.9995055291580204\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 18:43:39.904814: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 22.555625915527344 seconds ---\n",
      "Validation loss improved from 0.9995055291580204 to 0.9987652083891045\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 11.973741054534912 seconds ---\n",
      "Validation loss improved from 0.9987652083891045 to 0.9980862782003191\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 13.04357123374939 seconds ---\n",
      "Validation loss improved from 0.9980862782003191 to 0.9969056207156779\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 9.28758430480957 seconds ---\n",
      "Validation loss improved from 0.9969056207156779 to 0.9959681302242037\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 9.100560665130615 seconds ---\n",
      "Validation loss improved from 0.9959681302242037 to 0.9956002943134138\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 9.660341024398804 seconds ---\n",
      "Validation loss improved from 0.9956002943134138 to 0.9933439149996849\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 8.958252668380737 seconds ---\n",
      "Validation loss improved from 0.9933439149996849 to 0.9920031605069596\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 8.921626091003418 seconds ---\n",
      "Validation loss improved from 0.9920031605069596 to 0.9869044387248573\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 9.572531461715698 seconds ---\n",
      "Validation loss improved from 0.9869044387248573 to 0.9687042675250702\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 28.859158992767334 seconds ---\n",
      "Validation loss improved from 0.9687042675250702 to 0.9588008348565792\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 10.866137981414795 seconds ---\n",
      "Validation loss improved from 0.9588008348565792 to 0.8983699942584578\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 8.860955238342285 seconds ---\n",
      "Validation loss improved from 0.8983699942584578 to 0.8805149880620815\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 11.400964975357056 seconds ---\n",
      "Validation loss improved from 0.8805149880620815 to 0.8538392708322915\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 8.74369215965271 seconds ---\n",
      "Validation loss improved from 0.8538392708322915 to 0.837900558041381\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 8.718484163284302 seconds ---\n",
      "Validation loss improved from 0.837900558041381 to 0.8248644652161131\n",
      "save to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4\n",
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/HY_Lead4/assets\n",
      "--- 8.741334676742554 seconds ---\n",
      "Validation loss 0.8274906630346172 NOT improved\n",
      "Validation loss 0.890344995663135 NOT improved\n",
      "Validation loss 0.8481693242988443 NOT improved\n",
      "Validation loss 0.9010429683640709 NOT improved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref = np.sum(VALID_Y) / len(VALID_Y)\n",
    "grid_shape = lon_80km.shape\n",
    "day_min = np.min(TRAIN_neg_days)\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "\n",
    "batch_dir = '/glade/scratch/ksha/DATA/NCAR_batch/'\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = 'HY_Lead{}'.format(lead_name)\n",
    "\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(TRAIN_pos_indx)\n",
    "L_neg = len(TRAIN_neg_indx)\n",
    "\n",
    "record = 1.1\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 100 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "L_train = 16\n",
    "\n",
    "N_pos = 32\n",
    "N_neg = batch_size - N_pos\n",
    "NEG_Input = np.empty((N_neg, L_vec, 128))\n",
    "POS_Input = np.empty((N_pos, L_vec, 128))\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    if r == 0:\n",
    "        tol = 0\n",
    "    else:\n",
    "        tol = -200\n",
    "\n",
    "    model = create_model()\n",
    "    #\n",
    "    # model.compile(loss=keras.losses.mean_squared_error,\n",
    "    #               optimizer=keras.optimizers.Adam(lr=1e-5))\n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "    \n",
    "    set_seeds(int(seeds[r]))\n",
    "    print('Training round {}'.format(r))\n",
    "\n",
    "    \n",
    "    for i in range(epochs):            \n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            \n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "\n",
    "            ind_neg_pick = ind_neg[:N_neg]\n",
    "            ind_pos_pick = ind_pos[:N_pos]\n",
    "            \n",
    "            # ========== #\n",
    "            for ineg, neg_pick in enumerate(ind_neg_pick):\n",
    "                \n",
    "                count = 0\n",
    "                vec_merge = ()\n",
    "\n",
    "                indx_temp = TRAIN_neg_indx[neg_pick]\n",
    "                indy_temp = TRAIN_neg_indy[neg_pick]\n",
    "                days_temp = TRAIN_neg_days[neg_pick] - day_min\n",
    "\n",
    "                indx_left = np.max([indx_temp - 1, 0])\n",
    "                indx_right = np.min([indx_temp + 1, grid_shape[0]])\n",
    "\n",
    "                indy_bot = np.max([indy_temp - 1, 0])\n",
    "                indy_top = np.min([indy_temp + 1, grid_shape[1]])\n",
    "                \n",
    "                for ix in [indx_temp, indx_left, indx_right]:\n",
    "                    for iy in [indy_temp, indy_bot, indy_top]:\n",
    "                        for il in range(4):\n",
    "                            vec_temp = TRAIN_VEC[days_temp, il, ix, iy, :]\n",
    "                            if np.sum(np.isnan(vec_temp)) == 0 and count < L_vec:\n",
    "                                vec_merge += (vec_temp[None, ...],)\n",
    "                                count += 1\n",
    "\n",
    "                VEC_merge = np.concatenate(vec_merge, axis=0)\n",
    "                NEG_Input[ineg, ...] = VEC_merge\n",
    "                \n",
    "                \n",
    "            for ipos, pos_pick in enumerate(ind_pos_pick):\n",
    "                count = 0\n",
    "                vec_merge = ()\n",
    "\n",
    "                indx_temp = TRAIN_pos_indx[pos_pick]\n",
    "                indy_temp = TRAIN_pos_indy[pos_pick]\n",
    "                days_temp = TRAIN_pos_days[pos_pick] - day_min\n",
    "\n",
    "                indx_left = np.max([indx_temp - 1, 0])\n",
    "                indx_right = np.min([indx_temp + 1, grid_shape[0]])\n",
    "\n",
    "                indy_bot = np.max([indy_temp - 1, 0])\n",
    "                indy_top = np.min([indy_temp + 1, grid_shape[1]])\n",
    "                \n",
    "                for ix in [indx_temp, indx_left, indx_right]:\n",
    "                    for iy in [indy_temp, indy_bot, indy_top]:\n",
    "                        for il in range(4):\n",
    "                            vec_temp = TRAIN_VEC[days_temp, il, ix, iy, :]\n",
    "                            if np.sum(np.isnan(vec_temp)) == 0 and count < L_vec:\n",
    "                                vec_merge += (vec_temp[None, ...],)\n",
    "                                count += 1\n",
    "\n",
    "                VEC_merge = np.concatenate(vec_merge, axis=0)\n",
    "                POS_Input[ipos, ...] = VEC_merge\n",
    "            \n",
    "            # ========== #\n",
    "            \n",
    "            X_batch = np.concatenate((NEG_Input, POS_Input), axis=0)\n",
    "            Y_batch = np.ones([batch_size,])\n",
    "            Y_batch[:N_neg] = 0.0\n",
    "            \n",
    "            #Y_batch = np.zeros([batch_size,])\n",
    "            #Y_batch[:N_neg] = np.random.uniform(low=0.0, high=0.1, size=N_neg)\n",
    "            #Y_batch[N_neg:] = np.random.uniform(low=0.95, high=1.0, size=N_pos)\n",
    "\n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "\n",
    "            X_batch = X_batch[ind_, :]\n",
    "            Y_batch = Y_batch[ind_]\n",
    "            \n",
    "            model.train_on_batch(X_batch, Y_batch);\n",
    "\n",
    "        # epoch end operations\n",
    "        Y_pred = model.predict(VALID_Input)\n",
    "\n",
    "        Y_pred[Y_pred<0] = 0\n",
    "        Y_pred[Y_pred>1] = 1\n",
    "\n",
    "        record_temp = verif_metric(VALID_Y, Y_pred, ref)\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     model.save(model_path_backup)\n",
    "\n",
    "        if (record - record_temp > min_del):\n",
    "            print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "            record = record_temp\n",
    "            tol = 0\n",
    "            \n",
    "            #print('tol: {}'.format(tol))\n",
    "            # save\n",
    "            print('save to: {}'.format(model_path))\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            print('Validation loss {} NOT improved'.format(record_temp))\n",
    "            if record_temp > 1.1:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
