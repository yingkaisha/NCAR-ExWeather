{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb17daf5-e59a-4f3e-929a-91e7b21ff304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 09:22:38.917615: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "# general tools\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from keras_unet_collection import utils as k_utils\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6a16cd-2edc-47b8-993b-4c44538b6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "def Head(num_classes=1000, name=None):\n",
    "    \"\"\"Implementation of classification head of RegNet.\n",
    "    Args:\n",
    "      num_classes: number of classes for Dense layer\n",
    "      name: name prefix\n",
    "    Returns:\n",
    "      Classification head function.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = str(backend.get_uid(\"head\"))\n",
    "\n",
    "    def apply(x):\n",
    "        x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n",
    "        x = layers.LayerNormalization(\n",
    "            epsilon=1e-6, name=name + \"_head_layernorm\"\n",
    "        )(x)\n",
    "        x = layers.Dense(num_classes, name=name + \"_head_dense\")(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def create_model(input_shape=(64, 64, 15)):\n",
    "\n",
    "    depths=[3, 3, 27, 3]\n",
    "    projection_dims=[32, 64, 96, 128]\n",
    "    drop_path_rate=0.0\n",
    "    layer_scale_init_value=1e-6\n",
    "\n",
    "\n",
    "    model_name='Branch64X'\n",
    "    IN64 = layers.Input(shape=input_shape)\n",
    "    X = IN64\n",
    "\n",
    "    # X = layers.LocallyConnected2D(32, kernel_size=1, strides=(1, 1), padding=\"valid\", implementation=1)(X)\n",
    "    # X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_lc1_norm\".format(model_name))(X)\n",
    "    # X = layers.Activation(\"gelu\", name=\"{}_lc1_gelu\".format(model_name))(X)\n",
    "\n",
    "    # X = layers.LocallyConnected2D(96, kernel_size=1, strides=(1, 1), padding=\"valid\", implementation=1)(X)\n",
    "    # X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_lc2_norm\".format(model_name))(X)\n",
    "    # X = layers.Activation(\"gelu\", name=\"{}_lc2_gelu\".format(model_name))(X)\n",
    "\n",
    "    # ----- convnext block 0 ----- #\n",
    "\n",
    "    X = layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=\"{}_down0\".format(model_name))(X)\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_norm\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[0]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[0], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[0], name=\"{}_down0_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down0_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[0], name=\"{}_down0_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down0_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[0], name=\"{}_down0_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[0], name=\"{}_down0_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "\n",
    "    # ----- convnext block 1 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[1], kernel_size=2, strides=2, name=\"{}_down1\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[1]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[1], kernel_size=7, padding=\"same\",\n",
    "                                   groups=projection_dims[1], name=\"{}_down1_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down1_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[1], name=\"{}_down1_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down1_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[1], name=\"{}_down1_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[1], name=\"{}_down1_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 2 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[2], kernel_size=2, strides=2, name=\"{}_down2\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[2]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[2], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[2], name=\"{}_down2_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down2_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[2], name=\"{}_down2_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down2_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[2], name=\"{}_down2_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[2], name=\"{}_down2_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    # ----- convnext block 3 ----- #\n",
    "\n",
    "    X = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_norm\".format(model_name))(X)\n",
    "    X = layers.Conv2D(projection_dims[3], kernel_size=2, padding='same', name=\"{}_down3\".format(model_name))(X)\n",
    "\n",
    "    for j in range(depths[3]):\n",
    "\n",
    "        X_convnext = X\n",
    "        X_convnext = layers.Conv2D(filters=projection_dims[3], kernel_size=5, padding=\"same\",\n",
    "                                   groups=projection_dims[3], name=\"{}_down3_dconv{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.LayerNormalization(epsilon=1e-6, name=\"{}_down3_dconv{}_norm\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(4 * projection_dims[3], name=\"{}_down3_dense{}_p1\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Activation(\"gelu\", name=\"{}_down3_gelu{}\".format(model_name, j))(X_convnext)\n",
    "        X_convnext = layers.Dense(projection_dims[3], name=\"{}_down3_dense{}_p2\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X_convnext = LayerScale(layer_scale_init_value, projection_dims[3], name=\"{}_down3_layerscale{}\".format(model_name, j))(X_convnext)\n",
    "\n",
    "        X = X + X_convnext\n",
    "\n",
    "    V1 = X\n",
    "\n",
    "    OUT = layers.GlobalMaxPooling2D(name=\"{}_head_pool64\".format(model_name))(V1)\n",
    "    OUT = layers.LayerNormalization(epsilon=1e-6, name=\"{}_head_norm64\".format(model_name))(OUT)\n",
    "\n",
    "    OUT = layers.Dense(256, name=\"{}_dense1\".format(model_name))(OUT)\n",
    "    OUT = layers.LayerNormalization(epsilon=1e-6, name=\"{}_dense1_norm\".format(model_name))(OUT)\n",
    "    OUT = layers.Activation(\"gelu\", name=\"{}_dense1_gelu{}\".format(model_name, j))(OUT)\n",
    "\n",
    "    OUT = layers.Dense(1, name=\"{}_head_out\".format(model_name))(OUT)\n",
    "\n",
    "    model = Model(inputs=IN64, outputs=OUT, name=model_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def verif_metric(VALID_target, Y_pred):\n",
    "\n",
    "\n",
    "    # fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    # AUC = auc(fpr, tpr)\n",
    "    # AUC_metric = 1 - AUC\n",
    "    \n",
    "    BS = np.mean((VALID_target.ravel() - Y_pred.ravel())**2)\n",
    "    #ll = log_loss(VALID_target.ravel(), Y_pred.ravel())\n",
    "    \n",
    "    print('{}'.format(BS))\n",
    "    metric = BS\n",
    "\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de69f0f8-35e2-4601-93a2-ebe1c5881925",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_pick_from_batch = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "L_vars = len(ind_pick_from_batch)\n",
    "\n",
    "filename_neg_train = sorted(glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*neg_neg_neg*lead2.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*neg_neg_neg*lead3.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*neg_neg_neg*lead4.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*neg_neg_neg*lead5.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*neg_neg_neg*lead6.npy\"))\n",
    "\n",
    "filename_pos_train = sorted(glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*pos*lead2.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*pos*lead3.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*pos*lead4.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*pos*lead5.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/TRAIN*pos*lead6.npy\"))\n",
    "\n",
    "filename_neg_valid = sorted(glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*neg_neg_neg*lead2.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*neg_neg_neg*lead3.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*neg_neg_neg*lead4.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*neg_neg_neg*lead5.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*neg_neg_neg*lead6.npy\"))\n",
    "\n",
    "filename_pos_valid = sorted(glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*pos*lead2.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*pos*lead3.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*pos*lead4.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*pos*lead5.npy\")+\\\n",
    "                            glob(\"/glade/scratch/ksha/DATA/NCAR_batch/VALID*pos*lead6.npy\"))\n",
    "\n",
    "\n",
    "filename_valid = filename_neg_valid[::200] + filename_pos_valid\n",
    "\n",
    "with h5py.File(save_dir+'HRRR_domain.hdf', 'r') as h5io:\n",
    "    lon_3km = h5io['lon_3km'][...]\n",
    "    lat_3km = h5io['lat_3km'][...]\n",
    "    lon_72km = h5io['lon_72km'][...]\n",
    "    lat_72km = h5io['lat_72km'][...]\n",
    "    land_mask_72km = h5io['land_mask_72km'][...]\n",
    "    land_mask_3km = h5io['land_mask_3km'][...]\n",
    "\n",
    "L_valid = len(filename_valid)\n",
    "L_var = L_vars\n",
    "\n",
    "TEST_input_64 = np.empty((L_valid, 64, 64, L_var))\n",
    "TEST_target = np.ones(L_valid)\n",
    "\n",
    "for i, name in enumerate(filename_valid):\n",
    "    data = np.load(name)\n",
    "    for k, c in enumerate(ind_pick_from_batch):\n",
    "        \n",
    "        TEST_input_64[i, ..., k] = data[..., c]\n",
    "\n",
    "        if 'pos' in name:\n",
    "            TEST_target[i] = 1.0\n",
    "        else:\n",
    "            TEST_target[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf25ad-c5aa-4cc8-b47d-aa66e98ecae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rounds = 20\n",
    "\n",
    "seeds = [12342, 2536234, 98765, 473, 865, 7456, 69472, 3456357, 3425, 678,\n",
    "         2452624, 5787, 235362, 67896, 98454, 12445, 46767, 78906, 345, 8695]\n",
    "\n",
    "min_del = 0\n",
    "max_tol = 8 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 200\n",
    "L_train = 64 #int(len(TRAIN_Y_pick) / batch_size)\n",
    "\n",
    "X_batch_64 = np.empty((batch_size, 64, 64, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "\n",
    "X_batch_64[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "# =========== Model Section ========== #\n",
    "key = 'F15_Lead6'\n",
    "model_name = '{}'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "\n",
    "tol = 0\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "L_pos = len(filename_pos_train)\n",
    "L_neg = len(filename_neg_train)\n",
    "\n",
    "record = 1.1 #0.09750284033269506\n",
    "print(\"Initial record: {}\".format(record))\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "for r in range(training_rounds):\n",
    "    \n",
    "    tol = 0 - count - r\n",
    "    \n",
    "    #flag_count_change = True\n",
    "    \n",
    "    model = create_model(input_shape=(64, 64, 15))\n",
    "    \n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True), optimizer=keras.optimizers.Adam(lr=5e-4))\n",
    "    \n",
    "    # W_old = k_utils.dummy_loader('/glade/work/ksha/NCAR/Keras_models/LIGHT5_Lead6_tune5')\n",
    "    # model.set_weights(W_old)\n",
    "    \n",
    "    set_seeds(seeds[r])\n",
    "    print('Training round {}'.format(r))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "    \n",
    "        #backend.set_value(model.optimizer.learning_rate, learning_rate[i])\n",
    "        \n",
    "        #print('epoch = {}'.format(i))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # loop of batch\n",
    "        for j in range(L_train):\n",
    "            \n",
    "            N_pos = 30\n",
    "            N_neg = batch_size - N_pos\n",
    "            \n",
    "            ind_neg = du.shuffle_ind(L_neg)\n",
    "            ind_pos = du.shuffle_ind(L_pos)\n",
    "        \n",
    "            file_pick_neg = []\n",
    "            for ind_temp in ind_neg[:N_neg]:\n",
    "                file_pick_neg.append(filename_neg_train[ind_temp])\n",
    "\n",
    "            file_pick_pos = []\n",
    "            for ind_temp in ind_pos[:N_pos]:\n",
    "                file_pick_pos.append(filename_pos_train[ind_temp])\n",
    "        \n",
    "            file_pick = file_pick_neg + file_pick_pos\n",
    "            \n",
    "            if len(file_pick) != batch_size:\n",
    "                sregwet\n",
    "        \n",
    "            for k in range(batch_size):\n",
    "                data = np.load(file_pick[k])\n",
    "                \n",
    "                for l, c in enumerate(ind_pick_from_batch):\n",
    "                    temp = data[..., c] \n",
    "                    X_batch_64[k, ..., l] = temp\n",
    "                    \n",
    "                if 'pos' in file_pick[k]:\n",
    "                    Y_batch[k, :] = 1.0 #np.random.uniform(0.9, 0.99)\n",
    "                elif 'neg_neg_neg' in file_pick[k]:\n",
    "                    Y_batch[k, :] = 0.0 #np.random.uniform(0.01, 0.05)\n",
    "                else:\n",
    "                    werhgaer\n",
    "                    \n",
    "            ind_ = du.shuffle_ind(batch_size)\n",
    "            X_batch_64 = X_batch_64[ind_, ...]\n",
    "            Y_batch = Y_batch[ind_, :]\n",
    "            \n",
    "            # train on batch\n",
    "            model.train_on_batch(X_batch_64, Y_batch);\n",
    "            \n",
    "        # epoch end operations\n",
    "        Y_pred = model.predict([TEST_input_64])\n",
    "        # Y_pred[Y_pred<0] = 0\n",
    "        # Y_pred[Y_pred>1] = 1\n",
    "        \n",
    "        Y_pred = 1/(1 + np.exp(-Y_pred))\n",
    "        \n",
    "        record_temp = verif_metric(TEST_target, Y_pred)\n",
    "\n",
    "        # if i % 10 == 0:\n",
    "        #     model.save(model_path_backup)\n",
    "\n",
    "        if (record - record_temp > min_del):\n",
    "            print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "            record = record_temp\n",
    "            tol = 0\n",
    "            #print('tol: {}'.format(tol))\n",
    "            # save\n",
    "            print('save to: {}'.format(model_path))\n",
    "            model.save(model_path)\n",
    "        else:\n",
    "            print('Validation loss {} NOT improved'.format(record_temp))\n",
    "            if record_temp >= 2.0:\n",
    "                print('Early stopping')\n",
    "                break;\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= max_tol:\n",
    "                    print('Early stopping')\n",
    "                    break;\n",
    "                else:\n",
    "                    if tol == 1 and i > count:\n",
    "                        #flag_count_change = False\n",
    "                        count = i\n",
    "                        print(count)\n",
    "                    continue;\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf257b-898c-44a9-affe-67270b801099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf6dc2-2fe3-463c-86c0-36e3dd415c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc0b02-d3be-4b8b-aa2a-1fc347944c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
