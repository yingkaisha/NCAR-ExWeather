{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2691f0a5-e348-4592-a238-9c6858792779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 07:53:21.275633: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# general tools\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import time\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras_unet_collection import models as k_models\n",
    "from keras_unet_collection import utils as k_utils\n",
    "from keras_unet_collection import layer_utils as k_layers\n",
    "from keras_unet_collection.activations import GELU\n",
    "\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/NCAR/libs/')\n",
    "\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "\n",
    "from sklearn.metrics import classification_report, auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def verif_metric(VALID_target, Y_pred, thres=0.5):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(VALID_target.ravel(), Y_pred.ravel()>thres).ravel()\n",
    "\n",
    "    CSI = tp/(tp+fn+fp)\n",
    "    CSI_metric = 1 - CSI\n",
    "    \n",
    "    POFD = fp/(tn+fp)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(VALID_target.ravel(), Y_pred.ravel())\n",
    "    AUC = auc(fpr, tpr)\n",
    "    AUC_metric = 1 - AUC\n",
    "    print('{} {} {}'.format(CSI, POFD, AUC))\n",
    "    metric = 0.2*POFD + 0.7*CSI_metric + 0.1*AUC_metric\n",
    "\n",
    "\n",
    "    return metric\n",
    "\n",
    "# ========== training/validation split ========== #\n",
    "\n",
    "filename_aug = sorted(glob(\"/glade/scratch/ksha/DATA/NCAR_batch_aug/*.npy\"))\n",
    "filename_full = sorted(glob(\"/glade/scratch/ksha/DATA/NCAR_batch/*.npy\"))\n",
    "\n",
    "cut_train_aug = 136000\n",
    "cut_train_full = 2848299\n",
    "\n",
    "filename_train_aug = filename_aug[:cut_train_aug]\n",
    "filename_train_full = filename_full[:cut_train_full]\n",
    "\n",
    "factor = 20\n",
    "\n",
    "L_valid_aug = int(5*factor)\n",
    "L_valid_full = int(3000*factor)\n",
    "\n",
    "filename_valid_aug = filename_aug[cut_train_aug:]\n",
    "filename_valid_full = filename_full[cut_train_full:]\n",
    "\n",
    "shuffle(filename_valid_aug)\n",
    "shuffle(filename_valid_full)\n",
    "\n",
    "filename_valid_aug = filename_aug[-L_valid_aug:]\n",
    "filename_valid_full = filename_full[-L_valid_full:]\n",
    "\n",
    "# ========== Validation set ========== #\n",
    "\n",
    "ind_pick_from_batch = np.arange(19)\n",
    "\n",
    "L_vars = len(ind_pick_from_batch)\n",
    "\n",
    "grid_shape = (128, 128)\n",
    "\n",
    "L_valid = L_valid_aug+L_valid_full\n",
    "\n",
    "VALID_input = np.empty((L_valid,)+grid_shape+(L_vars,))\n",
    "VALID_target = np.empty(L_valid)\n",
    "\n",
    "for i, filename in enumerate(filename_valid_aug+filename_valid_full):\n",
    "    data = np.load(filename)\n",
    "    \n",
    "    for c, v in enumerate(ind_pick_from_batch):\n",
    "    \n",
    "        VALID_input[i, ..., c] = data[..., v]\n",
    "        \n",
    "    if 'pos' in filename:\n",
    "        VALID_target[i] = True\n",
    "    elif 'neg' in filename:\n",
    "        VALID_target[i] = False\n",
    "    else:\n",
    "        aergheagtha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519c468-8301-4d80-b085-bcb0741cdf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 10:00:22.075567: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 74835558400 exceeds 10% of free system memory.\n",
      "2022-08-19 10:24:22.431346: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-08-19 10:24:24.538889: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2631578947368421 0.003066717778629644 0.9607620523048321\n",
      "Initial record 0.5203266120094533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 10:59:10.444594: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 74835558400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020671834625323 0.06926782113035217 0.972633537291628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 11:31:28.072298: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /glade/work/ksha/NCAR/Keras_models/BIG19_tornado_backup/assets\n",
      "Validation loss 0.7021199262591815 NOT improved\n"
     ]
    }
   ],
   "source": [
    "# ========== Model ========== #\n",
    "\n",
    "# ---------- Layers ---------- #\n",
    "\n",
    "IN = tf.keras.Input((128, 128, 19))\n",
    "\n",
    "X = IN\n",
    "\n",
    "X = k_layers.CONV_stack(X, 32, kernel_size=3, stack_num=2, dilation_rate=1, activation='GELU', batch_norm=True, name='conv_stack1')\n",
    "X = tf.keras.layers.Conv2D(32, kernel_size=2, strides=(2, 2), padding='valid', use_bias=True, name='stride_conv1')(X)\n",
    "\n",
    "X = k_layers.CONV_stack(X, 64, kernel_size=3, stack_num=2, dilation_rate=1, activation='GELU', batch_norm=True, name='conv_stack2')\n",
    "X = tf.keras.layers.Conv2D(64, kernel_size=2, strides=(2, 2), padding='valid', use_bias=True, name='stride_conv2')(X)\n",
    "\n",
    "X = k_layers.CONV_stack(X, 128, kernel_size=3, stack_num=2, dilation_rate=1, activation='GELU', batch_norm=True, name='conv_stack3')\n",
    "X = tf.keras.layers.Conv2D(128, kernel_size=2, strides=(2, 2), padding='valid', use_bias=True, name='stride_conv3')(X)\n",
    "\n",
    "X = k_layers.CONV_stack(X, 256, kernel_size=3, stack_num=2, dilation_rate=1, activation='GELU', batch_norm=True, name='conv_stack4')\n",
    "X = tf.keras.layers.Conv2D(256, kernel_size=2, strides=(2, 2), padding='valid', use_bias=True, name='stride_conv4')(X)\n",
    "\n",
    "X = k_layers.CONV_stack(X, 512, kernel_size=3, stack_num=2, dilation_rate=1, activation='GELU', batch_norm=True, name='conv_stack5')\n",
    "X = tf.keras.layers.Conv2D(512, kernel_size=2, strides=(2, 2), padding='valid', use_bias=True, name='stride_conv5')(X)\n",
    "\n",
    "D = tf.keras.layers.Flatten()(X)\n",
    "\n",
    "D = tf.keras.layers.Dense(512, use_bias=False, name='dense1')(D)\n",
    "D = tf.keras.layers.BatchNormalization(axis=-1, name='dense_bn1')(D)\n",
    "D = GELU()(D)\n",
    "\n",
    "D = tf.keras.layers.Dense(128, use_bias=False, name='dense2')(D)\n",
    "D = tf.keras.layers.BatchNormalization(axis=-1, name='dense_bn2')(D)\n",
    "D = GELU()(D)\n",
    "\n",
    "D = tf.keras.layers.Dense(1, activation='sigmoid', name='head')(D)\n",
    "\n",
    "OUT = D\n",
    "\n",
    "model = keras.models.Model(inputs=[IN,], outputs=[OUT,])\n",
    "\n",
    "W_new = model.get_weights()\n",
    "\n",
    "# ---------- Weights ---------- #\n",
    "\n",
    "temp_dir = '/glade/work/ksha/NCAR/Keras_models/'\n",
    "\n",
    "key = 'BIG19'\n",
    "\n",
    "model_name = '{}_tornado'.format(key)\n",
    "model_path = temp_dir+model_name\n",
    "model_path_backup = temp_dir+model_name+'_backup'\n",
    "\n",
    "#W_new = k_utils.dummy_loader(model_path)\n",
    "\n",
    "W_old = k_utils.dummy_loader(temp_dir+'VGG_X_pp20_tune2')\n",
    "\n",
    "for l in range(len(W_old)):\n",
    "    if W_old[l].shape == W_new[l].shape:\n",
    "        W_new[l] = W_old[l]\n",
    "\n",
    "# ---------- Compile ---------- #\n",
    "\n",
    "model.set_weights(W_new)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=1e-5))\n",
    "\n",
    "# ========== Initial record ========== #\n",
    "\n",
    "Y_pred = model.predict([VALID_input,])\n",
    "record = verif_metric(VALID_target, Y_pred)\n",
    "print('Initial record {}'.format(record))\n",
    "\n",
    "# ========== Training hyper parameters ========== #\n",
    "\n",
    "tol = 0\n",
    "min_del = 0\n",
    "max_tol = 500 # early stopping with patience\n",
    "\n",
    "epochs = 500\n",
    "L_train = 64\n",
    "batch_size = 100\n",
    "batch_size_half = 50\n",
    "\n",
    "valid_size = 1\n",
    "\n",
    "X_batch = np.empty((batch_size, 128, 128, L_vars))\n",
    "Y_batch = np.empty((batch_size, 1))\n",
    "X_batch[...] = np.nan\n",
    "Y_batch[...] = np.nan\n",
    "\n",
    "# ========== Training loop ========== #\n",
    "\n",
    "L_full = len(filename_train_full)\n",
    "L_aug = len(filename_train_aug)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    if i <= 10:\n",
    "        batch_size_full = 70\n",
    "    if i > 10 and i <= 35:\n",
    "        batch_size_full = 85\n",
    "    if i > 35:\n",
    "        batch_size_full = 95\n",
    "    \n",
    "    batch_size_aug = batch_size - batch_size_full\n",
    "    \n",
    "    #print('epoch = {}'.format(i))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # loop of batch\n",
    "    for j in range(L_train):\n",
    "        \n",
    "        ind_full = du.shuffle_ind(L_full)\n",
    "        ind_aug = du.shuffle_ind(L_aug)\n",
    "        \n",
    "        file_pick_full = []\n",
    "        for ind_temp in ind_full[:batch_size_full]:\n",
    "            file_pick_full.append(filename_train_full[ind_temp])\n",
    "\n",
    "        file_pick_aug = []\n",
    "        for ind_temp in ind_aug[:batch_size_aug]:\n",
    "            file_pick_aug.append(filename_train_aug[ind_temp])\n",
    "        \n",
    "        file_pick = file_pick_full + file_pick_aug\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            \n",
    "            data = np.load(file_pick[k])\n",
    "            \n",
    "            for c, v in enumerate(ind_pick_from_batch):\n",
    "                \n",
    "                X_batch[k, ..., c] = data[..., v]\n",
    "            \n",
    "            if 'pos' in file_pick[k]:\n",
    "                Y_batch[k, :] = np.random.uniform(0.95, 0.99)\n",
    "            elif 'neg' in file_pick[k]:\n",
    "                Y_batch[k, :] = np.random.uniform(0.01, 0.05)\n",
    "            else:\n",
    "                werhgaer\n",
    "        \n",
    "        # # add noise within sparse inputs\n",
    "        # for v in flag_sparse:\n",
    "        #     X_batch[..., v] += np.random.uniform(0, 0.01, size=(batch_size, 128, 128))\n",
    "\n",
    "        # shuffle indices\n",
    "        ind_ = du.shuffle_ind(batch_size)\n",
    "        X_batch = X_batch[ind_, ...]\n",
    "        Y_batch = Y_batch[ind_, :]\n",
    "        \n",
    "        if np.sum(np.isnan(X_batch)) > 0:\n",
    "            asfeargagqarew\n",
    "        \n",
    "        # train on batch\n",
    "        model.train_on_batch([X_batch,], [Y_batch,]);\n",
    "    \n",
    "    # epoch end operations\n",
    "    Y_pred = model.predict([VALID_input,])\n",
    "    record_temp = verif_metric(VALID_target, Y_pred, thres=0.5)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        model.save(model_path_backup)\n",
    "    \n",
    "    if (record - record_temp > min_del) and (np.max(Y_pred) > 0.6):\n",
    "        print('Validation loss improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp\n",
    "        tol = 0\n",
    "        #print('tol: {}'.format(tol))\n",
    "        # save\n",
    "        print('save to: {}'.format(model_path))\n",
    "        model.save(model_path)\n",
    "    else:\n",
    "        print('Validation loss {} NOT improved'.format(record_temp))\n",
    "        tol += 1\n",
    "        #print('tol: {}'.format(tol))\n",
    "        if tol >= max_tol:\n",
    "            print('Early stopping')\n",
    "            sys.exit();\n",
    "        else:\n",
    "            #print('Pass to the next epoch')\n",
    "            continue;\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5af8db8-a88e-49b2-b54b-4522134440d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "101 % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5bdeb-aea3-4639-985b-c5cf349902e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c295b-e524-46b5-804d-b1737d404f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
